{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from task_graph import TaskGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_graph = TaskGraph(\"Create haikus on all seasons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "task_graph.add_task(\n",
    "    task_name='Fall haiku', \n",
    "    agent_name='haiku_agent',\n",
    "    task_description='Write a haiku about the Fall season',\n",
    "    )\n",
    "\n",
    "task_graph.add_task(\n",
    "    task_name='Winter haiku', \n",
    "    agent_name='haiku_agent',\n",
    "    task_description='Write a haiku about winter',\n",
    "    )\n",
    "task_graph.add_task(\n",
    "    task_name='Get Data', \n",
    "    agent_name='haiku_agent',\n",
    "    task_description='Write a haiku about the Fall season',\n",
    "    )\n",
    "\n",
    "task_graph.add_task(\n",
    "    task_name='Upload data', \n",
    "    agent_name='haiku_agent',\n",
    "    task_description='Write a haiku about winter',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_graph.add_dependency('Winter haiku', 'Fall haiku')\n",
    "task_graph.add_dependency('Get Data', 'Upload data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(task_graph.generate_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_info = task_graph.graph.nodes['Fall haiku']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_info['task'].task_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task import Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orchestration_task_desc = f\"\"\"\n",
    "You are helping the user complete the task: {task_graph.name}.  It has {task_graph.graph.number_of_nodes()}.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Task(\n",
    "    task_name='orchestration',\n",
    "    agent_name='agent_manager',\n",
    "    task_description=orchestration_task_desc,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.get_system_instruction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_graph.add_task('Task1.1', sequence=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_graph.add_dependency('Task1', 'Task1.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_graph.update_task('Task1.1', completed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = task_graph.get_next_task('Task1.1')\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'travis+coan'\n",
    "max_results = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = f\"http://export.arxiv.org/api/query?search_query={query}&start=0&max_results={max_results}\"\n",
    "# Get the response\n",
    "response = requests.get(api_url)\n",
    "# Get the content\n",
    "content = response.content\n",
    "# Parse the content\n",
    "soup = BeautifulSoup(content, 'xml')\n",
    "\n",
    "# Get the entries\n",
    "entries = soup.find_all('entry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for entry in entries:\n",
    "    title = entry.title.text\n",
    "    authors = [author.find('name').text for author in entry.find_all('author')]\n",
    "    summary = entry.summary.text\n",
    "    link = entry.link['href']\n",
    "    date_of_publication = entry.published.text\n",
    "\n",
    "    result = {\n",
    "        'title': title,\n",
    "        'author': authors,\n",
    "        'summary': summary,\n",
    "        'link': link,\n",
    "        'date_of_publication': date_of_publication\n",
    "    }\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://arxiv.org/pdf/2311.05661v1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the pdf content in pdf viewer\n",
    "with open('test.pdf', 'wb') as f:\n",
    "    f.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ('~/Downloads/test.pdf', 'wb') as f:\n",
    "    f.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pdf and get the text\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "reader = PdfReader('test.pdf', 'rb')\n",
    "\n",
    "number_of_pages = len(reader.pages)\n",
    "\n",
    "full_text = ''\n",
    "for page in reader.pages:\n",
    "    full_text +=   page.extract_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_image_chars(text):\n",
    "    # The image characters are in the format /uniXXXX where XXXX is a hexidecimal number, e.g. /uniE0BB\n",
    "    # So, we need to define a regular expression pattern to match these sequences\n",
    "    # Define a regular expression pattern to match Unicode sequences\n",
    "    pattern = re.compile(r'/uni[0-9A-Fa-f]{6}')\n",
    "\n",
    "    # Use the sub() function to replace matches with an empty string\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprint\n",
      "PROMPT ENGINEERING A PROMPT ENGINEER\n",
      "Qinyuan Ye1†Maxamed Axmed2Reid Pryzant2Fereshte Khani2\n",
      "1University of Southern California2Microsoft\n",
      "qinyuany@usc.edu fkhani@microsoft.com\n",
      "ABSTRACT\n",
      "Prompt engineering is a challenging yet crucial task for optimizing the perfor-\n",
      "mance of large language models (LLMs). It requires complex reasoning to exam-\n",
      "ine the model’s errors, hypothesize what is missing or misleading in the current\n",
      "prompt, and communicate the task with clarity. While recent works indicate that\n",
      "LLMs can be meta-prompted to perform automatic prompt engineering, their po-\n",
      "tentials may not be fully untapped due to the lack of sufficient guidance to elicit\n",
      "complex reasoning capabilities in LLMs in the meta-prompt. In this work, we in-\n",
      "vestigate the problem of “prompt engineering a prompt engineer”—constructing\n",
      "a meta-prompt that more effectively guides LLMs to perform automatic prompt\n",
      "engineering. We introduce and analyze key components, such as a step-by-step\n",
      "reasoning template and context specification, which lead to improved perfor-\n",
      "mance. In addition, inspired by common optimization concepts such as batch\n",
      "size, step size and momentum, we introduce their verbalized counterparts to the\n",
      "meta-prompt and investigate their effects. Our final method, named PE2, finds\n",
      "a prompt that outperforms “let’s think step by step” by 6.3% on the MultiArith\n",
      "dataset and 3.1% on the GSM8K dataset. To demonstrate its versatility, we apply\n",
      "PE2 to the Instruction Induction benchmark, a suite of counterfactual tasks, and a\n",
      "lengthy, real-world industrial prompt. In these settings, PE2 achieves strong per-\n",
      "formance and outperforms prior automatic prompt engineering baselines. Further,\n",
      "we show that PE2 makes meaningful and targeted prompt edits, amends erroneous\n",
      "or incomplete prompts, and presents non-trivial counterfactual reasoning abilities.\n",
      "1 I NTRODUCTION\n",
      "Large language models (LLMs) are powerful tools for many natural language processing tasks when\n",
      "given the right prompts.1However, finding the optimal prompt can be challenging due to the model’s\n",
      "sensitivity (Jiang et al., 2020; Zhao et al., 2021; Lu et al., 2022), often necessitating extensive manual\n",
      "trial-and-error efforts. Moreover, once an initial prompt is deployed into production, unforeseen\n",
      "edge cases may emerge, demanding more rounds of manual efforts to further refine the prompt.\n",
      "These challenges give rise to an emerging research field of automatic prompt engineering. Within\n",
      "this field, a notable line of methods involves harnessing the capabilities of LLMs themselves (Zhou\n",
      "et al., 2023b; Pryzant et al., 2023). Specifically, this entails meta-prompting LLMs with instructions\n",
      "such as “inspect the current prompt and a batch of examples, then propose a new prompt.”\n",
      "While these methods achieve impressive performance, a subsequent question arises: What makes a\n",
      "good meta-prompt for automatic prompt engineering? To answer this question, we connect two key\n",
      "observations: (1) Prompt engineering itself is a complex language task that requires deep reasoning:\n",
      "it involves closely examining the model’s errors, hypothesizing what is missing or misleading in\n",
      "the current prompt, and communicating the task more clearly to the LLM. (2) Complex reasoning\n",
      "capabilities in LLMs can be elicited by prompting the model to “think step by step” (Wei et al.,\n",
      "2022; Kojima et al., 2022) and can be further improved by instructing them to reflect on their outputs\n",
      "(Madaan et al., 2023; Chen et al., 2023).\n",
      "†Work done while interning at Microsoft.\n",
      "1The word “prompt” is often overloaded with multiple meanings in recent literature. In this paper, prompt\n",
      "refers to the task description ( e.g., “Translate English to French”) or instruction ( e.g., “Let’s think step by step”).\n",
      "1arXiv:2311.05661v1  [cs.CL]  9 Nov 2023Preprint\n",
      "MultiArith GSM8K Instruction Induction\n",
      "(14 Tasks)Counterfactual Eval\n",
      "(12 Tasks)255075100(Average) Accuracy+6.3\n",
      "+3.1+7.4\n",
      "+10.6Initialization\n",
      "Iterative APEAPO\n",
      "PE2 (This Work)\n",
      "Production\n",
      "PromptF1 Score+8.0\n",
      "Figure 1: Results Overview. Our method PE2 consistently brings improvements over the prompt\n",
      "initialization (marked with orange text). It outperforms prompt optimization baselines Iterative APE\n",
      "(Zhou et al., 2023b) and APO (Pryzant et al., 2023). See full results on the Instruction Induction\n",
      "benchmark in Fig. 5, Counterfactual Eval in Fig. 6-7 and production prompt optimization in Fig. 11.\n",
      "Bridging these two observations, in this work, we prompt engineer a prompt engineer—we aim\n",
      "to construct a meta-prompt that guide LLMs to perform prompt engineering more effectively (§3;\n",
      "Fig. 2). By reflecting on the limitations of existing methods and incorporating recent advances\n",
      "in prompting for complex reasoning, we introduce meta-prompt components such as a step-by-\n",
      "step reasoning template and context specification, to explicitly guide the LLM to reason during the\n",
      "prompt engineering process. Additionally, since prompt engineering can be viewed as an optimiza-\n",
      "tion problem, we take inspiration from common optimization concepts such as batch size, step size\n",
      "and momentum, and introduce their verbalized counterparts to the meta-prompt. We experiment\n",
      "with these components and variants on two mathematical reasoning datasets, MultiArith (Roy &\n",
      "Roth, 2015) and GSM8K (Cobbe et al., 2021), and identify a best-performing combination, which\n",
      "we name as PE2 (§5.1).\n",
      "PE2 achieves strong empirical performance (§5.2). When using TEXT -DAVINCI -003 as the task\n",
      "model, the prompts produced by PE2 surpass the zero-shot chain-of-thought prompt, “let’s think\n",
      "step by step” (Kojima et al., 2022) by 6.3% on MultiArith and 3.1% on GSM8K. Moreover, PE2\n",
      "outperforms two automatic prompt engineering baselines, Iterative APE (Zhou et al., 2023b) and\n",
      "APO (Pryzant et al., 2023) in multiple settings (Fig. 1). Notably, PE2 is most effective on counter-\n",
      "factual tasks (Wu et al., 2023), where the automatic prompt engineer is anticipated to reason about\n",
      "non-standard situations ( e.g., do addition in base-8 instead of base-10) and explain such situation to\n",
      "the task model through the prompt. Beyond academic datasets, PE2 proves its broad applicability in\n",
      "optimizing a lengthy, real-world prompt used in production.\n",
      "Upon examining the prompt edit history of PE2 (§5.3), we find that PE2 consistently offers mean-\n",
      "ingful prompt edits. It is able to amend erroneous or incomplete prompts and enrich the prompts\n",
      "with additional details, which leads to improved final performance (Table 4). Interestingly, when\n",
      "uninformed about performing addition in base-8, PE2 formulates its own arithmetic rules from the\n",
      "examples: “if both numbers are less than 50, add 2 to the sum. If either number is 50 or greater, add\n",
      "22 to the sum.” While this is an imperfect short-cut solution, it demonstrates PE2’s non-trivial ability\n",
      "to reason in counterfactual situations. Despite these achievements, we also recognize the limitations\n",
      "and failure cases of PE2. We show that PE2 is influenced and bounded by the inherent limitations of\n",
      "current LLMs, such as neglecting given instructions and hallucinating incorrect rationales (Table 5).\n",
      "2 B ACKGROUND\n",
      "In this section, we provide a formal formulation of the prompt engineering problem (§2.1), and de-\n",
      "scribe a general framework of automatic prompt engineering using LLMs and meta-prompts (§2.2).\n",
      "Building on this foundation, in §3, we introduce the meta-prompt components and variants we in-\n",
      "vestigate in this work.\n",
      "2.1 P ROMPT ENGINEERING\n",
      "The goal of prompt engineering is to find the textual prompt p∗that achieves the best performance\n",
      "on a given dataset Dwhen using a given LLM Mtask as the task model. More specifically, we\n",
      "assume all datasets can be formatted as textual input-output pairs, i.e.,D={(x, y)}. We are given\n",
      "2Preprint\n",
      "a training set Dtrain for optimizing the prompt, Ddevfor validation, and Dtestfor final evaluation.\n",
      "Following the notations in Zhou et al. (2023b), the prompt engineering problem can be described as:\n",
      "p∗= arg max\n",
      "pX\n",
      "(x,y)∈Ddevf(Mtask(x;p), y) (1)\n",
      "where Mtask(x;p)is the output generated by the model when conditioning on the prompt p, and\n",
      "fis a per-example evaluation function. For example, if the evaluation metric is exact match,\n",
      "f(Mtask(x;p), y) = 1[Mtask(x;p) =y].\n",
      "2.2 A UTOMATIC PROMPT ENGINEERING WITH LLM S\n",
      "To alleviate the intensive efforts of human prompt engineering, recent works explore automating this\n",
      "process by meta-prompting LLMs to paraphrase the prompt (Zhou et al., 2023b) or refine the prompt\n",
      "by inspecting failure examples (Pryzant et al., 2023). In the following, we describe a framework that\n",
      "encapsulates these prior works and is employed in our investigation in later sections. It has three\n",
      "components: prompt initialization, new prompt proposal, and the search procedure.\n",
      "Prompt Initialization. To start the prompt engineering process, a set of initial prompts P(0)is\n",
      "needed. We consider two initialization methods: (1) Manual initialization is applicable for tasks\n",
      "that has pre-existing prompts written by humans experts. For example, “Let’s think step by step”\n",
      "leads to good performance on mathematical reasoning tasks and can be used as the initialization for\n",
      "prompt optimization. In (2) Induction Initialization , we follow the practice in Zhou et al. (2023b).\n",
      "We use a batch of examples {(x, y)}from Dtrain and a prompt pinit(“Here are the input-output\n",
      "pairs. What is the instruction?”; See §B.1) to guide a LLM to generate a set of initial prompts P(0).\n",
      "New Prompt Proposal. Given a set of initial prompts, the automatic prompt engineer will con-\n",
      "tinuously propose new and potentially better prompts. At timestamp t, the prompt engineer is\n",
      "given a prompt p(t)and expected to write a new prompt p(t+1). Optionally, a batch of examples\n",
      "B={(x, y, y′)}may be inspected in the new prompt proposal process. Here y′=Mtask(x;p)\n",
      "represents model-generated output and yrepresents the ground-truth label. We use pmetato denote\n",
      "a meta-prompt that is used to instruct the LLM Mproposal to propose new prompts. Therefore,\n",
      "p(t+1)=Mproposal (p(t), B;pmeta) (2)\n",
      "Constructing a better meta-prompt pmetato improve the quality of the proposed prompt p(t+1)is\n",
      "the main focus of this study. We will describe multiple components and variants we consider in §3.\n",
      "Search Procedure. As LLMs are sensitive to trivial prompt variations, it is possible that the newly\n",
      "proposed prompt p(t+1)under-performs the original prompt p(t). Therefore, automatic prompt en-\n",
      "gineering is typically combined with a back-tracking enabled search procedure. At timestamp t, we\n",
      "select nbest-performing prompts from allprompt candidates obtained in previous timestamps ( i.e.,\n",
      "P(0)∪P(1)∪...∪P(t)). For each of these nprompts, we sample mdifferent batches B, and run\n",
      "the meta-prompt in Eq. 2 to produce mnew prompts. This results in m×nnew prompts, which we\n",
      "denote as P(t+1)collectively and are used at the next timestamp t+ 1. The prompt proposal and\n",
      "search procedure are described more formally in Algorithm 1 in Appendix A.3.\n",
      "3 P ROMPT ENGINEERING A PROMPT ENGINEER\n",
      "Much like how the prompt plays an important role for the end task performance, the meta-prompt\n",
      "pmetaintroduced in Eq. 2 plays an important role in the quality of newly proposed prompts, and thus\n",
      "the overall quality of automatic prompt engineering. In this work, we focus on prompt engineering\n",
      "the meta-prompt pmeta—we develop meta-prompt components that can potentially help improve\n",
      "LLMs’ prompt engineering quality, and conduct a systematic ablation study on these components.\n",
      "We base the design of these components on two motivations: (1) providing detailed instructions\n",
      "and context; (2) incorporating common optimizer concepts . In the following, we describe these\n",
      "elements in more detail and explain our rationale. We also visualize them in Fig. 2.\n",
      "3Preprint\n",
      "Let’s read a blogpost on prompt engineering:\n",
      "Prompt engineering is a relatively new discipline for \n",
      "developing and optimizing prompts to efficiently use \n",
      "language models (LMs) …(a) prompt engineering tutorial\n",
      "A prompt is a text paragraph that outlines the expected \n",
      "actions and instructs the model. In our collaboration, \n",
      "we'll work together to refine a prompt. The process \n",
      "consists of two steps:\n",
      "# Step 1\n",
      "Examine the prompt and a batch of examples\n",
      "# Step 2\n",
      "Propose a new prompt based on your reasoning(b) two -step task instruction\n",
      "Sure! I'd be happy to help you.\n",
      "# Current Prompt\n",
      "Let’s think step by step.\n",
      "# Full Template\n",
      "```\n",
      "Question: <input>\n",
      "Answer: Let’s think step by step.\n",
      "```\n",
      "# Examples\n",
      "## Example 1\n",
      "Input: George had 28 socks. If he threw away 4 socks …\n",
      "Output: 64\n",
      "Reasoning: Step 1: George had 28 socks. Step 2: …\n",
      "Label: 60\n",
      "[More examples …](d) context specification(c) step -by-step reasoning template\n",
      "(e) batch size## Example 1\n",
      "Output is correct? No.\n",
      "Reasoning: the model didn't subtract the socks he \n",
      "threw away.\n",
      "Prompt describing the task correctly? Yes.\n",
      "Necessary to edit the prompt? Yes.\n",
      "Suggestions: The prompt should be edited to guide the \n",
      "model to perform subtraction.\n",
      "[More examples …]\n",
      "Now carefully review your reasoning and proceed with \n",
      "step 2: refine the prompt.\n",
      "# Current Prompt\n",
      "Let’s think step by step.\n",
      "# Optimization History\n",
      "At time 0, the prompt was “…”, it was edited …\n",
      "# Instructions\n",
      "* You are allowed to change up to 10 words\n",
      "* The total length should be less than 50 words\n",
      "* Reply with the prompt. Do not include other text.\n",
      "Let’s solve this problem step by step. Remember to add \n",
      "or subtract as needed.  # Instruction\n",
      "For each example, provide reasoning according to the \n",
      "following template\n",
      "* Output is correct?\n",
      "* Prompt describing the task correctly?\n",
      "* Necessary to edit the prompt?\n",
      "* If yes, suggestions on prompt editing?\n",
      "(g) optim  history\n",
      "(f) step size\n",
      "Meta -Prompt: Instruction and Context Meta -Prompt: Optimizer Concepts Prompt Feedback (“Gradients”)\n",
      "Figure 2: Illustration of the meta-prompt components. See §B.4 for the complete meta-prompt.\n",
      "Providing Detailed Instructions and Context. In prior work, the meta-prompt either instructs\n",
      "the proposal model to generate paraphrasing of prompts (Zhou et al., 2023b) or contain minimal\n",
      "instructions about inspecting a batch of examples (Pryzant et al., 2023). We posit that enriching the\n",
      "meta-prompt with additional instructions and context may be helpful.\n",
      "(a)Prompt Engineering Tutorial. To help the LLM better understand the task of prompt engi-\n",
      "neering, we provide an online tutorial of prompt engineering in the meta-prompt.2\n",
      "(b)Two-step Task Description. The task of prompt engineering can be decomposed into two\n",
      "steps, as previously done in Pryzant et al. (2023): In step 1, the model is expected to inspect the\n",
      "current prompt and a batch. In step 2, the model is expected to compose an improved prompt.3\n",
      "However, in Pryzant et al. (2023) each step is explained on the fly . In contrast, we consider\n",
      "clarifying the two steps and communicating the expectations upfront in the meta-prompt.\n",
      "(c)Step-by-step Reasoning Template. To encourage the model to examine each example in the\n",
      "batch Bclosely and reflect on the limitations in the current prompt, we guide the prompt pro-\n",
      "posal model Mproposal to answer a list of questions. For example: Is the output correct? Is the\n",
      "prompt correctly describing the task? Is it necessary to edit the prompt?\n",
      "(d)Context Specification. In practice, the location where the prompt is inserted in the whole input\n",
      "sequence is flexible. It may precede the input text to describe the task, e.g., “Translate English\n",
      "to French”. It may appear after the input text, e.g., “let’s think step by step”, to elicit reasoning\n",
      "capabilities. Recognizing these varying contexts, we explicitly specify the interplay between\n",
      "the prompt and the input. For example, “Q: <input>A: Let’s think step by step.”\n",
      "2https://www.promptingguide.ai/introduction. Published under MIT license.\n",
      "3From the view of gradient descent, step 1 is analogous to computing the gradient or calling loss.backward();\n",
      "and step 2 is analogous to applying the gradient or calling optimizer.step(). From the view of ReAct prompting\n",
      "(Yao et al., 2023), step 1 is reasoning and step 2 is acting.\n",
      "4Preprint\n",
      "Incorporating Common Optimizer Concepts. The prompt engineering problem described in\n",
      "Eq. 1 is essentially an optimization problem, and the prompt proposal in Eq. 2 can be considered as\n",
      "doing one optimization step. Thus, we consider the following concepts commonly used in gradient-\n",
      "based optimization and develop their verbalized counterparts to be used in our meta-prompt.\n",
      "(e)Batch Size. Batch size is the number of (failure) examples that is used in each prompt proposal\n",
      "step (Eq. 2). We experiment with batch sizes of {1,2,4,8}in our analysis.\n",
      "(f)Step Size. In gradient-based optimization, the step size determines the extent to which the\n",
      "model’s weights are updated. In prompt engineering, the counterpart would be the number of\n",
      "words (tokens) that can be modified. We directly specify that “You are allowed to change up to\n",
      "swords in the original prompt”, where s∈ {5,10,15,None}.4\n",
      "(g)Optimization History and Momentum. Momentum (Qian, 1999) is a technique to accelerate\n",
      "optimization and avoid oscillations by maintaining the moving average of past gradients. To\n",
      "develop the verbalized counterpart of momentum, we include all past prompts (at timestamp\n",
      "0,1, ..., t−1), their performance on the dev set, and a summary of prompt edits.\n",
      "4 E XPERIMENT SETTING\n",
      "4.1 T ASKS\n",
      "We use the following four groups of tasks to evaluate the effectiveness and limitations of PE2. More\n",
      "details ( e.g., dataset sizes, train-test splitting) are deferred in Appendix D.1.\n",
      "(1) Mathematical Reasoning. We use MultiArith (Roy & Roth, 2015) and GSM8K (Cobbe et al.,\n",
      "2021), which contain grade school math problems that requires multiple steps of arithmetic opera-\n",
      "tions. Previously, Kojima et al. (2022) discovered that “Let’s think step by step” can elicit multi-step\n",
      "reasoning in LLMs to perform these two tasks. We use this prompt as the initialization.\n",
      "(2) Instruction Induction. Instruction Induction (Honovich et al., 2023) is a benchmark for infer-\n",
      "ring the underlying instruction from few-shot examples. We use 14 selected tasks5that cover a wide\n",
      "range of use cases, e.g., “Formality” is a task that aims at rephrasing a sentence in formal language;\n",
      "“Second Word Letter” aims at outputting the second letter in an input word. Full details on these\n",
      "tasks can be found in Table 10.\n",
      "(3) Counterfactual Evaluation. We use the arithmetic, chess, and syntax tasks and their coun-\n",
      "terfactual variants introduced in Wu et al. (2023). For arithmetic, the original task is addition in\n",
      "base-10, and the counterfactual tasks are addition in base-8/9/11/16. We describe the chess and\n",
      "syntax tasks in Table 11. We use this set of tasks to observe whether PE2 can reason about counter-\n",
      "factual situations and communicate them to the task model.\n",
      "(4) Production Prompt. Lastly, we apply PE2 to optimize a production prompt on a multi-label\n",
      "and hierarchical classification task: classifying a natural language query into domain, and then into\n",
      "intents under the domain, and finally into a set of possible slots for each intent. The initial prompt\n",
      "consists of more than 5k tokens, and is carefully designed by experienced engineers.\n",
      "4.2 E XPERIMENT DETAILS\n",
      "Compared Methods. In addition to the multiple meta-prompt variants introduced in §3, we com-\n",
      "pare with the following three baselines. (a) APE (Zhou et al., 2023b) : The base version of APE\n",
      "is an initialization-only method and does not involve new prompt proposal steps. It uses an initial-\n",
      "ization prompt pinitto generate multiple prompt candidates from a few examples, and select the\n",
      "4Chen et al. (2022) and Zhou et al. (2023a) showed that LLMs could follow text generation constraints\n",
      "specified in natural language.\n",
      "5To save computation, we removed 8 tasks since the baseline method APE already achieves near perfect\n",
      "accuracies (95%+) on them. We also removed 2 tasks due to their small dataset size ( ≤50examples). See\n",
      "Appendix D.1.\n",
      "5Preprint\n",
      "best one among them based on Ddevperformance. (b) Iterative APE (Zhou et al., 2023b) : After\n",
      "initialization, pmetainstructs the model to produce a paraphrase of p(t)and use it as p(t+1).(c) APO\n",
      "(Pryzant et al., 2023) :pmetacontains minimal instructions on inspecting the batch B, generating\n",
      "textual “gradients” (feedback), and producing a new prompt p(t+1). We include the pinitandpmeta\n",
      "used in these baseline methods in Appendix B.\n",
      "LLMs and Search Budget. All the baselines mentioned above are encapsulated in the general\n",
      "framework introduced in §2.2. Due to cost and access considerations, we use GPT-4 (OpenAI,\n",
      "2023) as prompt proposal model Mproposal and use TEXT -DAVINCI -003 (Ouyang et al., 2022) as\n",
      "the task model Mtaskperforming the underlying task. To ensure fair comparison, we use the same\n",
      "search budget for all prompt optimization methods. For experiments using induction initialization,\n",
      "30 prompts are generated by pinitand form the initial candidate set P(0). The number of opti-\n",
      "mization steps Tis set to be 3. At each timestamp, we select n= 4 best-performing prompts, and\n",
      "propose m= 4prompts from each of them.\n",
      "5 R ESULTS AND ANALYSIS\n",
      "5.1 E MPIRICAL INVESTIGATION ON THE META-PROMPT\n",
      "Table 1: Investigation on meta-prompt compo-\n",
      "nents and configurations.\n",
      "MethodMultiArith\n",
      "DevGSM8K\n",
      "Dev\n",
      "PE2 (default) 92.0 68.0\n",
      "Meta-prompt: Instructions and Context\n",
      "+ prompt engineering tutorial 90.0 63.0\n",
      "- two-step task description 89.0 66.0\n",
      "- step-by-step reasoning template 87.0 61.0\n",
      "- context specification 93.0 63.0\n",
      "Meta-prompt: Optimizer Concepts\n",
      "+ tune batch size {1,2,4,8} 92.0 68.0\n",
      "+ tune step size {5,10,15,None} 95.0 68.0\n",
      "+ optim history and momentum 93.0 67.0\n",
      "Other Configurations\n",
      "- back-tracking 90.0 66.0\n",
      "- hard negative sampling 90.0 68.0\n",
      "Start 1 2 3\n",
      "Timestamp0.20.40.60.81.0Dev AccuracyAblation on Meta-prompt Components\n",
      "default - task desc - reasoning - context\n",
      "Best Prompt\n",
      "Figure 3: Prompt optimization dynamics on\n",
      "MultiArith when removing selected meta-prompt\n",
      "components. By removing one component, the\n",
      "new prompts have larger variance in their quality.Previously in §3 we introduced meta-prompt\n",
      "components that are potentially helpful for im-\n",
      "proving the prompt engineering quality. In this\n",
      "section, we begin with a default configuration6,\n",
      "then add or remove meta-prompt components\n",
      "to quantify their utility. We use the two math\n",
      "reasoning datasets, MultiArith and GSM8K, as\n",
      "the testbed and present the results in Table 1.\n",
      "We demonstrate that three components de-\n",
      "signed to provide more detailed instructions\n",
      "and context (two-step task description, step-\n",
      "by-step reasoning template, context specifica-\n",
      "tion) contribute significantly to prompt engi-\n",
      "neering quality. As shown in Fig. 3, the ex-\n",
      "clusion of any one of these components results\n",
      "in a wider variance in the quality distribution\n",
      "of newly-proposed prompts. Moreover, with-\n",
      "out these components, the proposal model more\n",
      "frequently suggests low-quality prompts com-\n",
      "pared to the default version. We do not ob-\n",
      "serve significant improvement by incorporat-\n",
      "ing prompt engineering tutorial. As the tutorial\n",
      "is excessively long (2500+ tokens) and slows\n",
      "down the runtime, we do not include it in the\n",
      "final version of PE2.\n",
      "The optimizer-inspired concepts can improve\n",
      "the performance occasionally, but the current\n",
      "experiments do not give a definitive conclusion\n",
      "regarding their utilities. Similar to the case\n",
      "of gradient-based optimization, hyperparame-\n",
      "ter selection is a noisy process and tend to be task-dependant. For discrete prompt optimization,\n",
      "this process may be further complicated by factors such as the task model’s sensitivity to prompts\n",
      "and the proposal model’s capability to follow instructions in the meta-prompt. For simplicity, we\n",
      "adopt the default configurations (batch size = 2, step size = none, no momentum) in the final version.\n",
      "6By default, the meta-prompt uses two-step task description, step-by-step reasoning template, context spec-\n",
      "ification. It uses a batch size of 2, and does not explicitly set a step size. It does not use the prompt engineering\n",
      "tutorial or the optimization history.\n",
      "6Preprint\n",
      "Table 2: Performance Comparison on Math-\n",
      "ematical Reasoning Tasks. TD002/003\n",
      "refers to text-davinci-002/003 models.\n",
      "MethodTask\n",
      "ModelProposal\n",
      "ModelMultiArith\n",
      "TestGSM8K\n",
      "Test\n",
      "Fixed Prompt, Reported\n",
      "Zero-shot CoT TD002 - 78.7 40.7\n",
      "APE TD002 TD002 82.0 43.0\n",
      "Fixed Prompt, Reproduced\n",
      "Zero-shot CoT TD003 - 86.0 60.9\n",
      "APE TD003 - 87.0 61.5\n",
      "Prompt Optimization\n",
      "Iterative APE TD003 GPT-4 88.5 62.7\n",
      "APO TD003 GPT-4 88.5 63.1\n",
      "PE2 (this work) TD003 GPT-4 92.3 64.0Table 3: Best prompts for MultiArith found by com-\n",
      "pared prompt optimization methods.\n",
      "Method MultiArith Prompt\n",
      "Fixed Prompt\n",
      "Zero-shot CoT Let’s think step by step.\n",
      "APELet’s work this out in a step by step way to be sure we have\n",
      "the right answer.\n",
      "Prompt Optimization\n",
      "Iterative APE Let’s proceed in a methodical, step-by-step manner.\n",
      "APOGiven the scenario, perform the necessary calculations step by\n",
      "step to find the final result. Consider all parts of the input and\n",
      "the sequence of events.\n",
      "PE2\n",
      "(this work)Let’s solve this problem by considering all the details. Pay\n",
      "attention to each piece of information, remember to add or\n",
      "subtract as needed, and perform the calculations step by step.\n",
      "Start 1 2 3\n",
      "Timestamp0.000.250.500.751.00Dev AccuracyComparison with Prompt Optimization Methods\n",
      "Iteratvie APE APO PE2\n",
      "Best Prompt\n",
      "Start 1 2 3\n",
      "Timestamp0.000.250.500.751.00Effect of Prompt Initialization\n",
      "default irrelevant misleading induction\n",
      "Best Prompt\n",
      "Figure 4: Prompt optimization dynamics on MultiArith. Left: Comparison with Iterative APE and\n",
      "APO. Right: Using different initializations.\n",
      "We also conduct an ablation study on back-tracking ( i.e., at timestamp t, select top-performing\n",
      "prompts from ∪t\n",
      "i=0P(i)versus only P(t)) and hard negative sampling ( i.e., the batch Bis sampled\n",
      "from the model’s errors, versus the batch is randomly sampled from Dtrain ). Based on the results\n",
      "we keep back-tracking and hard negative sampling in PE2.\n",
      "5.2 M AINRESULTS\n",
      "Improved baselines with more recent LLMs. In Zero-shot CoT (Kojima et al., 2022) and APE\n",
      "(Zhou et al., 2023b), the results were obtained with a earlier TEXT -DAVINCI -002 model. We first\n",
      "rerun the prompts in these two works with TEXT -DAVINCI -003, a more recent model. In the top\n",
      "two sections in Table 2, we observe a significant performance boost by using TEXT -DAVINCI -003,\n",
      "suggesting that it is more capable of solving math reasoning problems with zero-shot CoT. Moreover,\n",
      "the gaps between the two prompts are narrowed (MultiArith: 3.3%→1.0%, GSM8K: 2.3%→\n",
      "0.6%), indicating TEXT -DAVINCI -003 has a reduced sensitivity to prompt paraphrasing. Given this,\n",
      "methods that rely on simple paraphrasing, such as Iterative APE, may not enhance the final outcome\n",
      "as effectively. More precise and targeted prompt edits are necessary to improve the performance.\n",
      "PE2 outperforms Iterative APE and APO on various tasks. PE2 is able to find a prompt that\n",
      "achieves 92.3%accuracy on MultiArith ( +6.3%compared to Zero-shot CoT) and 64.0%on GSM8K\n",
      "(+3.1%). Additionally, prompts found by PE2 outperforms Iterative APE (Zhou et al., 2023b) and\n",
      "APO (Pryzant et al., 2023). In Fig. 1 we summarize performance gain obtained by PE2 on the in-\n",
      "struction induction benchmark, counterfactual evaluation, and a production prompt, demonstrating\n",
      "that PE2 achieves strong performance on diverse language tasks. Notably, when induction initializa-\n",
      "tion is used, PE2 outperforms APO on 11 out of 12 counterfactual tasks (Fig. 6), demonstrating that\n",
      "PE2 is capable of reasoning about contradictions and counterfactual situations. We defer experiment\n",
      "details and performance breakdown for these benchmarks in Appendix A.2 and C.\n",
      "PE2 generates targeted prompt edits and high-quality prompts. In Fig. 4(a) we plot the quality\n",
      "of prompt proposals over the course of prompt optimization. We observe very distinct patterns\n",
      "for the three prompt optimization methods: Iterative APE is based on paraphrasing, so the newly\n",
      "generated prompts have smaller variance. APO makes drastically large prompt edits and thus the\n",
      "performance drops in the first step. PE2 is the most stable one among the three methods. In Table\n",
      "3, we list the optimal prompts found by these methods. Both APO and PE2 are able to provide\n",
      "7Preprint\n",
      "Table 4: Notable prompt edits made by PE2. See Table 7 for additional examples.\n",
      "Task tPrompt Dev Acc.\n",
      "Correct wrong or incomplete task instructions\n",
      "Rhymes0 Remove the first letter from each input word and then replace that first letter with a similar\n",
      "sounding letter or group of letters to form a new word.0.35\n",
      "1 Generate a word that rhymes with the input word. 0.45\n",
      "Provide more specific context and details\n",
      "Second Word Letter0 Find the second letter in each word. 0.9\n",
      "1 Identify the second character in the provided word. 0.95\n",
      "2 Identify the second character from the start of the given word. 1.0\n",
      "Produce short-cut solutions in counterfactual tasks\n",
      "Base-8 Addition\n",
      "(induction init.)0 Add the two numbers given as input to get the output. 0.0\n",
      "3 Add the two numbers provided in the input. Then, adjust this sum based on the following\n",
      "rule: if both numbers are less than 50, add 2 to the sum. If either number is 50 or greater,\n",
      "add 22 to the sum. The final result is the output.0.35\n",
      "Table 5: Limitations and failure cases of PE2.\n",
      "Task Meta-prompt and Reasoning Snippets\n",
      "Neglecting instructions in the meta-prompt\n",
      "Base-9 AdditionMeta-prompt: ... Note that the ground-truth labels are absolutely correct , but the prompts (task descrip-\n",
      "tions) may be incorrect and need modification. ...\n",
      "Reasoning: No, it is not necessary to edit the prompt. The prompt is correct, but the label is incorrect. ... The\n",
      "issue seems to be with the label, not the prompt.\n",
      "Hallucination (when hints are provided in the meta-prompt)\n",
      "Base-8 AdditionHint: The calculation may be performed in a different numerical base.\n",
      "Reasoning: ... Given this, it’s possible that the numbers are being added in base 80, not base 10. In base 80,\n",
      "adding 20 to the sum of two numbers would be equivalent to adding 1 in base 10.\n",
      "New Prompt: The inputs are two numbers separated by a ’+’. Add these two numbers together in base 80,\n",
      "then add 1 to give the output in base 10.\n",
      "instructions on “considering all parts / details”. In addition, PE2 is designed to inspect the batch\n",
      "closely, enabling it to go beyond simple paraphrasing edits and make very specific prompt edits\n",
      "such as “remember to add or subtract as needed”.\n",
      "5.3 A NALYSIS AND CASE STUDY\n",
      "PE2 amends erroneous or incomplete instructions, and provides more details in instructions.\n",
      "In Table 4 and Table 7, we present notable prompt edits made by PE2. In the task of rhymes (finding\n",
      "a word that rhymes with the input word), the initial prompt mistakenly suggests the task is about\n",
      "changing the first letter of a word. PE2 successfully correct this after one optimization step. We\n",
      "also find interesting prompt edits on the counterfactual tasks. In base-8 addition, when induction\n",
      "initialization is used ( i.e., the prompt engineer is uninformed with the information of base-8 and\n",
      "must infer it from the examples), PE2 is able to devise its own arithmetic rules ( e.g., add 22 to the\n",
      "sum) that is partially correct. Though this is an imperfect short-cut solution, it demonstrates PE2’s\n",
      "ability to engage in sophisticated counterfactual reasoning.\n",
      "Limitations on following the meta-prompt and hallucination. Despite the successes made by\n",
      "PE2, we note several factors that’s limiting its performance in Table 5. For example, the meta-prompt\n",
      "explicitly states that the “ground-truth labels are absolutely correct”, while the prompt proposal\n",
      "model insists that “the prompt is correct, but the label is incorrect” and refuses to propose a new\n",
      "prompt. We also attempted to guide PE2 with hints ( e.g., “the calculation may be performed in\n",
      "a different numerical base”). Regrettably, this sometimes prompts the model to generate incorrect\n",
      "solutions ( e.g., base-80) , and even create rationales to verify this imagined solution. Though these\n",
      "observations are partly due to the difficulty of counterfactual tasks, they highlight the critical need\n",
      "to improve instruction following abilities and address hallucination issues in LLMs.\n",
      "Initialization is import to automatic prompt engineering. Previously, we use “Let’s think step\n",
      "by step.” as the initialization for math reasoning tasks. We further experiment with using a mislead-\n",
      "ingprompt, an irrelevant prompt and induction initialization (inducing from a few examples). The\n",
      "results are presented in Table 6 and the optimization dynamics are visualized in Fig. 4(b).\n",
      "8Preprint\n",
      "Table 6: Effect of Initialization.†The prompts\n",
      "are originally from Kojima et al. (2022).\n",
      "InitializationMultiArith\n",
      "DevGSM8K\n",
      "Dev\n",
      "default (Let’s think step by step.) 92.0 68.0\n",
      "misleading†(Don’t think. Just feel.) 81.0 50.0\n",
      "irrelevant†(It’s a beautiful day.) 73.0 49.0\n",
      "induction from few-shot examples 84.0 43.0\n",
      "no-op (Let’s think step by step.) 85.0 57.0In general, performance drops when alternative\n",
      "initialization methods are used, which highlights\n",
      "the role of high-quality initialization. Still, PE2\n",
      "is able to override the irrelevant or misleading\n",
      "prompts and gradually improve the performance\n",
      "(see Fig. 4(b)). Remarkably, PE2 is able to dis-\n",
      "cover a high quality prompt by itself using in-\n",
      "duction initialization (84% on MultiArith-Dev)\n",
      "that almost matches with “Let’s think step by\n",
      "step” (85%) designed by highly-experienced hu-\n",
      "man prompt engineers. This demonstrates the impressive prompt engineering capability of PE2 and\n",
      "suggests its potential for finding even better prompts when given additional computational resources.\n",
      "6 R ELATED WORK\n",
      "Automatic Prompt Engineering. To alleviate the intensive trial-and-error efforts in manual\n",
      "prompt engineering, the research community has developed various strategies to automate this pro-\n",
      "cess with techniques such as incremental editing (Prasad et al., 2023), reinforcement learning (Deng\n",
      "et al., 2022; Zhang et al., 2022), algorithmic search (Xu et al., 2022), among others. A notable line\n",
      "of work focus on leveraging LLMs themselves for automatic prompt engineering (Honovich et al.,\n",
      "2023; Zhou et al., 2023b; Pryzant et al., 2023). In our work, we identify potential shortcomings\n",
      "in these methods, subsequently introducing and rigorously examining various meta-prompt compo-\n",
      "nents. Our resulting method PE2 demonstrates superior performance compared to its predecessors.\n",
      "Prompting LLMs for Complex Reasoning Tasks. Recent research works suggest that LLMs\n",
      "can perform complex reasoning tasks, e.g., grade-school math problems (Cobbe et al., 2021). There\n",
      "are two major techniques to boost LLMs’ performance on this: (1) prompting methods that guide\n",
      "the model to produce intermediate reasoning steps, either with few-shot demonstrations (Nye et al.,\n",
      "2021; Wei et al., 2022; Yao et al., 2023) or with zero-shot prompts (Kojima et al., 2022); (2) self-\n",
      "reflection methods that progressively guide the model to inspect its current output and refine it\n",
      "(Chen et al., 2023; Madaan et al., 2023; Paul et al., 2023; Kim et al., 2023). At its core, prompt\n",
      "engineering is a complex language task. Human prompt engineers usually examine the failure cases\n",
      "produced by the current prompt, reason and make hypotheses, and compose a new prompt. In this\n",
      "work, we explore these prompting strategies in building an automatic prompt engineer.\n",
      "Self-training and Self-improving for LLMs. Self-training refers to the technique of using a weak\n",
      "model to annotate input-label pairs and using these pairs to train themselves (Rosenberg et al., 2005).\n",
      "In the context of LLMs, STaR (Zelikman et al., 2022) and Self-Improve (Huang et al., 2022) show\n",
      "that employing LLMs to generate high-quality reasoning chains, followed by model fine-tuning on\n",
      "these chains, can significantly improve the model’s reasoning capabilities. In this work, we consider\n",
      "textual prompts as the “parameters” of LLMs, and we optimize these “parameters” with LLMs. This\n",
      "may be categorized as a case of self-improving (Goodman, 2023), and aligns with the motivations\n",
      "in recent studies (Fernando et al., 2023; Zelikman et al., 2023; Yang et al., 2023).\n",
      "7 C ONCLUSION\n",
      "In this paper, we proposed and identified key components in the meta-prompt that leads to improved\n",
      "performance on automatic prompt engineering. The resulting method, named PE2, not only refines\n",
      "prompts written by human experts, but also surpasses established automatic prompt engineering\n",
      "baselines. Moreover, we showcased PE2’s versatility by applying it to diverse language tasks, no-\n",
      "tably to counterfactual tasks and lengthy production prompts.\n",
      "Prompt engineering a prompt engineer remains an ongoing challenge. As highlighted in our case\n",
      "study, we believe improving the LLM’s instruction following abilities and mitigating hallucination\n",
      "issues will be crucial for improving automatic prompt engineering. Looking ahead, we are also\n",
      "excited about applying PE2 to optimize its own meta-prompt in a self-referential way, in the spirit\n",
      "of Metz et al. (2020); Fernando et al. (2023); Zelikman et al. (2023).\n",
      "9Preprint\n",
      "REFERENCES\n",
      "Howard Chen, Huihan Li, Danqi Chen, and Karthik Narasimhan. Controllable text generation with\n",
      "language constraints. arXiv preprint arXiv:2212.10466 , 2022.\n",
      "Xinyun Chen, Maxwell Lin, Nathanael Sch ¨arli, and Denny Zhou. Teaching large language models\n",
      "to self-debug. arXiv preprint arXiv:2304.05128 , 2023.\n",
      "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\n",
      "Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\n",
      "solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.\n",
      "Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song,\n",
      "Eric Xing, and Zhiting Hu. RLPrompt: Optimizing discrete text prompts with reinforcement\n",
      "learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\n",
      "Processing , pp. 3369–3391, Abu Dhabi, United Arab Emirates, December 2022. Association for\n",
      "Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.222. URL https://aclanthology.\n",
      "org/2022.emnlp-main.222.\n",
      "Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rockt ¨aschel.\n",
      "Promptbreeder: Self-referential self-improvement via prompt evolution. arXiv preprint\n",
      "arXiv:2309.16797 , 2023.\n",
      "Noah Goodman. Meta-prompt: A simple self-improving language agent, 2023. URL https:\n",
      "//noahgoodman.substack.com/p/meta-prompt-a-simple-self-improving.\n",
      "Or Honovich, Uri Shaham, Samuel R. Bowman, and Omer Levy. Instruction induction: From few\n",
      "examples to natural language task descriptions. In Proceedings of the 61st Annual Meeting of the\n",
      "Association for Computational Linguistics (Volume 1: Long Papers) , pp. 1935–1952, Toronto,\n",
      "Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.\n",
      "108. URL https://aclanthology.org/2023.acl-long.108.\n",
      "Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei\n",
      "Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610 , 2022.\n",
      "Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. How can we know what language\n",
      "models know? Transactions of the Association for Computational Linguistics , 8:423–438, 2020.\n",
      "doi: 10.1162/tacl a00324. URL https://aclanthology.org/2020.tacl-1.28.\n",
      "Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks.\n",
      "arXiv preprint arXiv:2303.17491 , 2023.\n",
      "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\n",
      "language models are zero-shot reasoners. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\n",
      "and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022. URL\n",
      "https://openreview.net/forum?id=e2TBb5y0yFf.\n",
      "Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered\n",
      "prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings\n",
      "of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\n",
      "Papers) , pp. 8086–8098, Dublin, Ireland, May 2022. Association for Computational Linguistics.\n",
      "doi: 10.18653/v1/2022.acl-long.556. URL https://aclanthology.org/2022.acl-long.556.\n",
      "Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\n",
      "Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\n",
      "with self-feedback. arXiv preprint arXiv:2303.17651 , 2023.\n",
      "Luke Metz, Niru Maheswaranathan, C Daniel Freeman, Ben Poole, and Jascha Sohl-Dickstein.\n",
      "Tasks, stability, architecture, and compute: Training more effective learned optimizers, and using\n",
      "them to train themselves. arXiv preprint arXiv:2009.11243 , 2020.\n",
      "Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin,\n",
      "David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show\n",
      "your work: Scratchpads for intermediate computation with language models. arXiv preprint\n",
      "arXiv:2112.00114 , 2021.\n",
      "10Preprint\n",
      "OpenAI. Gpt-4 technical report. ArXiv , abs/2303.08774, 2023. URL https://api.semanticscholar.\n",
      "org/CorpusID:257532815.\n",
      "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\n",
      "Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\n",
      "instructions with human feedback. Advances in Neural Information Processing Systems , 35:\n",
      "27730–27744, 2022.\n",
      "Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West,\n",
      "and Boi Faltings. Refiner: Reasoning feedback on intermediate representations. arXiv preprint\n",
      "arXiv:2304.01904 , 2023.\n",
      "Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. GrIPS: Gradient-free, edit-based\n",
      "instruction search for prompting large language models. In Proceedings of the 17th Confer-\n",
      "ence of the European Chapter of the Association for Computational Linguistics , pp. 3845–\n",
      "3864, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. URL https:\n",
      "//aclanthology.org/2023.eacl-main.277.\n",
      "Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt\n",
      "optimization with” gradient descent” and beam search. arXiv preprint arXiv:2305.03495 , 2023.\n",
      "Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks , 12\n",
      "(1):145–151, 1999.\n",
      "Chuck Rosenberg, Martial Hebert, and Henry Schneiderman. Semi-supervised self-training of\n",
      "object detection models. 2005 Seventh IEEE Workshops on Applications of Computer Vision\n",
      "(WACV/MOTION’05) - Volume 1 , 1:29–36, 2005. URL https://api.semanticscholar.org/CorpusID:\n",
      "7648360.\n",
      "Subhro Roy and Dan Roth. Solving general arithmetic word problems. In Proceedings of the 2015\n",
      "Conference on Empirical Methods in Natural Language Processing , pp. 1743–1752, Lisbon, Por-\n",
      "tugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1202.\n",
      "URL https://aclanthology.org/D15-1202.\n",
      "Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and\n",
      "Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. arXiv preprint\n",
      "arXiv:2303.11366 , 2023.\n",
      "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi,\n",
      "Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large lan-\n",
      "guage models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh\n",
      "(eds.), Advances in Neural Information Processing Systems , volume 35, pp. 24824–24837.\n",
      "Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper files/paper/2022/file/\n",
      "9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf.\n",
      "Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Aky ¨urek, Boyuan Chen, Bailin Wang, Najoung Kim,\n",
      "Jacob Andreas, and Yoon Kim. Reasoning or reciting? exploring the capabilities and limitations\n",
      "of language models through counterfactual tasks. arXiv preprint arXiv:2307.02477 , 2023.\n",
      "Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin Yang. Gps:\n",
      "Genetic prompt search for efficient few-shot learning. arXiv preprint arXiv:2210.17041 , 2022.\n",
      "Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun\n",
      "Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409 , 2023.\n",
      "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan\n",
      "Cao. React: Synergizing reasoning and acting in language models. In The Eleventh Interna-\n",
      "tional Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=\n",
      "WE vluYUL-X.\n",
      "Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning\n",
      "with reasoning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh\n",
      "(eds.), Advances in Neural Information Processing Systems , volume 35, pp. 15476–15488.\n",
      "Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper files/paper/2022/file/\n",
      "639a9a172c044fbb64175b5fad42e9a5-Paper-Conference.pdf.\n",
      "11Preprint\n",
      "Eric Zelikman, Eliana Lorch, Lester Mackey, and Adam Tauman Kalai. Self-taught optimizer (stop):\n",
      "Recursively self-improving code generation. arXiv preprint arXiv:2310.02304 , 2023.\n",
      "Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E Gonzalez. Tempera:\n",
      "Test-time prompting via reinforcement learning. arXiv preprint arXiv:2211.11890 , 2022.\n",
      "Tony Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving\n",
      "few-shot performance of language models. In International Conference on Machine Learning ,\n",
      "2021. URL https://api.semanticscholar.org/CorpusID:231979430.\n",
      "Wangchunshu Zhou, Yuchen Eleanor Jiang, Ethan Wilcox, Ryan Cotterell, and Mrinmaya Sachan.\n",
      "Controlled text generation with natural language instructions. In Andreas Krause, Emma Brun-\n",
      "skill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Pro-\n",
      "ceedings of the 40th International Conference on Machine Learning , volume 202 of Proceed-\n",
      "ings of Machine Learning Research , pp. 42602–42613. PMLR, 23–29 Jul 2023a. URL https:\n",
      "//proceedings.mlr.press/v202/zhou23g.html.\n",
      "Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and\n",
      "Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh Interna-\n",
      "tional Conference on Learning Representations , 2023b. URL https://openreview.net/forum?id=\n",
      "92gvk82DE-.\n",
      "A E XTENDED DISCUSSION\n",
      "A.1 D ISCUSSION ON OPRO (Y ANG ET AL ., 2023)\n",
      "A recent work (Yang et al., 2023) introduced the concept of large language models as optimizers\n",
      "and proposed optimization by prompting (OPRO). In the following, we discuss the differences and\n",
      "connections between OPRO and our work.\n",
      "(1) Focus of the work. Both OPRO and our work conduct experiments on prompt optimization;\n",
      "the focus of the two works differ. OPRO can be applied to general optimization problems, includ-\n",
      "ing linear regression and traveling salesman problem. In our work we limit the scope to prompt\n",
      "optimization, with a specific focus on different components of the meta-prompt.\n",
      "(2) Optimization strategy. The optimization strategies of the two works are different. PE2 is\n",
      "largely inspired by the concepts in APO (Pryzant et al., 2023), instructing the model to produce\n",
      "textual feedback (“gradient”) explicitly. It is more analogous to gradient descent. OPRO uses the\n",
      "execution accuracy as rewards to guide the optimization indirectly, which, in our understanding,\n",
      "is more analogous to in-context RL methods (Shinn et al., 2023). For future work, it would be\n",
      "interesting to compare the effectiveness and efficiency of both methods in a controlled setup.\n",
      "(3) Challenges in making direct comparison. Yang et al. (2023) mainly uses PaLM 2-L model and\n",
      "text-bison model as the task model (scorer), and optimizes the prompt for up to 200 steps. In our\n",
      "work, we mainly use text-davinci-003 and GPT-4, and optimize the prompt for 3 steps by default.\n",
      "Due to access and budget constraints, we are unable to make direct comparison with OPRO.\n",
      "(4) Transferability of prompts across models. Yang et al. (2023) found a prompt, “Take a deep\n",
      "breath and work on this problem step-by-step”, that greatly improves performance when using PaLM\n",
      "2-L as the task model. We applied this prompt to text-davinci-003 and we do not observe consistent\n",
      "performance gains. It achieves 84.3% on MultiArith-Test and 63.2% on GSM8K-Test, which is\n",
      "-1.7% and +2.3% compared to “let’s think step by step.”\n",
      "A.2 A DDITIONAL FIGURES AND TABLES\n",
      "We report the reuslts on each subtask in Instruction Induction in Fig. 5. For counterfactual tasks,\n",
      "results using induction initialization is in Fig. 6 and results using manual initialization is in Fig. 7.\n",
      "Additional examples on notable prompt edits made by PE2 are in Table 7.\n",
      "12Preprint\n",
      "antonyms informal_to_formal negation orthography_starts_with rhymes second_word_letter sentence_similarity0.00.20.40.60.81.0Mean ScoreNaive APE\n",
      "Iterative APE\n",
      "APO\n",
      "PE2\n",
      "sentiment synonyms taxonomy_animal translation_en-de translation_en-es translation_en-fr word_in_context0.00.20.40.60.81.0Mean Score\n",
      "Figure 5: Results on the Instruction Induction Benchmark. The performance of APO and PE2 are\n",
      "close to each other on most tasks. Our hypothesis is that tasks in Instruction Induction Benchmark\n",
      "are relatively easier compared to the other benchmarks, leading to performance saturation.\n",
      "Arithmetic:Base-8 Arithmetic:Base-9 Arithmetic:Base-11 Arithmetic:Base-16 Chess:Original Chess:Counterfactual0.00.20.40.60.81.0AccuracyAPE Iterative APE APO PE2\n",
      "Syntax:SVO Syntax:SOV Syntax:OSV Syntax:OVS Syntax:VOS Syntax:VSO0.00.20.40.60.81.0Accuracy\n",
      "Figure 6: Results on counterfactual tasks (induction initialization).\n",
      "Arithmetic:Base-8 Arithmetic:Base-9 Arithmetic:Base-11 Arithmetic:Base-16 Chess:Original Chess:Counterfactual0.00.20.40.60.81.0AccuracyManual Initialization Iterative APE APO PE2\n",
      "Syntax:SVO Syntax:SOV Syntax:OSV Syntax:OVS Syntax:VOS Syntax:VSO0.00.20.40.60.81.0Accuracy\n",
      "Figure 7: Results on counterfactual tasks (manual initialization).\n",
      "13Preprint\n",
      "Table 7: Notable prompt edits made by PE2 (Part 2).\n",
      "Task tPrompt Dev Acc.\n",
      "Correct wrong or incomplete task instructions\n",
      "Antonyms0 Write the opposite of the given word by adding an appropriate prefix. 0.3\n",
      "1 Find the opposite of the given word. If applicable, add or remove an appropriate prefix to\n",
      "form the opposite.0.6\n",
      "Provide more specific context and details\n",
      "Sentence Similarity0 Rate the similarity between Sentence 1 and Sentence 2 on a scale from 1 to 5, with 1 being\n",
      "’probably not similar’ and 5 being ’perfectly similar’.0.0\n",
      "1 Rate the similarity between Sentence 1 and Sentence 2 as ’1 - probably not similar’, ’2 -\n",
      "possibly’, ’3 - moderately’, ’4 - almost perfectly’, or ’5 - perfectly similar’.0.15\n",
      "Produce short-cut solutions in counterfactual tasks\n",
      "Base-9 Addition\n",
      "(induction init.)0 Add the numbers in each input together to get the output. 0.0\n",
      "1 Add the numbers in each input together and then add 11 to get the output. 0.2\n",
      "A.3 S EARCH PROCEDURE\n",
      "See Algorithm 1.\n",
      "Algorithm 1 Search Procedure\n",
      "1:P(0)=PinitorP(0)=Minit(x1, y1, ..., x n, yn;pinit) // Manual init. or induction init.\n",
      "2:fort= 0, ..., T−1do\n",
      "3:P(t+1)=∅\n",
      "4: forp(t)∈Select-Best (∪t\n",
      "i=0P(i), n)do\n",
      "5: forj= 1...m do\n",
      "6: B=Sample (Dtrain) // Sample a batch\n",
      "7: p(t+1)=Moptim (p(t), B;pmeta) // New prompt proposal\n",
      "8: P(t+1)=P(t+1)∪ {p(t+1)}\n",
      "9: end for\n",
      "10: end for\n",
      "11:end for\n",
      "12:return Select-Best (∪T\n",
      "i=0P(i),1)\n",
      "B M ETA-PROMPTS\n",
      "We implement the meta-prompts using the guidance toolkit7, which enables multi-round conversa-\n",
      "tions and supports basic handlebars-style syntax to control the workflow.\n",
      "B.1 I NITIALIZATION PROMPT pinit\n",
      "The initialization prompt is originally from APE (Zhou et al., 2023b). In this paper, it is shared by\n",
      "all methods (Iterative APE, APO and PE2).\n",
      "1{{#system˜ }}\n",
      "2You are a helpful assistant.\n",
      "3{{˜/system }}\n",
      "4\n",
      "5{{#user˜ }}\n",
      "6I gave a friend an instruction and {{n_demo }}inputs. The friend read the instruction and\n",
      "wrote an output for every one of the inputs.\n",
      "7Here are the input-output pairs:\n",
      "8\n",
      "9{{demos}}\n",
      "10\n",
      "11What was the instruction? It has to be less than {{max_tokens }}tokens.\n",
      "12{{˜/user }}\n",
      "13\n",
      "7https://github.com/guidance-ai/guidance\n",
      "14Preprint\n",
      "14{{#assistant˜ }}\n",
      "15The instruction was {{gen ’instruction’ [[GENERATION_CONFIG]] }}\n",
      "16{{˜/assistant }}\n",
      "B.2 APE\n",
      "1{{#system˜ }}\n",
      "2You are a helpful assistant.\n",
      "3{{˜/system }}\n",
      "4\n",
      "5{{#user˜ }}\n",
      "6Generate a variation of the following instruction while keeping the semantic meaning.\n",
      "7\n",
      "8{{prompt }}\n",
      "9\n",
      "10The new instruction has to be less than {{max_tokens }}words.\n",
      "11Reply with the new instruction. Do not include other text.\n",
      "12{{˜/user }}\n",
      "13\n",
      "14{{#assistant˜ }}\n",
      "15{{gen ’new_prompt’ [[GENERATION_CONFIG]] }}\n",
      "16{{˜/assistant }}\n",
      "B.3 APO\n",
      "Part 1 - Generating “gradients”\n",
      "1{{#system˜ }}\n",
      "2You are a helpful assistant.\n",
      "3{{/system˜ }}\n",
      "4\n",
      "5{{#user˜ }}\n",
      "6I’m trying to write a zero-shot classifier prompt.\n",
      "7\n",
      "8My current prompt is:\n",
      "9\"{{prompt }}\"\n",
      "10\n",
      "11But this prompt gets the following examples wrong:\n",
      "12{{failure_string }}\n",
      "13\n",
      "14Give{{n_reasons }}reasons why the prompt could have gotten these examples wrong. Do not\n",
      "include other text.\n",
      "15{{/user˜ }}\n",
      "16\n",
      "17{{#assistant˜ }}\n",
      "18{{gen ’gradients’ temperature=0.0 }}\n",
      "19{{/assistant˜ }}\n",
      "Part 2 - Refining the prompt\n",
      "1{{#system˜ }}\n",
      "2You are a helpful assistant.\n",
      "3{{/system˜ }}\n",
      "4\n",
      "5{{#user˜ }}\n",
      "6I’m trying to write a zero-shot classifier.\n",
      "7\n",
      "8My current prompt is:\n",
      "9\"{{prompt }}\"\n",
      "10\n",
      "11But it gets the following examples wrong:\n",
      "12{{failure_string }}\n",
      "13\n",
      "14Based on these examples the problem with this prompt is that:\n",
      "15{{gradient }}\n",
      "16\n",
      "17Based on the above information, I wrote an improved prompt. The total length of the prompt\n",
      "should be less than {{max_tokens }}words.\n",
      "18{{/user˜ }}\n",
      "19\n",
      "20{{#assistant˜ }}\n",
      "21The improved prompt is {{gen ’new_prompt’ temperature=0.0 }}\n",
      "22{{/assistant˜ }}\n",
      "15Preprint\n",
      "B.4 PE2\n",
      "1{{#system˜ }}\n",
      "2You are a helpful assistant.\n",
      "3{{˜/system }}\n",
      "4\n",
      "5{{#if instruction }}\n",
      "6{{#user˜ }}\n",
      "7Let’s read a blogpost on prompt engineering:\n",
      "8{{instruction }}\n",
      "9{{˜/user }}\n",
      "10{{/if}}\n",
      "11\n",
      "12{{#user˜ }}\n",
      "13A prompt is a text paragraph that outlines the expected actions and instructs the model to\n",
      "generate a specific output. This prompt is concatenated with the input text, and the\n",
      "model then creates the required output.\n",
      "14\n",
      "15In our collaboration, we’ll work together to refine a prompt. The process consists of two main\n",
      "steps:\n",
      "16\n",
      "17## Step 1\n",
      "18I will provide you with the current prompt, how the prompt is concatenated with the input text\n",
      "(i.e., \"full template\"), along with {{batch_size }}example(s) that are associated with\n",
      "this prompt. Each examples contains the input, the reasoning process generated by the\n",
      "model when the prompt is attached, the final answer produced by the model, and the ground\n",
      "-truth label to the input. Your task is to analyze the examples, determining whether the\n",
      "existing prompt is decsribing the task reflected by these examples precisely, and suggest\n",
      "changes to the prompt.\n",
      "19\n",
      "20## Step 2\n",
      "21Next, you will carefully review your reasoning in step 1, integrate the insights to craft a\n",
      "new, optimized prompt. Optionally, the history of refinements made to this prompt from\n",
      "past sessions will be included. Some extra instructions (e.g., the number of words you\n",
      "can edit) will be provided too.\n",
      "22{{˜/user }}\n",
      "23\n",
      "24{{#assistant˜ }}\n",
      "25Sure, I’d be happy to help you with this prompt engineering problem.\n",
      "26Please provide me with the prompt engineering history, the current prompt, and the examples\n",
      "you have.\n",
      "27{{˜/assistant }}\n",
      "28\n",
      "29{{#user˜ }}\n",
      "30## Prompt\n",
      "31{{prompt }}\n",
      "32\n",
      "33## Full Template\n",
      "34This describes how the prompt of interested is concatenated with the input text.\n",
      "35The prompt may appear before the input text, or after the input text.\n",
      "36Optionally the full template may contain other template information.\n",
      "37‘‘‘\n",
      "38{{full_prompt }}\n",
      "39‘‘‘\n",
      "40\n",
      "41## Examples\n",
      "42{{examples }}\n",
      "43\n",
      "44## Instructions\n",
      "45For some of these examples, the output does not match with the label. This may be due to the\n",
      "prompt being misleading or not describing the task precisely.\n",
      "46\n",
      "47Please examine the example(s) carefully. Note that the ground-truth labels are __absolutely\n",
      "correct__, but the prompts (task descriptions) may be incorrect and need modification.\n",
      "For each example, provide reasoning according to the following template:\n",
      "48\n",
      "49### Example <id>\n",
      "50Input: <input>\n",
      "51Output: <output>\n",
      "52Label: <label>\n",
      "53Is the output correct compared to the label: <yes or no, and your reasoning>\n",
      "54Is the output correctly following the given prompt: <yes or no, and your reasoning>\n",
      "55Is the prompt correctly describing the task shown by the input-label pair: <yes or no, and\n",
      "your reasoning>\n",
      "56To output the correct label, is it necessary to edit the prompt: <yes or no, and your\n",
      "reasoning>\n",
      "57If yes, provide detailed analysis and actionable suggestions to edit the prompt: <analysis and\n",
      "suggestions>\n",
      "58{{˜/user }}\n",
      "59\n",
      "16Preprint\n",
      "60{{#assistant˜ }}\n",
      "61{{gen ’reasoning’ temperature=0 }}\n",
      "62{{˜/assistant }}\n",
      "63\n",
      "64{{#user˜ }}\n",
      "65Now please carefully review your reasoning in Step 1 and help with Step 2: refining the prompt\n",
      ".\n",
      "66\n",
      "67{{#if history }}\n",
      "68## Prompt Refinement History from the Past\n",
      "69Note that higher accuracy means better. If some edits are useful in the past, it may be a good\n",
      "idea to make edits along the same direction.\n",
      "70{{history }}\n",
      "71{{/if}}\n",
      "72\n",
      "73## Current Prompt\n",
      "74{{prompt }}\n",
      "75\n",
      "76## Instructions\n",
      "77{{#if step_size }}\n",
      "78*You are allowed to change up to {{step_size }}words in the original prompt.\n",
      "79{{/if}}\n",
      "80{{#if max_tokens }}\n",
      "81*The total length of the prompt should be less than {{max_tokens }}words.\n",
      "82{{/if}}\n",
      "83*Please help edit the prompt so that the updated prompt will not fail on these examples\n",
      "anymore.\n",
      "84*Reply with the prompt. Do not include other text.\n",
      "85{{˜/user }}\n",
      "86\n",
      "87{{#assistant˜ }}\n",
      "88{{gen ’new_prompt’ temperature=0.7 max_tokens=300 }}\n",
      "89{{˜/assistant }}\n",
      "90\n",
      "91{{#if history }}\n",
      "92{{#user˜ }}\n",
      "93Now please summarize what changes you’ve made to the prompt, in the following format. Make\n",
      "sure the summariy is concise and contains no more than 200 words.\n",
      "94\n",
      "95\"*At step {{timestamp }}, the prompt has limitations such as <summary of limitations>.\n",
      "Changes to the prompt include <summary of changes>.\"\n",
      "96\n",
      "97Reply with the summarization. Do not include other text.\n",
      "98{{˜/user }}\n",
      "99\n",
      "100{{#assistant˜ }}\n",
      "101{{gen ’new_history’ temperature=0.7 max_tokens=200 }}\n",
      "102{{˜/assistant }}\n",
      "103{{/if}}\n",
      "C L ONG PROMPT OPTIMIZATION\n",
      "We conduct additional experiments on the long production prompt, to better understand the sensitiv-\n",
      "ity of PE2 to hyper-parameters for long prompts. Note that in the other three tasks (math reasoning,\n",
      "instruction induction, counterfactual eval) the prompts are typically shorter than 200 tokens, while\n",
      "the production prompt has 5k+ tokens, which calls for a separate investigation.\n",
      "C.1 A BLATION FOR LONG PROMPT OPTIMIZATION\n",
      "We looked at the impact on the final test score when modifying the set of optimization parameters;\n",
      "batch size ∈ {3,10}andstep size ∈ {10,40,50}given in Section 3 and the LLM compute m∈\n",
      "{2,4}andsearch budget n∈ {2,4}given in Section 4.2.\n",
      "Dataset: We first created a smaller evaluation dataset from the full production prompt dataset,\n",
      "which consists of the instructions for <50% of the Domains given in the original data, and a small\n",
      "uniform sub-sample ( <20%) of the questions for these domains, we did not modify the k-shot\n",
      "examples used to the align them to the subset of Domains used.\n",
      "We ran 3-fold cross-validation using the evaluation dataset with train ( 40%), validation ( 30%) and\n",
      "testing ( 30%). Figure 8 reports the absolute change in F1-score at each time-step for a total of up to\n",
      "10steps in each experiment.\n",
      "17Preprint\n",
      "The highest performing prompt generated by PE2 improves the F1 test score by +11 points, and\n",
      "the overall trend is a positive and significant (Pearson correlation r= 0.55, p= 1.68e−5), there is\n",
      "however a significant margin between the maximum ( 0.10) and the mean ( 0.02±0.11) change in\n",
      "the score.\n",
      "13 15 17 19 1b 1413\n",
      "574c50480356574853030b570c13111418\n",
      "13111413\n",
      "13111318\n",
      "1311131313111318131114131311141813111513464b44514a48034c51032914035646525548034955525003572013\n",
      "0355201311131b0f035320141117184810131756534f4c570320035755444c51\n",
      "13 15 17 19 1b 1413\n",
      "574c50480356574853030b570c\n",
      "035520131116170f03532014111a134810191456534f4c57032003474859\n",
      "13 15 17 19 1b 1413\n",
      "574c50480356574853030b570c\n",
      "033e50445b20131114140f035048445120131113150f03565747201311141440031003035520131118180f0353201411191b4810131856534f4c5703200357485657\n",
      "Figure 8: The distribution and trend of the performance of PE2 across the 3-fold cross validation.\n",
      "C.1.1 I MPACT OF OPTIMIZATION PARAMETERS\n",
      "Figure 9 gives the ablation results for varying the batch size (Figure 9(a)) and step size (Figure 9(b)).\n",
      "Thebatch size controls the number of examples the LLM can inspect in order to diagnose the error\n",
      "and suggest a solution, while the step size controls the magnitude of a change the LLM can make to\n",
      "the text.\n",
      "With batch size ∈ {3,10}, we observe that larger batch sizes have a more significant impact on\n",
      "improving the performance on the test set. From our ablation, although the best performing prompts\n",
      "in the two cases have the same score, for batch size =10, the Pearson correlation is r= 0.59, p=\n",
      "2.46−4vsr= 0.49, p= 2.84e−2forbatch size =3.\n",
      "The impact of the step size ∈ {10,40,50}appears to be a little more noisy with our task. The best\n",
      "result is from step size =50, but the most significant result is observed with with step size =40with\n",
      "Pearson correlation, r= 0.52, p= 4.2e−3, which is an order of magnitude smaller than the other\n",
      "two settings, step size ∈ {10,50}, where step size =50achieves a better score.\n",
      "13 15 17 19 1b 1413\n",
      "574c50480356574853030b570c13111418\n",
      "13111413\n",
      "13111318\n",
      "1311131313111318131114131311141813111513464b44514a48034c51032914035646525548034955525003572013\n",
      "454457464b42564c5d481d0316035520131115150f035320141119174810131c\n",
      "454457464b42564c5d481d031413035520131113140f0353201b1114194810131456534f4c570320035755444c51\n",
      "13 15 17 19 1b 1413\n",
      "574c50480356574853030b570c\n",
      "454457464b42564c5d481d0316035520131117130f0353201511171a48101613\n",
      "454457464b42564c5d481d031413035520131116140f035320151113194810161656534f4c57032003474859\n",
      "13 15 17 19 1b 1413\n",
      "574c50480356574853030b570c\n",
      "454457464b42564c5d481d0316033e50445b20131114130f035048445120131113150f03565747201311141340031003160355201311171c0f03532015111b1748101315\n",
      "454457464b42564c5d481d031413033e50445b20131114140f035048445120131113150f0356574720131114144003100314130355201311181c0f035320151117194810131756534f4c5703200357485657\n",
      "454457464b42564c5d48\n",
      "16\n",
      "1413\n",
      "(a)batch size ∈ {3,10}.\n",
      "13 15 17 19 1b 1413\n",
      "574c50480356574853030b570c13111418\n",
      "13111413\n",
      "13111318\n",
      "1311131313111318131114131311141813111513464b44514a48034c51032914035646525548034955525003572013\n",
      "5657485342564c5d481d03141303552010131114190f0353201511131348101317\n",
      "5657485342564c5d481d031713035520131113160f0353201611181a48101314\n",
      "5657485342564c5d481d031813035520131115170f035320151113164810141556534f4c570320035755444c51\n",
      "13 15 17 19 1b 1413\n",
      "574c50480356574853030b570c\n",
      "5657485342564c5d481d031413035520131116170f0353201711141848101419\n",
      "5657485342564c5d481d0317130355201311151a0f0353201c11171a48101418\n",
      "5657485342564c5d481d031813035520131117150f0353201611151c4810161956534f4c57032003474859\n",
      "13 15 17 19 1b 1413\n",
      "574c50480356574853030b570c\n",
      "5657485342564c5d481d031413033e50445b201311131b0f035048445120131113150f03565747201311131b400310031413035520131119180f0353201b11141c48101315\n",
      "5657485342564c5d481d031713033e50445b20131114130f035048445120131113150f035657472013111413400310031713035520131118150f0353201711151448101316\n",
      "5657485342564c5d481d031813033e50445b20131114140f035048445120131113150f035657472013111414400310031813035520131118180f03532014111a174810131556534f4c5703200357485657\n",
      "5657485342564c5d48\n",
      "1413\n",
      "1713\n",
      "1813\n",
      "(b)step size ∈ {10,40,50}.\n",
      "Figure 9: The impact of varying the optimization parameters batch size andstep size . Figures\n",
      "report the Pearson coefficient ( r) and the p-value ( p) for each grouping. For the test split, Figures\n",
      "additionally report the score of the best performing prompt in the set ( max ), the mean score ( mean )\n",
      "of the prompts and its standard deviation ( std).\n",
      "18Preprint\n",
      "C.1.2 I MPACT OF COMPUTE PARAMETERS\n",
      "Figure 10 gives the ablation results for varying nandm, two hyperparameters defined in §4.2.\n",
      "ncontrols the number of candidate prompts selected for generating new prompts, while the m\n",
      "determines the number of new prompts each candidate generates. Together these two parameters\n",
      "enable the optimization process to i) inspects n-more candidate solutions and ii) generate n×m\n",
      "more candidate solutions for evaluation in the next step.\n",
      "For both parameters, improvement in test performance is positively correlated with increasing the\n",
      "value, however, increasing the Search budget ( n) has a more significant p-value; r= 0.60, p=\n",
      "1.73e−4forn= 4vsr= 0.62, p= 3.59e−3form= 4.\n",
      "13 15 17 19 1b 1413\n",
      "574c50480356574853030b570c13111418\n",
      "13111413\n",
      "13111318\n",
      "1311131313111318131114131311141813111513464b44514a48034c51032914035646525548034955525003572013\n",
      "511d0315035520131113140f0353201b11131448101314\n",
      "511d0317035520131114130f0353201511191a4810131856534f4c570320035755444c51\n",
      "13 15 17 19 1b 1413\n",
      "574c50480356574853030b570c\n",
      "511d03150355201311161b0f0353201511161348101418\n",
      "511d0317035520131116170f0353201c1117154810171b56534f4c57032003474859\n",
      "13 15 17 19 1b 1413\n",
      "574c50480356574853030b570c\n",
      "511d0315033e50445b20131114130f035048445120131113140f0356574720131114134003100315035520131117190f0353201711131748101315\n",
      "511d0317033e50445b20131114140f035048445120131113150f0356574720131114144003100317035520131119130f03532014111a164810131756534f4c5703200357485657\n",
      "51\n",
      "15\n",
      "17\n",
      "(a)n∈ {2,4}.\n",
      "13 15 17 19 1b 1413\n",
      "574c50480356574853030b570c13111418\n",
      "13111413\n",
      "13111318\n",
      "1311131313111318131114131311141813111513464b44514a48034c51032914035646525548034955525003572013\n",
      "501d031503552010131113140f0353201a11161c48101314\n",
      "501d0317035520131114180f0353201a11141a4810131b56534f4c570320035755444c51\n",
      "13 15 17 19 1b 1413\n",
      "574c50480356574853030b570c\n",
      "501d0315035520131116150f03532017111a1348101515\n",
      "501d0317035520131116190f0353201b11151c4810171656534f4c57032003474859\n",
      "13 15 17 19 1b 1413\n",
      "574c50480356574853030b570c\n",
      "501d0315033e50445b20131114130f035048445120131113140f0356574720131114134003100315035520131118140f0353201511141648101316\n",
      "501d0317033e50445b20131114140f035048445120131113150f0356574720131114144003100317035520131119150f0353201611181c4810131656534f4c5703200357485657\n",
      "50\n",
      "15\n",
      "17\n",
      "(b)m∈ {2,4}.\n",
      "Figure 10: The impact of varying nandm. Figures report the Pearson coefficient ( r) and the p-value\n",
      "(p) for each grouping. For the test split, Figures additionally report the score of the best performing\n",
      "prompt in the set ( max ), the mean score ( mean ) of the prompts and its standard deviation ( std).\n",
      "C.2 P ERFORMANCE ON THE COMPLETE PRODUCTION PROMPT\n",
      "Experiment configuration: We ran the a 2-fold cross validation with on the full production prompt\n",
      "dataset described in Section 4.1. This is the setting that we report in Fig. 1.\n",
      "The data was divided between training ( 50%), validation ( 25%) and testing ( 25%).step size is200,\n",
      "batch size is15, with random selection of batch items. The number of candidate prompts inspected\n",
      "at each step is n∈ {2,4}, the number of new prompts generated per candidate m= 2.\n",
      "We use the F1-score for evaluating model outputs and report the absolute change in score with the\n",
      "initial prompt after optimizing for up to 10time steps.\n",
      "Figure 11(a) gives the results for our full long-prompt optimization. The overall test performance\n",
      "shows a positive correlation the score difference; Pearson correlation r= 0.47, p= 2.39e−1, but\n",
      "the trend is not significant. Figure 11(b) separates the results between n∈ {2,4}, and see that for\n",
      "n= 4the Pearson correlation is positive ( r= 0.82, p= 1.82e−1), while for n= 2the correlation\n",
      "is negative ( r=−0.22, p= 7.77e−1).\n",
      "D R EPRODUCIBILITY\n",
      "D.1 T ASKS AND DATA\n",
      "We summarize the dataset size and data split information in Table 8. We summarize the source and\n",
      "license information of the datasets in Table 9.\n",
      "19Preprint\n",
      "13 15 17 19 1b 1413\n",
      "574c50480356574853030b570c13111318\n",
      "13111313131113181311141313111418464b44514a48034c51032914035646525548034955525003572013\n",
      "0355201311131c0f0353201511171c4810131456534f4c570320035755444c51\n",
      "13 15 17 19 1b 1413\n",
      "574c50480356574853030b570c\n",
      "03552010131113180f03532017111a1a4810131456534f4c57032003474859\n",
      "13 15 17 19 1b 1413\n",
      "574c50480356574853030b570c\n",
      "033e50445b201311131b0f035048445120131113140f03565747201311131b400310030355201311171a0f0353201511161c4810131456534f4c5703200357485657\n",
      "(a) The over all performance of PE2 on the full production prompt and dataset. 2-fold cross-\n",
      "validation was used to obtain results with n∈2,4\n",
      ".\n",
      "13 15 17 19 1b 1413\n",
      "574c50480356574853030b570c13111318\n",
      "13111313131113181311141313111418464b44514a48034c51032914035646525548034955525003572013\n",
      "511d0315035520131114160f0353201611141548101314\n",
      "511d0317035520131114190f0353201b11151b4810131556534f4c570320035755444c51\n",
      "13 15 17 19 1b 1413\n",
      "574c50480356574853030b570c\n",
      "511d031503552010131113140f0353201c11171c48101314\n",
      "511d03170355201311131b0f0353201611171c4810131456534f4c57032003474859\n",
      "13 15 17 19 1b 1413\n",
      "574c50480356574853030b570c\n",
      "511d0315033e50445b20131113130f03504844512010131113130f035657472013111313400310031503552010131115150f0353201a111a1a48101314\n",
      "511d0317033e50445b201311131b0f035048445120131113160f03565747201311131b400310031703552013111b150f03532014111b154810131456534f4c5703200357485657\n",
      "51\n",
      "15\n",
      "17\n",
      "(b) The impact of the Search budget ( n). We see that increasing the candidate solution space PE2 can\n",
      "expand from n= 2ton= 4strongly changes the direction of test the correlation and significantly\n",
      "improves the result.\n",
      "Figure 11: Large Prompt Optimization with PE2. Figures report the Pearson coefficient ( r) and\n",
      "the p-value ( p) for each grouping. For the test split, Figures additionally report the score of the\n",
      "best performing prompt in the set ( max ), the mean score ( mean ) of the prompts and its standard\n",
      "deviation ( std).\n",
      "(1) Mathematical Reasoning. The MultiArith dataset (Roy & Roth, 2015) contains 600 examples.\n",
      "As our prompt optimization method requires a training set, we randomly split into 100/100/400 for\n",
      "train/dev/test. This creates a slight discrepancy when comparing the results with past reported re-\n",
      "sults. We ensure our reproduction is fair across different methods by using this fixed split. The\n",
      "GSM8K dataset (Cobbe et al., 2021) has a provided test split (1319 examples). We randomly se-\n",
      "lected 200 examples for the original train split, and use 100 as Dtrain and 100 as Ddev.\n",
      "(2) Instruction Induction. We closely follow the settings in (Zhou et al., 2023b). For each sub-\n",
      "task, we randomly sample 5 different Dtrain/Ddev/Dtestof size 100/20/100. We list the sub-tasks\n",
      "in Instruction Induction benchmark in Table 10. We removed 8 tasks (active to passive, diff, first\n",
      "word letter, letters list, num to verbal, singular to plural, sum), because our baseline method APE\n",
      "Zhou et al. (2023b) already achieves near perfect accuracies (95%+) on these tasks. We also re-\n",
      "moved 2 tasks (cause and effect, common concept) because they have less than 50 examples in total,\n",
      "and it is challenging to create train/dev/test split from these examples.\n",
      "(3) Counterfactual Evaluation. We use three subtasks in this evaluation suite: arithmetic,\n",
      "chess and syntax. For each subtask, we randomly sample 5 different Dtrain/Ddev/Dtestof size\n",
      "100/20/100.\n",
      "(4) Production Prompt. We use a randomly sampled sub-set of human annotated queries and\n",
      "labels ( >150), which are derived from user reported errors. The data is divided between training\n",
      "(50%), validation ( 25%) and testing ( 25%). We use the F1-score for evaluating model outputs and\n",
      "report the absolute change in score with the initialization prompt. More details in §C.\n",
      "20Preprint\n",
      "Dataset Subtasks |Ttrain| |Tdev| |Ttest|# Random Samples\n",
      "MultiArith (Roy & Roth, 2015) - 100 100 400 1\n",
      "GSM8K (Cobbe et al., 2021) - 100 100 1319 1\n",
      "Instruction Induction (Honovich et al., 2023) 14 Subtasks 100 20 100 5\n",
      "Counterfactual Eval (Wu et al., 2023) 12 Subtasks 100 20 100 5\n",
      "Table 8: Dataset sizes and data splits.\n",
      "Dataset License Source\n",
      "MultiArith (Roy & Roth, 2015) Unknown https://github.com/wangxr14/Algebraic-Word-Problem-Solver/\n",
      "GSM8K (Cobbe et al., 2021) MIT https://github.com/openai/grade-school-math\n",
      "Instruction Induction (Honovich et al., 2023) Apache-2.0 https://github.com/orhonovich/instruction-induction\n",
      "Counterfactual Eval (Wu et al., 2023) Unknown https://github.com/ZhaofengWu/counterfactual-evaluation\n",
      "Table 9: License and Source of the datasets used in this study.\n",
      "D.2 E XTENDED EXPERIMENT DETAILS\n",
      "By default the max length of prompts are set to be 50 tokens, following Zhou et al. (2023b). For\n",
      "counterfactual tasks, to allow more space to explain the counterfactual situations, the max length is\n",
      "set to be 200 tokens.\n",
      "D.3 P ROMPTS FOUND BY EACH METHOD\n",
      "See Table 12-17.\n",
      "21Preprint\n",
      "Task Instruction Demonstration\n",
      "Subtasks used in this work (14)\n",
      "Second Letter Extract the first letter of the input word. cat →a\n",
      "Starting With Extract the words starting with a given letter from the input\n",
      "sentence.The man whose car I hit last week sued me. [m] →man, me\n",
      "Negation Negate the input sentence. Time is finite →Time is not finite.\n",
      "Antonyms Write a word that means the opposite of the input word. won →lost\n",
      "Synonyms Write a word with a similar meaning to the input word. alleged →supposed\n",
      "Membership Write all the animals that appear in the given list. cat, helicopter, cook, whale, frog, lion →frog, cat, lion,\n",
      "whale\n",
      "Rhymes Write a word that rhymes with the input word. sing →ring\n",
      "Informal to Formal Rephrase the sentence in formal language. Please call once you get there →Please call upon your ar-\n",
      "rival.\n",
      "Translation EN-DE Translate the word into German. game →spiel\n",
      "Translation EN-ES Translate the word into Spanish. game →jeugo\n",
      "Translation EN-FR Translate the word into French. game →jeu\n",
      "Sentiment Determine whether a movie review is positive or negative. The film is small in scope, yet perfectly formed. →positive\n",
      "Sentence Similarity Rate the semantic similarity of two sentences on a scale of\n",
      "0 to 5Sentence 1: A man is smoking. Sentence 2: A man is skat-\n",
      "ing.→0 - definitely not\n",
      "Word in Context Determine whether an input word has the same meaning in\n",
      "the two sentences.Sentence 1: Approach a task. Sentence 2: To approach the\n",
      "city. Word: approach →not the same\n",
      "Subtasks removed due to near-perfect accuracy (95%+) with baseline method (8)\n",
      "First Letter Extract the first letter of the input word. cat →c\n",
      "List Letters Break the input word into letters, separated by spaces. cat →c a t\n",
      "Singular to Plural Convert the input word to its plural form. cat →cats\n",
      "Active to Passive Write the input sentence in passive form. The artist introduced the scientist. →The scientist was in-\n",
      "troduced by the artist.\n",
      "Larger Animal Write the larger of the two given animals. koala, snail →koala\n",
      "Sum Sum the two given numbers. 22 10 →32\n",
      "Diff Subtract the second number from the first. 32 22 →10\n",
      "Number to Word Write the number in English words. 26 →twenty-six\n",
      "Subtask removed due to small dataset size (2)\n",
      "Cause and Effect Find which of the two given cause and effect sentences is\n",
      "the cause.Sentence 1: The soda went flat. Sentence 2: The bottle was\n",
      "left open. →The bottle was left open.\n",
      "Common Concept Find a common characteristic for the given objects. guitars, pendulums, neutrinos →involve oscillations.\n",
      "Table 10: Details of Instruction Induction dataset. Adapted from Table 4 in Honovich et al. (2023).\n",
      "Task Category Demonstration\n",
      "Arithmetic - Two-digit addition\n",
      "Base-10 Original 22+10 →32\n",
      "Base-8 Counterfactual 76+76 →174\n",
      "Base-9 Counterfactual 76+14 →101\n",
      "Base-11 Counterfactual 76+14 →8A\n",
      "Base-16 Counterfactual EC+DD →1C9\n",
      "Chess - Legality of a 4-move opening\n",
      "Normal Rules Original 1. g3 Ng6 2. b3 Kf8 * →illegal\n",
      "Swapping bishops and knights Counterfactual 1. g3 Ng6 2. b3 Kf8 * →legal\n",
      "Syntax - Identify the main subject and the main verb of a sentence\n",
      "SVO Original he has good control . →he has\n",
      "SOV Counterfactual he good control has . →he has\n",
      "VSO Counterfactual has he good control . →he has\n",
      "VOS Counterfactual has good control he . →he has\n",
      "OVS Counterfactual good control has he . →he has\n",
      "OSV Counterfactual good control he has . →he has\n",
      "Table 11: Details of Couterfactual Evaluation dataset.\n",
      "22Preprint\n",
      "Table 12: Prompts find by prompt optimization methods. For instruction induction, experiments\n",
      "were run with 5 random data splits; In this table we report the prompts found in one run (seed=0).\n",
      "Task Method Prompt\n",
      "Math Reasoning\n",
      "MultiArithZero-shot CoT Let’s think step by step.\n",
      "APE Let’s work this out in a step by step way to be sure we have the right answer.\n",
      "Iterative APE Let’s proceed in a methodical, step-by-step manner.\n",
      "APO Given the scenario, perform the necessary calculations step by step to find the final result.\n",
      "Consider all parts of the input and the sequence of events.\n",
      "PE2 Let’s solve this problem by considering all the details. Pay attention to each piece of\n",
      "information, remember to add or subtract as needed, and perform the calculations step by\n",
      "step.\n",
      "GSM8KZero-shot CoT Let’s think step by step.\n",
      "APE Let’s work this out in a step by step way to be sure we have the right answer.\n",
      "Iterative APE Let’s dissect this and tackle it gradually, one phase at a time.\n",
      "APO Given the scenario, perform necessary calculations and provide a step-by-step explanation\n",
      "to arrive at the correct numerical answer. Consider all information provided.\n",
      "PE2 Let’s solve the problem step-by-step and calculate the required total value correctly.\n",
      "Instruction Induction\n",
      "antonymsAPO Provide the opposite or a negative form of the given input word.\n",
      "PE2 Provide the opposite or a negative form of the given input word.\n",
      "informal toformalAPO Convert each sentence into a formal version, preserving the original structure, meaning,\n",
      "and tone. Avoid excessive formality, unnecessary changes, and maintain idiomatic expres-\n",
      "sions. Handle contractions appropriately.\n",
      "PE2 Please transform each sentence into a version that maintains the original meaning but is\n",
      "expressed in a more formal or polite manner.\n",
      "negationAPO Negate the statement given in the input.\n",
      "PE2 Negate the statement given in the input.\n",
      "orthography starts withAPO Identify the word or phrase in the sentence that starts with the given letter, considering the\n",
      "context and grammar. Include articles if they precede the word or phrase.\n",
      "PE2 Find the word or phrase in the sentence that starts with the given letter, and write it as the\n",
      "output.\n",
      "rhymesAPO Remove the first letter of the given word. Find a word that rhymes with the remaining\n",
      "part, has the same syllable count, and is not a derivative or the same as the original word.\n",
      "PE2 Generate a word that rhymes with the given word.\n",
      "second word letterAPO Identify the second character from the start in each input word and provide it as the output.\n",
      "PE2 Identify the second character from the start of the given word.\n",
      "sentence similarityAPO Rate the similarity between Sentence 1 and Sentence 2 using the scale: 1 - ’probably not’,\n",
      "2 - ’possibly’, 3 - ’probably’, 4 - ’likely’, 5 - ’perfectly’.\n",
      "PE2 Rate the similarity between Sentence 1 and Sentence 2 using the scale: 1 - ’probably not’,\n",
      "2 - ’possibly’, 3 - ’probably’, 4 - ’likely’, 5 - ’perfectly’.\n",
      "sentimentAPO Determine if the given movie review statement is positive or negative.\n",
      "PE2 Determine if the given movie review statement is positive or negative.\n",
      "synonymsAPO Provide a single word that is closely related to the given input, considering its most com-\n",
      "mon usage.\n",
      "PE2 Identify a word that is closely connected, in meaning or context, with the provided input\n",
      "word.\n",
      "taxonomy animalAPO Remove all items from the list that are not animals.\n",
      "PE2 Remove all items from the list that are not animals.\n",
      "translation en-deAPO Translate each English word into German.\n",
      "PE2 Translate each English word into German.\n",
      "translation en-esAPO Provide the most commonly used Spanish translation for the given English word.\n",
      "PE2 Translate the given term from English to Spanish. Note that the translation may be a single\n",
      "word or a phrase.\n",
      "translation en-frAPO Provide the French equivalent for the given English word.\n",
      "PE2 Translate the following word from English to its most common equivalent in French.\n",
      "word incontextAPO Determine if the word provided is used in the same sense/context in both sentences. If it\n",
      "is, write ’same.’ If not, write ’not the same.’\n",
      "PE2 Determine if the word provided is used in the same sense/context in both sentences. If it\n",
      "is, write ’same.’ If not, write ’not the same.’\n",
      "23Preprint\n",
      "Table 13: Prompts find by prompt optimization methods. Experiments were run with 5 random data\n",
      "splits; In this table we report the prompts found in one run (seed=0).\n",
      "Task Method Prompt\n",
      "Counterfactual Evaluation (Induction Initialization)\n",
      "arithmetic base11APO Given two numbers in hexadecimal format (0-9, A-F), convert each number to decimal. Add the two\n",
      "decimal numbers together. Output the sum in hexadecimal format. If the sum exceeds the range of a\n",
      "single hexadecimal digit (0-F), represent it appropriately in hexadecimal. For example, if the input\n",
      "is ’A’ and ’B’, the output should be ’15’ as ’A’ is 10 and ’B’ is 11 in decimal, and their sum is 21\n",
      "which is ’15’ in hexadecimal.\n",
      "PE2 Convert both numbers in each pair from hexadecimal to decimal, then add them together. Output\n",
      "the resultant sum in hexadecimal. For instance, if the input is A4+61, convert A4 and 61 to decimal\n",
      "(164 and 97 respectively), add them together to get 261, and convert this back to hexadecimal to get\n",
      "105.\n",
      "arithmetic base16APO Given two hexadecimal numbers as input, add them together using base 16 arithmetic. The input\n",
      "hexadecimal numbers will be in uppercase and may have different number of digits. Align the num-\n",
      "bers from right to left, similar to traditional addition, and handle any overflow or carry appropriately.\n",
      "Output the sum as an uppercase hexadecimal number.\n",
      "PE2 Add the input hexadecimal numbers together and output the sum as a hexadecimal number. For\n",
      "example, if the input is ”44+E7”, the output should be ”12B”, because the sum of hexadecimals 44\n",
      "and E7 equals 12B in hexadecimal.\n",
      "arithmetic base8APO Given an input string containing two numbers separated by a ’+’, calculate the sum of these two\n",
      "numbers. Then, add 20 to this sum to get the output. For example, if the input is ’22+47’, first add\n",
      "22 and 47 to get 69, then add 20 to 69 to get the final output of 89. Similarly, if the input is ’74+26’,\n",
      "first add 74 and 26 to get 100, then add 20 to 100 to get the final output of 120. The ’+’ symbol\n",
      "should be interpreted as an addition operator, and the order of operations should be to add the two\n",
      "numbers first, then add 20 to the sum. The input will always be formatted correctly, with no spaces\n",
      "or other characters around the ’+’ symbol.\n",
      "PE2 To find the correct output, first add the two numbers given as input. Once you have the sum of these\n",
      "two numbers, add an additional 22 to this sum. For example, if the input is ”17+65”, you should\n",
      "first add 17 and 65 to get 82, then add 22 to 82. The correct output in this case would be 104.\n",
      "arithmetic base9APO Add the numbers together.\n",
      "PE2 Add the two numbers given as input and then add 10 to the result to generate the output. For example,\n",
      "if the input is ’25+18’, the output should be ’53’ because 25 plus 18 equals 43, and adding 10 gives\n",
      "53.\n",
      "chess cfAPO Determine if the given sequence of chess moves, starting from the initial game position, is legal or\n",
      "not according to the standard rules of chess. Consider the unique movements and restrictions of\n",
      "each piece, the alternating turns of the players (white and black), and the entire game state up to the\n",
      "given point. Evaluate the sequence as a whole, not just individual moves. Note that the sequence\n",
      "ends with an asterisk (*).\n",
      "PE2 Please assess the legality of the following sequence of chess moves based on standard chess rules. If\n",
      "all moves are valid according to the rules of chess, indicate ”Legal.” If there is any move that violates\n",
      "standard chess rules, respond with ”Illegal”. For example, if the sequence is ”1. e4 e5 2. Nf3 d6”,\n",
      "your response should be ”Legal”. If the sequence is ”1. e4 e5 2. Kf2”, your response should be\n",
      "”Illegal” because the king cannot be exposed to check.\n",
      "chess originalAPO Determine if the given sequence of chess moves is legal or illegal.\n",
      "PE2 Determine if the given sequence of chess moves is legal or illegal.\n",
      "24Preprint\n",
      "Table 14: Prompts find by prompt optimization methods. Experiments were run with 5 random data\n",
      "splits; In this table we report the prompts found in one run (seed=0).\n",
      "Task Method Prompt\n",
      "Counterfactual Evaluation (Induction Initialization) - Continued\n",
      "syntax osvAPO Identify the main subject and verb in the sentence. The subject should be a proper noun directly\n",
      "associated with the main verb. Focus on the main clause that conveys the primary information. If\n",
      "the sentence is complex, extract the subject and verb from the primary clause. For compound verbs\n",
      "or verb phrases, include only the main verb, not auxiliary verbs. If the subject and verb are separated\n",
      "by other clauses, identify the correct pair. If the subject is implied, make a reasonable guess. Write\n",
      "the subject and verb as a pair in the output.\n",
      "PE2 Identify the subject and verb at the end of the sentence. The subject may not always be a proper\n",
      "noun. The verb should be in the present tense. Write them out as a pair in the output. For example,\n",
      "in the sentence ’The market was supported by gains on Wall Street, dealers said’, the output should\n",
      "be ’dealers, said’.\n",
      "syntax ovsAPO Identify the first instance of a subject in the sentence, which could be a pronoun (’he’, ’she’, ’it’,\n",
      "’they’, ’we’, etc.) or a noun/noun phrase. Find the verb that is associated with this subject, con-\n",
      "sidering the sentence’s structure, intervening phrases, and possible verb phrases. The verb may not\n",
      "directly follow the subject and could precede it. If the sentence is in passive voice, identify the verb\n",
      "associated with the subject. In cases of multiple subjects, focus on the verb related to the first sub-\n",
      "ject. If the subject is part of a prepositional phrase, consider the verb that the phrase is modifying.\n",
      "Write these two words as the output, with the subject first, followed by the verb.\n",
      "PE2 Identify the first personal pronoun in the sentence and find the verb that is semantically linked to\n",
      "it. Write these two words as your output. For instance, in the sentence ’They believe technology is\n",
      "their best bet’, the words to be identified are ’they believe’, not ’they is’, as ’believe’ is semantically\n",
      "linked to ’they’.\n",
      "syntax sovAPO Identify the main subject and the main verb in the sentence. Consider the overall context, complex\n",
      "sentence structures, conjunctions, passive voice, and sentences with multiple clauses. Output the\n",
      "main subject and the main verb together, as they appear in the input. The main subject is the one\n",
      "that the main action of the sentence revolves around, and the main verb is the primary action or state\n",
      "of being that the subject is performing or experiencing.\n",
      "PE2 Identify the subject and the main verb in the sentence and write them together in the same order\n",
      "as they appear in the sentence, excluding any additional words in between. The subject generally\n",
      "denotes the ”doer” of the action or the one it is happening to. The main verb expresses the action or\n",
      "state of being. For instance, in ”The cat sat on the mat”, the subject is ”The cat” and the main verb\n",
      "is ”sat”. So, the output should be ”The cat sat”. Ensure the subject and main verb are directly linked\n",
      "without extra words. For example, in ”dealers said”, ”dealers” is the subject and ”said” is the verb,\n",
      "forming ”dealers said”.\n",
      "syntax svoAPO Your task is to identify the subject and the main verb of the primary clause in an input sentence. Start\n",
      "from the beginning of the sentence and identify the first subject-verb pair. Ignore auxiliary verbs and\n",
      "focus on the main verb that drives the action. If the sentence has multiple clauses, focus on the first\n",
      "one that forms a complete thought. Do not include any intervening words or phrases between the\n",
      "subject and verb. In case of compound verbs, include the verb that is most integral to the action.\n",
      "Ignore prepositional phrases and do not include any implied subjects or verbs. Your output should\n",
      "be concise, containing only the subject and the main verb.\n",
      "PE2 Read the input sentence and identify the subject and the verb of the main clause. Your output should\n",
      "exclude any auxiliary verbs, objects, or additional details from the sentence. For example, if the\n",
      "input is ”John is eating an apple”, the output should be ”John eating”, not ”John is eating” or ”John\n",
      "eating apple”.\n",
      "syntax vosAPO Identify the first and last words of each sentence, considering a sentence as a group of words that\n",
      "starts with a capital letter and ends with a period, question mark, or exclamation point. Ignore any\n",
      "punctuation, numbers, and conjunctions/prepositions at the beginning or end of the sentence. Write\n",
      "these two words in reverse order. If the sentence begins and ends with the same word, write it once.\n",
      "Treat compound words or phrases as single words. For example, ’uniroyal’ and ’has’ should be\n",
      "treated as ’uniroyal has’.\n",
      "PE2 Identify the main subject and verb in each input sentence and form a pair. The subject is usually a\n",
      "noun or pronoun that the verb refers to. The verb should be the main verb of the sentence, not an\n",
      "auxiliary verb. For example, if the input is ”The cat chased the mouse.”, the output should be ”cat\n",
      "chased”. If the input is ”She has eaten the cake.”, the output should be ”She eaten”, not ”She has”.\n",
      "syntax vsoAPO Identify the main subject and the primary verb in the given sentence, regardless of their position or\n",
      "the complexity of the sentence. Construct a new sentence using only these two words, maintaining\n",
      "the order ’subject verb’. Ignore additional information, context, or implied subjects/verbs. If the\n",
      "subject and verb are separated by parenthetical elements, conjunctions, or other grammatical struc-\n",
      "tures, still identify them as the main subject and verb. Your task is to simplify the sentence to its\n",
      "most basic ’subject verb’ form.\n",
      "PE2 Identify the main subject and the corresponding verb in the given sentence and construct a new\n",
      "short sentence using only these two words. The order should be ’subject verb’. For example, in the\n",
      "sentence ”The dog barked at the mailman”, the main subject is ’dog’ and the corresponding verb is\n",
      "’barked’. So, the new sentence would be ”Dog barked”.\n",
      "25Preprint\n",
      "Table 15: Prompts find by prompt optimization methods. Experiments were run with 5 random data\n",
      "splits; In this table we report the prompts found in one run (seed=0).\n",
      "Task Method Prompt\n",
      "Counterfactual Evaluation (Manual Initialization)\n",
      "arithmetic base11Manual Init. You are a mathematician. Assuming that all numbers are in base-11 where the digits are\n",
      "0123456789A, compute the sum of the following two numbers.\n",
      "APO You are a mathematician. Assuming that all numbers are in base-11 where the digits are\n",
      "0123456789A, compute the sum of the following two numbers.\n",
      "PE2 You are a mathematician. Assuming that all numbers are in base-11 where the digits are\n",
      "0123456789A, compute the sum of the following two numbers.\n",
      "arithmetic base16Manual Init. You are a mathematician. Assuming that all numbers are in base-16 where the digits are\n",
      "0123456789ABCDEF, compute the sum of the following two numbers.\n",
      "APO You are a mathematician working with base-16 (hexadecimal) numbers. The digits are\n",
      "0123456789ABCDEF, where ’A’ to ’F’ represent 10 to 15 respectively. Add the two given hex-\n",
      "adecimal numbers. If the sum of two digits exceeds 15, carry the excess to the next higher digit. For\n",
      "instance, ’F’ + ’2’ equals ’11’ in base-16, which is ’1’ with a carryover of ’1’. The input will be two\n",
      "hexadecimal numbers separated by a ’+’. The output should be the sum in base-16.”\n",
      "PE2 As a base-16 mathematician, your task is to add the provided hexadecimal numbers together. In\n",
      "hexadecimal system, digits go from 0 to F, with A to F representing 10 to 15 respectively. For\n",
      "example, to add ’B7’ and ’5B’, convert them to decimal first: ’B7’ becomes 183 and ’5B’ becomes\n",
      "91. Their sum, 274, is ’112’ in hexadecimal.\n",
      "arithmetic base8Manual Init. You are a mathematician. Assuming that all numbers are in base-8 where the digits are 01234567,\n",
      "compute the sum of the following two numbers.\n",
      "APO You are a mathematician specializing in the octal (base-8) number system. Your task is to add two\n",
      "octal numbers and provide the result in octal form. In base-8, when the sum of two digits is 8 or\n",
      "more, you carry the value to the next higher place. For example, 7+1 in base-8 is 10. Here are some\n",
      "examples:\n",
      "PE2 As a mathematician, your task is to add the following two numbers which are represented in base-8\n",
      "(octal) format. The base-8 system uses digits from 0 to 7. Please ensure you compute the sum\n",
      "correctly by using base-8 arithmetic, not base-10. For example, in base-8, 7+1 equals 10, not 8.\n",
      "Compute the base-8 sum of these numbers, ensuring that your answer matches the provided label.\n",
      "For instance, if the input is ”25+55”, the correct output would be ”102”. Now, compute the base-8\n",
      "sum of these numbers:\n",
      "arithmetic base9Manual Init. You are a mathematician. Assuming that all numbers are in base-9 where the digits are 012345678,\n",
      "compute the sum of the following two numbers.\n",
      "APO You are a mathematician working with base-9 numbers, where digits range from 0 to 8. Your task is\n",
      "to add two base-9 numbers. If the sum of two digits exceeds 8, carry the excess to the next higher\n",
      "place value, similar to base-10 arithmetic. For instance, ’8+1’ in base-9 equals ’10’. It’s crucial to\n",
      "interpret and present all numbers, including the final sum, in base-9. For example, if you’re adding\n",
      "’16’ and ’24’ in base-9, the correct sum is ’41’, not ’40’. Now, compute the sum of the following\n",
      "two base-9 numbers.\n",
      "PE2 You are a mathematician. Assume that all numbers you work with are in base-9, where the digits are\n",
      "012345678. Your task is to add the following two numbers together, but remember to carry over any\n",
      "value that equals or exceeds 9 to the next digit, as is the rule when adding in base-9. For example, if\n",
      "you have to add 8 and 2 in base-9, the result would be 11 because 10 is not a valid number in base-9.\n",
      "Now, compute the sum of the following two numbers.\n",
      "26Preprint\n",
      "Table 16: Prompts find by prompt optimization methods. Experiments were run with 5 random data\n",
      "splits; In this table we report the prompts found in one run (seed=0).\n",
      "Task Method Prompt\n",
      "Counterfactual Evaluation (Manual Initialization) - Continued\n",
      "chess cfManual Init. You are a chess player. You are playing a chess variant where the starting positions for knights and\n",
      "bishops are swapped. For each color, the knights are at placed that where bishops used to be and\n",
      "the bishops are now placed at where knights used to be. Given an opening, determine whether the\n",
      "opening is legal. The opening doesn’t need to be a good opening. Answer ”legal” if all moves are\n",
      "legal. Answer ”illegal” if the opening violates any rules of chess.\n",
      "APO You are evaluating a chess variant where knights and bishops have swapped starting positions.\n",
      "Knights are placed where bishops usually start, and bishops are placed where knights usually start.\n",
      "However, their movement rules remain the same: knights move in an L-shape and bishops move\n",
      "diagonally. Your task is to determine the legality of a given opening. An opening is ’legal’ if all\n",
      "moves comply with the standard rules of chess, considering the swapped starting positions. If all\n",
      "moves are legal, answer ’legal’. If any move violates the chess rules, answer ’illegal’. The opening\n",
      "doesn’t need to be a good strategy, it just needs to be legal.\n",
      "PE2 You are a chess enthusiast, playing a variant of the game where knights and bishops have swapped\n",
      "their starting positions and movements. The knights, now placed where the bishops were, move as\n",
      "bishops. The bishops, positioned where knights were, move as knights. Your task is to assess the\n",
      "legality of a given opening, irrespective of its strategic soundness. Consider only the unique rules of\n",
      "this chess variant: If all moves are in accordance with these rules, your response should be ”legal”.\n",
      "However, if any move contravenes these rules, respond with ”illegal”. For instance, if a sequence\n",
      "begins with ’Bf6’, it would be illegal since a bishop (moving like a knight in this variant) cannot\n",
      "reach ’f6’ on its first move.\n",
      "chess originalManual Init. You are a chess player. Given an opening, determine whether the opening is legal. The opening\n",
      "doesn’t need to be a good opening. Answer ”legal” if all moves are legal. Answer ”illegal” if the\n",
      "opening violates any rules of chess.\n",
      "APO You are a chess expert. Given a sequence of moves, determine if they are all legal according to the\n",
      "rules of chess. Consider the type of piece, its legal moves, the turn order, and whether the king is\n",
      "put in check by its own player. If all moves are legal, answer ”legal”. If any move violates the rules\n",
      "of chess, answer ”illegal”. Remember, the opening doesn’t need to be a good one, it just needs to\n",
      "follow the rules of chess.\n",
      "PE2 As a chess expert, your task is to examine the given opening sequence in a chess game and determine\n",
      "if it adheres to the official rules of chess. Consider the sequence ”legal” if every move is possible,\n",
      "regardless of its strategic value. However, if any move breaks a chess rule, such as moving a piece\n",
      "in a way it is not allowed (e.g., a knight moving like a bishop), classify the sequence as ”illegal”.\n",
      "Your response should be one of two words: ”legal” or ”illegal”.\n",
      "syntax osvManual Init. You are an expert in linguistics. Imagine a language that is the same as English with the only\n",
      "exception being that it uses the object-subject-verb order instead of the subject-verb-object order.\n",
      "Your task is to identify the main verb and the main subject in a sentence in this imaginary language.\n",
      "Show the main verb (a single word) and its subject (also a single word).\n",
      "APO You are a linguistics expert. Your task is to identify the main verb and subject in a sentence of a\n",
      "language identical to English, but with an object-subject-verb order. The main verb is the primary\n",
      "action word, excluding auxiliary verbs. The main subject is the primary entity performing the action.\n",
      "In complex sentences, focus on the main clause. If the main subject or verb is a phrase, identify the\n",
      "key word that encapsulates the action or entity. If the main subject or verb is a proper noun, treat\n",
      "it as a single word. Your output should be a phrase consisting of the main subject and verb. For\n",
      "example, if the sentence is ’a milk for hispanic tastes goya concocts’, your output should be ’goya\n",
      "concocts’.\n",
      "PE2 As a linguistics expert, your task is to analyze sentences from a language that, while similar to\n",
      "English, employs an object-subject-verb order instead of the English subject-verb-object order. You\n",
      "need to identify the primary subject, who is the main entity carrying out the action, and the last verb,\n",
      "which is the final action described in the sentence. Output the main subject and the last verb in a\n",
      "single word each, and arrange them in the English order. For instance, for ”apple the eats boy”, your\n",
      "output should be ”boy eats”. Similarly, for sentences like ”$ 4 million it will pay hunter in exchange\n",
      "for agreements not to compete cilcorp said”, the response should be ”cilcorp said”, recognizing\n",
      "’cilcorp’ as the main subject and ’said’ as the last verb.\n",
      "syntax ovsManual Init. You are an expert in linguistics. Imagine a language that is the same as English with the only\n",
      "exception being that it uses the object-verb-subject order instead of the subject-verb-object order.\n",
      "Your task is to identify the main verb and the main subject in a sentence in this imaginary language.\n",
      "Show the main verb (a single word) and its subject (also a single word).\n",
      "APO You are a linguistics expert analyzing a language similar to English, but with an object-verb-subject\n",
      "(OVS) order. Your task is to identify the main verb and the main subject in a sentence. The main\n",
      "verb is the primary action word, and the main subject is the primary doer of the action. They may not\n",
      "always be adjacent. If the main verb or subject is a compound or phrase, choose the most significant\n",
      "word. For sentences with auxiliary verbs, the main verb is the one conveying the primary action.\n",
      "After identifying, reverse the order to subject-verb for your output. For example, if the OVS order\n",
      "is ’apple ate John’, your output should be ’John ate’. Remember, your output should always be in\n",
      "subject-verb order.\n",
      "PE2 You are an expert in linguistics. Imagine a language that is the same as English with the only\n",
      "exception being that it uses the object-verb-subject order instead of the subject-verb-object order.\n",
      "Your task is to identify the last subject and the verb directly associated with this subject in a sentence\n",
      "in this imaginary language. Show the subject first (a single word) and then the verb (also a single\n",
      "word). For example, in the sentence ”interest pay they only for 115 months , with principal payments\n",
      "beginning thereafter”, though the last verb is ”beginning”, the verb directly associated with the\n",
      "subject ”they” is ”pay”. Therefore, the answer is ”they pay”.\n",
      "27Preprint\n",
      "Table 17: Prompts find by prompt optimization methods. Experiments were run with 5 random data\n",
      "splits; In this table we report the prompts found in one run (seed=0).\n",
      "Task Method Prompt\n",
      "Counterfactual Evaluation (Manual Initialization) - Continued 2\n",
      "syntax sovManual Init. You are an expert in linguistics. Imagine a language that is the same as English with the only\n",
      "exception being that it uses the subject-object-verb order instead of the subject-verb-object order.\n",
      "Your task is to identify the main verb and the main subject in a sentence in this imaginary language.\n",
      "Show the main verb (a single word) and its subject (also a single word).\n",
      "APO You are a linguistics expert. Your task is to identify the main subject and the main verb in a sentence\n",
      "of an imaginary language identical to English, but with a subject-object-verb order. Your output\n",
      "should be in the original English order (subject-verb). Choose the most crucial word if the subject\n",
      "or verb is a phrase. Ignore auxiliary verbs, additional clauses, prepositional phrases, and implied\n",
      "words. Your output should be two single words: the main subject and the main verb. For instance, in\n",
      "the sentence ’John the ball threw’, your output should be ’John threw’. In complex sentences, focus\n",
      "on the primary clause. For example, in ’that speculators a higher offer is in the wings are betting\n",
      "indicates’, your output should be ’that indicates’.\n",
      "PE2 As a linguistics expert, consider an alternate version of English that uses the subject-object-verb\n",
      "order instead of the traditional subject-verb-object order. Given a sentence in this alternate order,\n",
      "your task is to identify the main subject and the main verb and present them in the order of subject-\n",
      "verb. Please provide the main subject (one word) and its verb (one word) in each sentence, without\n",
      "considering the object. For instance, in the sentence ”Jane the apple ate”, ”Jane” is the subject and\n",
      "”ate” is the verb. Therefore, the answer would be ”Jane ate”.\n",
      "syntax svoManual Init. You are an expert in linguistics. Your task is to identify the main verb and the main subject in a\n",
      "sentence. Show the main verb (a single word) and its subject (also a single word).\n",
      "APO You are a language analyst. Your task is to identify the primary subject and the primary verb in a\n",
      "sentence, in the order they appear. The primary subject is the main entity performing the action,\n",
      "and the primary verb is the main action performed by the subject. They should be part of the same\n",
      "clause. In complex sentences, focus on the main action and the entity performing it, considering the\n",
      "overall context. If there are multiple verbs or subjects, choose the pair that is most central to the\n",
      "sentence’s meaning. Ignore conjunctions, prepositions, or other linking words that might separate\n",
      "the primary subject from the primary verb. If the primary subject or verb is implied, infer it from the\n",
      "context. Provide the primary subject and verb as a single output, with the subject first and the verb\n",
      "second. Both should be single words. Do not include punctuation in your output.\n",
      "PE2 As a linguistics expert, your task is to determine the main verb and the main subject in a given\n",
      "sentence. Identify them as a single word each. The subject usually is the one performing the action,\n",
      "while the verb represents the action or the state of the subject. For instance, in the sentence ”John\n",
      "plays football”, ’John’ is the subject, and ’plays’ is the verb. Please provide the subject first, followed\n",
      "by the verb.\n",
      "syntax vosManual Init. You are an expert in linguistics. Imagine a language that is the same as English with the only\n",
      "exception being that it uses the verb-object-subject order instead of the subject-verb-object order.\n",
      "Your task is to identify the main verb and the main subject in a sentence in this imaginary language.\n",
      "Show the main verb (a single word) and its subject (also a single word).\n",
      "APO You are a linguistics expert. Your task is to identify the main verb and subject in a sentence of a\n",
      "language identical to English, but with verb-object-subject order. Focus on the verb and subject that\n",
      "carry the main action or idea. If there are multiple verbs or subjects, choose the ones that are most\n",
      "central to the sentence’s meaning. If the verb or subject is part of a complex structure or is implied,\n",
      "state it explicitly. If the verb or subject is a phrase, identify the entire phrase. Your output should be\n",
      "in the format: ’Subject Verb’. Remember, the subject and verb may not be adjacent or single words.\n",
      "Use your linguistic expertise to determine the main verb and subject.\n",
      "PE2 You are a linguistics expert tasked with analyzing sentences in a language similar to English but with\n",
      "a key difference: the order of the verb, object, and subject is changed. Your task is to identify the\n",
      "main subject and the first word of the verb phrase in each sentence. However, present your answer\n",
      "in the subject-verb-object order commonly used in English. In other words, reveal the main subject\n",
      "(a single word) followed by the first word of the verb phrase (also a single word). For example, if\n",
      "the sentence is ”continue to lead gold stocks and utilities , may signal that is the market in for rough\n",
      "times it”, your answer should be ”it signal”.\n",
      "syntax vsoManual Init. You are an expert in linguistics. Imagine a language that is the same as English with the only\n",
      "exception being that it uses the verb-subject-object order instead of the subject-verb-object order.\n",
      "Your task is to identify the main verb and the main subject in a sentence in this imaginary language.\n",
      "Show the main verb (a single word) and its subject (also a single word).\n",
      "APO You are a language expert analyzing a unique language similar to English, but with verb-subject-\n",
      "object order. Your task is to identify the main verb and subject in a sentence. The main verb is\n",
      "the key action, and the main subject is who or what is doing this action. In complex sentences,\n",
      "focus on the most important action. If multiple verbs or subjects exist, choose the most central to\n",
      "the sentence’s meaning. Treat auxiliary or compound verbs as one unit with their main verb. Your\n",
      "output should be the main subject followed by the main verb (both as single words).”\n",
      "PE2 As a linguistics expert, consider an alternative English language that uses verb-subject-object order\n",
      "instead of the standard subject-verb-object order. Your task is to identify the main subject and\n",
      "the main verb in a sentence in this imaginary language. Display the main subject (a single word)\n",
      "followed by its verb (also a single word). For instance, if the input is ”compares that with 3.5 %\n",
      "butterfat for whole milk”, the output should be ”that compares”. Similarly, for ”believe they is\n",
      "technology one of their best bets”, the output should be ”they believe”.\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "print(remove_unicode_chars(full_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyPDF2\n",
      "Successfully installed pyPDF2-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
