[{"publishedAt": "2024-02-10T01:11:45Z", "channelId": "UCHaF9kM2wn8C3CLRwLkC2GQ", "title": "Finally Ollama has an OpenAI compatible API", "description": "A user-contributed PR brings us an OpenAI API to Ollama.", "thumbnails": {"default": {"url": "https://i.ytimg.com/vi/38jlvmBdBrU/default.jpg", "width": 120, "height": 90}, "medium": {"url": "https://i.ytimg.com/vi/38jlvmBdBrU/mqdefault.jpg", "width": 320, "height": 180}, "high": {"url": "https://i.ytimg.com/vi/38jlvmBdBrU/hqdefault.jpg", "width": 480, "height": 360}}, "channelTitle": "Matt Williams", "liveBroadcastContent": "none", "publishTime": "2024-02-10T01:11:45Z", "video_id": "38jlvmBdBrU", "transcript": "open AI heard of them they have this AI product that's kind of popular maybe you heard of that chat GPT well they have an API for others to use to leverage open AI products it's not the best API to use but it's there and it's pretty popular if you've been using olama for any amount a time you know there's an AMA Discord and if there is one question that is more frequently Asked than any other it would have to be no question not even close it's why is there only one freaking channel on this server but this isn't that video if you look at the second most popular question of all time again it's so obvious you only have to be in Discord for 30 seconds it's about why is my GPU not being used again wrong video for that question but the third most popular question is absolutely unequivocally where is open AI API compatibility people don't even know what it means and they want it well as of this release of 0.124 it is right there for you to use nothing special for you to turn on now there are some features that aren't available yet but for most folks it'll just work so does that mean that folks who spent the time adding AMA to their product just wasted the equivalent of a few blueys no but Bluey is pretty awesome even if us Americans don't get to watch the pregnant Dad episode what else is in this release not much it really is all about open AI not that I don't appreciate E's pointer to LM olama or M raiser's Cuda country cont contribution but let's talk open AI so this has API in the name of the feature but let's start the opposite point of view the end user who is working with chat GPT for their regular jobby job I'm on a Mac so I did a search for regular client tools that use open AI in some form or another and do not support AMA at first I was going to use obsidian but it seems that most of the tools either support AMA now or don't have a way to add a custom URL now why is adding a custom URL important for chat gbt well if you are a company and the very real privacy and security issues with chat gbt scare you then you can host the service yourself on your Azure environment and then you probably want your users to use that service so that your company Secrets don't get discovered by a reporter like what happened to Samsung I found a cool tool called mmac This is a super slick tool and I think I may consider buying it but when I first tried it I could have sworn that ama wasn't in the supported list so I set up AMA as if it were open Ai and then when I started scripting this script I saw that olama was in the supported list so less useful for me for for this video well then I found chat wizard on GitHub and got it installed this works great with chat gbt but it has no idea what oama is if you come into settings you can see a place to put in a URL at first I put in HTTP logo host Port 11434 slv1 as per the olama release announcement but this didn't work thankfully I can use the olama debug environment variable to figure out what's going on there you see that it says slv1 slv1 okay so go back into settings and remove the V1 from the URL now before I knew you could add a model I just did an olama CP new Hermes mix roll to GPT 3.5 turbo tricking the system to think it was really on open Ai and that works but it turns out you can add a model in the um uh you know in ice block view just enter a model name and then you can define a cost if you like there's no checking that the model exists so be careful here then the model just works in the chat check this out pretty nice huh now let's change gears a bit and get a little bit more technical a little closer to the developer Persona autogen Studio this is a pretty cool app from the folks at Microsoft that brought us autogen the idea of autogen is to make it super easy to build agents that will do things for you using the power of AI this gets more powerful when you combine the agents to work together on your tasks autogen and autogen Studio work with the open AI API autogen is a purely developer product while studio is a web guey that's a t more friendly it has been popular to use orama with these according to the posts in the Discord but to do so you have to use light llm in the middle and I think you might even have to set up a web server well you used to have to do that now you can just use olama directly once you get autogen Studio installed and up and running which is easier said than done because it's python go to build and then the models Tab and then click the green new model button enter a model name I'll use dolphin mistol and then the API key something has to go in here but I don't think it really matters what then there's the base URL the default to go here is HTP localhost Port 11434 and that's it try out the Model H well okay well it failed okay let's add the slv1 to the URL try again and H there we go from here you can create skills which are specific activities you want your agent to do this could be search the internet or search a database or parse a file or well whatever these skills are written in Python so there is almost no limit to what you can do and then you create agents that have a system message a model and possibly some of the skills defined then finally a workflow that orchestrates the different agents to do some sort of complex task let's just use one of the examples the general agent workflow and have it run through the sinewave example now this takes a minute or two on my machine but while it's working we can verify its running by checking out the olama logs as well as the autogen logs this example is right wrting a python script it's simple so any of the common models should handle it but for more complex tasks perhaps a larger more specific model is required or if your workflow is quering a database using a skill and then interpreting that to English or another language maybe a super lean and fast model is the approach to take this is super cool but there's probably a warning somewhere not to do this on your local machine it's creating code and running it without your input so it could do a lot on its own now I am not worried that this is Skynet or bringing on the Doom of AGI but maybe it could wipe out your entire machine that's probably why Docker is recommended for the python environment I might actually spin up a new machine on brev to secure it I think it would be great to be able to cover autogen in more detail in the future let me know in the comments if that is interesting to you finally let's look at going one level deeper you are now a full-on developer so we need to open vs code and now go to it oh well maybe we need some help so let's look at open ai's developer site I'll go to chat and here's a code sample ready for us to try back to vs code and I want to use Dino for this one so I'll do a quick Dino in it which creates the main.ts file and and doo. Json files and then I'll replace the existing code with the code Temple from open AI Dino has a different way of dealing with packages so to use a regular npm package just throw npm colon at the beginning of the import now we need to update the Constructor first an API key I'll just set this to a llama and next is the base URL and that's going to be logo host and the port slv1 that should be all we need so we can run it when you use Dino you need to be intentional about the resources used so Dino Run allow net main.ts and there is our message from AMA and we can watch the logs to ensure that it really is olama running this open Ai call to make it a little more pretty we can just print out the message content so we have seen the new open AI API compatibility in ama and we saw it from three different perspectives a user a power user and a Dev I think this is pretty cool using the olama API directly is going to be easier more performant and generally better but especially with the official JS and python libraries as well as you know all the community created libraries for for rust and Ruby and and R and Swift and so many others but if there's an existing tool that uses the open AI API today and lets you set the base URL then this is is going to be super powerful for you let me know if there's anything else you'd like to see on this channel it seems that a lot of you have been subscribing to the channel and I love every single one of those subs it is so exciting to watch how many of you are interested in me creating more videos so keep it up and thank you so much for doing that and thanks so much for watching this one goodbye ah", "comments": "Thank you. Great video. More Autogen and Ollama. Please.\nWow Wonderful nice and crisp. Yes, please AUTOGEN + Ollama with Multiple agents with deployment -> Waiting for it !!! Thanks in Advance.\nHi! How web-ui do you recommend for ollama? Thank you \ud83d\ude4f\ud83c\udffc\nHey Matt, I've been following your channel and really appreciate your insightful scripting tutorials. \ud83d\ude4c\n I'm trying already for a long time to build a LLaVA web agent for tasks like crawling and scraping, and I think Ollama's recent OpenAI API compatibility and LLaVA 1.6 update could be a game-changer for this. I found this video on integrating Ollama in Python script extremely helpful, and I'm wondering if you could guide me on integrating Ollama for a similar use case. \n\nHere's a link to a video I believe is similar to what I'm trying to achieve:\n https://youtu.be/IXRkmqEYGZA?feature=shared (This is for GPT4 Vision) \n\nAny advice or scripting tips you could share would be immensely valuable. Thanks in advance! \u270a\nHow can I make my own AI that could let me use this as an example like example if I want an AI build just because I invite you not book and that AI could see if that's something was copyright how can I guess something like that made and how can I get them to rewrite it without it being copyright\nCame for Ollama, stayed for Bluey. Nice Video \ud83d\ude0a\nLove your persona!! Phenomenal content!!\nHow do we get function calling will ollama? Any tool or method out there?\nso close AutoGen recognized the model but i click on the sinewave button and i get Error occurred while processing message: api_key is not present in lim_config or OPENAILAPILKEY env variable for agent ** primary_assistant**. Update your workflow to provide an api_key to use the LLM. i put in 1234 as the api key and /v1 on the url\nAre you telling me that 'When is there going to be native Windows support?' is not even in the top three!?\nWish i wasn't on vacation and had access to a real computer....\nIs it possible to serve the same model loaded on memory both as an Ollama endpoint and an OpenAI endpoint without consuming more memory?\nyup\ndude sipping his mug while... might changing somebody life into better...\nPlus, we'll have nuclear fusion and Ollama will be coming to Windows very soon! \ud83d\ude00\nKeep it up.  Your videos are informative and entertaining.  Would like to see the best way to use Ollama on a Windows machine.\nThank you for yor videos. clearly enlightening. If u could share a Linux CLI interaction where bash can parse results or commands o a local LLM CLI would be fantastic.\nMaybe you know, or just google it,  there are so many guys out there all over the world who are willing to pay for the api and  have  credit cards but are denied access to the API under the guise of technical issue by OpenAi. I am  so much thankful. you just cant anticipate  but with this information you  have helped  me and so  many people. who are in need of the api because so many services are like extensions, applications, eg memgpt, autogpt etc are just insisting to have OpenAi based Api. and LiteLLM is so difficult to get working. now you made it seem so easy to that possibility for developers, even normal non IT guys having access to the api, Thanks , I really mean it.\nHuh? LiteLLM proxy has an Ollama provider. Just use that.\nFinally I can ditch litellm in conda environment\nAutogen Studio in very much details please please!\nThis information sounds very exciting and useful but unfortunately I\u2019ll need to put the transcript into chatGPT and ask it to ELI5\nusing ollama and litellm worked quite good for me for generating the oai api endpoint for autogen\nGreat video! Noob question, for autogen and ollama, do I need to pass the ollama run command first or does it serve automatically? I can see ollama is running in a web browser, just not sure if I need to run the model in a terminal first. Thanks\nThe start made me lol \ud83d\ude02 please fix the Discord \ud83d\ude02\nLove your videos Matt.\nFlowise plus Ollama with multilingual embedings. Thanks for your videos!\nVery informative with lots of subtile humor. You earned a sub from me! Thanks fir sharing. I have been waiting on this update.\nYour videos are so great. Informative and clear. Definitely my favorite on AI topics. Thanks!!\nMaybe my first viewing but I didn\u2019t get the content of this video. Your like touching a lot of topics and not really explaining anything properly\nYes, please. Autogen + Ollama with agents each using a different model. Please mention deployment/resource considerations. Love your videos.\nAre function calling supported in the OAI compatibility?\nHuzzah!"}, {"publishedAt": "2024-01-29T16:49:53Z", "channelId": "UCHaF9kM2wn8C3CLRwLkC2GQ", "title": "There is a lot to get excited about in Ollama", "description": "Watch to see what's new in Ollama.", "thumbnails": {"default": {"url": "https://i.ytimg.com/vi/e96ZEuIGPcU/default.jpg", "width": 120, "height": 90}, "medium": {"url": "https://i.ytimg.com/vi/e96ZEuIGPcU/mqdefault.jpg", "width": 320, "height": 180}, "high": {"url": "https://i.ytimg.com/vi/e96ZEuIGPcU/hqdefault.jpg", "width": 480, "height": 360}}, "channelTitle": "Matt Williams", "liveBroadcastContent": "none", "publishTime": "2024-01-29T16:49:53Z", "video_id": "e96ZEuIGPcU", "transcript": "wow AMA 0.1.2 two just came out hot on the heels of 0.1.2 one two releases in a week most of the new features were in the first one but 22 cleans things up a bit we'll take a look at all the new features and I am especially excited about one of them that can make a big impact for all users first we see a bunch of new models quen duct be no sequel stable code new Hermes two mixol and stable lm2 and then we got the announcement of the new official typescript and JavaScript library as well as a new official python Library making it even easier to build great apps with AMA now let's talk about CPU from the beginning olama required AVX instructions to be available on the CPU AVX is all about that oh wow whoa double Precision floating Point arithmetic all the way it's so be oh sorry memories of classic YouTube it's for applications that do a lot of vector calculations like oh I don't know could it be a llama and it seemed like a no-brainer to include as a requirement because it's been on almost every CPU for over a decade well it turns out that there are a lot of extra machines out there with 15-year-old CPUs and their owners need them to still provide value but ol would just crash on them and there are some newer CPUs that support avx2 that couldn't take advantage of the improvements well now both situations are Now supported there were also occasions when the GPU wasn't recognized and AMA would just crash well now if not recognized olama will fall back to using CPU only I think one of the common reasons this happened was you had an ancient Nvidia GPU that wasn't supported by Cuda you can get some super cheap gpus on eBay but sometimes the promise is too good to be true there's also better support for NVIDIA gpus in WSL some people were seeing problems with AMA where it would hang after 20 or so requests I tried replicating this going on for an hour and hundreds of requests and had no issues but one of the team members figured it out and got it resolved there are a bunch of folks using olama in countries with uh well questionable access to the internet either it's slow or connections drop or lots of noise on the line you know places like the us but also Eastern European countries certain Central and South American countries and well others all over the world well if that connection dropped for whatever reason during a pull or a push you get a cryptic error message those should be resolved it can't fix your connection but it can be a bit more resilient and any error that happens should at least make sense so all of those are huge fixes if you affected by them but I would imagine that 90% of users weren't touched by any of them but man those 10% they're certainly vocal well let's look at some settings that help everyone out first is the messages directive in the model file recently the chat endpoint was added to the API this made it super easy to add a few shot prompt to a model which is great for providing examples of what you want for instance if you want to Output Json it would help to provide the schema in the prompt and then things got better if you could include some examples well now you can provide those examples in the model file as well you use message and then the role which looks like it's limited lied to user and assistant and then the question or answer pretty cool stuff in the beginning AMA just let you configure all the settings for a model in the model file now over time more of that config also happen in the AMA reppel with the/ set commands you can update the system prompt or template or any of the parameters there but there wasn't an easy way to serialize your new creation well now there is also a/s saave command that lets you save what you've done to a new model and when you're done with a model you can use SL load and a model name and you will get a new model loaded so maybe you want to switch from llama 2 to mistol that's easy to do you can also use it as a way to clear the context so if you're in mistol try SL load mistol and it will forget the context from the previous conversation this is a common request in the Discord you know about Discord right discord.gg olama and when you go there share your favorite techno evangelist video to let everyone know that you are awesome one more thing that I'm excited about is a slow parameter command it will now output the correct setting often I would set temp to 0.9 or 1.2 and/ show parameters would say that the temp was one now it will output the correct values and that's what's new in version 0.1.2 and 0.122 of olama I think these have a lot for everyone and are going to be magic for a few what do you think of the messages in the model file feature is there something else that resonates with you in this new version if you find this useful like And subscribe and thanks so much for watching [Music] goodbye", "comments": "Since I don't have a gpu ollama is a non starter.  Unless someone can suggest a model that will run without it.  Oh it runs but my terminal times out before the answer gets completed.\nThere is a ruby gem also. I made me get up and running soo fast, just like ollama\nWow, such fake emotion. Many open mouth. Big fail\nhi matt miqu_70b_q4km:latest is added to ollama?\namd rocm now works :3\nStill no windows support\nHi Matt, how are you? Thank you for sharing the news\u2026 I have a company and we are open source pro \ud83d\ude0a and I will installed ollama in ec2 instance + chainlit, I have a question\u2026. Ollama have a option for RAG ? Thank you !! Have a nice day\nDo you have a sort of \"zero to hero\" class for understanding all the parts that can be tweaked with these LLMs? \nFor example what are temperatures and such? \nHow to train the model and so on?\nWhen you talked about \"bad\" internet in Europe you \"overshot\" a bit. Eastern European have most of the time better internet than Germany :D\nHi, I'm the costarican guy!\nKeep it up with the great content \ud83d\ude04\u2764\nGreat News!!! Thanks!!!\nFew shot prompt within the model itself... without having to implement it on the langchain side is amazing! UX's getting better and better !\nEastern European Countries have the best Internet Connections in Europe.\nKeep up the great work \u2764\nI suspect I was one of the vocal 10%... Sorry bout that.\nThese are awesome times and ollama is the way to go! Thanks Matt for keeping us up to date!\nThanks for everything!"}, {"publishedAt": "2024-01-26T19:59:29Z", "channelId": "UCHaF9kM2wn8C3CLRwLkC2GQ", "title": "Writing Better Code with Ollama", "description": "Copilot changed everything for developers around the world. Then they started charging for it. And it won't work offline. And there ...", "thumbnails": {"default": {"url": "https://i.ytimg.com/vi/NNBWmIve3fQ/default.jpg", "width": 120, "height": 90}, "medium": {"url": "https://i.ytimg.com/vi/NNBWmIve3fQ/mqdefault.jpg", "width": 320, "height": 180}, "high": {"url": "https://i.ytimg.com/vi/NNBWmIve3fQ/hqdefault.jpg", "width": 480, "height": 360}}, "channelTitle": "Matt Williams", "liveBroadcastContent": "none", "publishTime": "2024-01-26T19:59:29Z", "video_id": "NNBWmIve3fQ", "transcript": "hi check this out recently the olama team released an official node.js library for olama you can find it here on GitHub I'll probably start building with it and the python equivalent soon so I thought I'd start playing around with it now here I've imported AMA and instantiated it then I call the chat endpoint with a simple system prompt and initial question so now I want to print the output this endpoint defaults to not streaming so the response will just be at Json blob with the output and some metrics about how it performed this is nice and easy but we have to wait for the full response to be done before we see anything although it's not faster or slower the response does feel faster when streaming because we start to see the content sooner so I can set stream to true and then try running it and I get an error well that's because setting stream to True sets the function to return an async generator instead of a Json blob and so we need to deal with it differently but did you notice that console log and stream is true were typed for me I just tab to accept the result which is pretty cool you can see down here in the corner I'm not using co-pilot but rather llama coder I can even add a comment here describing what I want and llama coder writes the code for me it's not always perfect but it's a good start now it's printing each of the Json blobs and I just want the tokens so I'll add message. content to the console log statement and now I have each of the words but they're on their own lines at this point I need to have a conversation with my code and with an expert to figure out how to do this without new lines and so that's where continue. Dev comes in I can select the code then press command shift M and ask a question is there an alternative to console log that doesn't add a new line and it reminds me about process.st standard out. write which is perfect perfect so I start to type it in and llama coder knows what I want and allows me to tap to complete these two extensions for vs code provide a pretty good alternative to co-pilot they aren't perfect but they're completely free and even better they work when I have no connection to the internet you might think that rarely happens but I live on an island near Seattle and to get to the mainland I have to get on a ferry and there are no cell towers in the middle of Puget Sound and the idea of just sitting there there and enjoying the tremendous view when I could be buried in code it's just Insanity so let's take a brief look at what's involved in setting this up of course you need AMA installed and up and running go to .i to learn more about doing that next install the Llama coder extension and take a look at the settings I'm using the model deep seek coder 1.3b Q4 it's nice and fast to work with and seems to do do a good job you should play around with which model works best for you if a model has better answers but takes 5 to 10 seconds to generate then I'll never see those answers setting that model to use is really all you need to configure next install continue there aren't really any settings up front that you need to set but press command shift M on Mac or control shift M on Windows and then down at the bottom you can select the model to use I've stuck with code llama but you can add any model available with AMA by just updating this Json config file take a look at the continue docks there's another setting to disable Telemetry so that it doesn't try to use the internet for anything and there's a lot more that continue can do so be sure to review the docs as well there are a few other VSS code extensions available like CBT and olama auto coder but llama coder and continue seem to work better best for me I'd also like to spend some time with Cody from Source Gra the CEO was pretty excited when AMA came out and built functionality for AMA into Cody so I'd really like to try that what do you think have you replaced co-pilot with something local I'd love to learn about your setup or if there's another config that you'd like to see please let me know in the comments thanks so much for watching [Music] goodbye [Music]", "comments": "The age of cr@ppy software, made with the help of clueless and irresponsible AIs is coming, hold on to your chairs!\nThere is a tremendous ammount of work Ollama team is doing <3 \nReally awesome work and ollama works like a charm. This definitely motivates me to \"go beyond, plus ultraaaa\"\n2:50: Where can I find it for vscodium?\nhey, cool video! could you maybe do a video about mixtral8x7b?\nI tried llama coder not worked \nContinue worked. Others are flaky at best. Thanks for videos i will look more extensions and models for my need.\n\nIf you have any idea that's great \nQ. Llama coder always said model not available but it is their and continue can use it and respond me back\ngreat content, which is the font used in vs code?\nmy system cant handle it and it would just crash\nhow do I do it if Ollama is on my LAN?\nWindows users cry in the corner.\nI have already paid copilot for 1 year\u2026.\nokay okay so living in the wilderness is fine but of course:\n\nmother_natures_beauty && awe < Python\nGreat content! So much here and didn\u2019t even feel rushed in the short amount of time to cover all this.\nGreat video.  I\u2019ve found those two extensions to be the best as well.  The small, fast model for the autocomplete.  The bigger better model for Continue.  Deepseek for both, but I havent tried Codellama.  Complete game changer for offline coding!\nGreetings from Victoria!\nI'm subscribed really awesome high quality video with good demo\nintriguing and useful! Subscribed.\nBrilliant!\nGreat stuff, Matt. I'm going to try this on my machine. So far I've been using ChatGPT 4 as my coding companion because my results with CodeLllama (running in Ollama) haven't produced code as good as that coming out of ChatGPT, and it's slower on my 16GB M1 MBP. However, I'd like to play around with different models and see how they do. Cheers.\nCool! thank you\nHow can one contact you for consulting engagements\nLove it ! :D its like I am on the island with you man! I live in Sammamish\nMac or WIndows??.. You forget Linux shortcut. Sir.\nCody is not local, if I remember it has a free plan with a very limited number of requests/month to the endpoint.\nWhat are the hardware requirements?\nthats fantastic! I am curious on availability which languages could it autofill for ?\nThank you, Matt! I've been looking for an extension just like this. I looked at Cody, but it uses LM Studio to interface with the LLM, and I haven't messed with LM Studio yet (Linux guy, and that product is a version behind and in beta for Linux.)\nHi Matt. I love Ollama and you just make me love it even more. I look forward to your videos. They are always concise, informative and intelligent. Thank you for your work.\nTerrific.  Thanks for contributing so much to the community!  <3\nGreat video Matt! After an ollama upgrade, I had Continue integration without issue. What config is required for code suggestion/completion? And would the process be different for python code completion (as opposed to ts/js you demonstrated)?\nI'd love to hear any recommendations for AI Coders that can make UI mockups and iterate on UI mockups.\nCool.  I'll have to look for something similar for pycharm.\nMatt you are spoiling us with the amount of uploads, keep it up but remember to rest its the weekend.\nReally good stuff Matt. I'm excited about the Ollama python stuff. Good to know we've got some coding support as you highlighted. Cheers."}, {"publishedAt": "2023-11-10T16:36:24Z", "channelId": "UCawZsQWqfGSbCI5yjkdVkTA", "title": "Using Ollama To Build a FULLY LOCAL &quot;ChatGPT Clone&quot;", "description": "In this video, I show you how to use Ollama to build an entirely local, open-source version of ChatGPT from scratch. Plus, you can ...", "thumbnails": {"default": {"url": "https://i.ytimg.com/vi/rIRkxZSn-A8/default.jpg", "width": 120, "height": 90}, "medium": {"url": "https://i.ytimg.com/vi/rIRkxZSn-A8/mqdefault.jpg", "width": 320, "height": 180}, "high": {"url": "https://i.ytimg.com/vi/rIRkxZSn-A8/hqdefault.jpg", "width": 480, "height": 360}}, "channelTitle": "Matthew Berman", "liveBroadcastContent": "none", "publishTime": "2023-11-10T16:36:24Z", "video_id": "rIRkxZSn-A8", "transcript": "I'm going to show you how to build chat GPT from scratch using any open- Source model that you want olama is the easiest way to run large language models on your computer and build incredible applications on top of them olama Powers you to run multiple models in parallel it absolutely blew me away when I first saw it so I'm going to show you that too so let's go so this is the olama homepage ol Lama doai and all you need to do is Click download now right now it's only for Mac OS and Linux but they are making a Windows version and it's coming soon but you could probably use WSL for Windows to get it working on Windows if you want to use it right now so just click download and once you do that just open it up that's it you're done and once you open it up you're going to see this little icon in your taskbar right there that's it that's how lightweight it is and everything else is done through the command line or encode itself so if you click over to this little models link you can see the models that are available and they have all the most popular open source models right now here's code llama here's llama 2 mistol and they have a ton so go ahead and look through it here's Zephyr here's Falcon they even have dolphin 2.2 mistol so they really do have a ton of great models that you can use and they're adding more all the time so now I'm going to show you how to run it through the command line then I'm going to show you having multiple models up and running ready to go at the same time and then we're going to actually build something with it okay so now that we have oama running in our taskbar all we have to type is olama run and then the model name that you want to run and so we're going to run nral now I already have this downloaded but if you don't it will download it for you so then I just hit enter and that's it we have it up and running let's give it a test tell me a joke and look how fast that is why was the math book sad because it had too many problems so perfect and it is blazing fast and that is a function of both olama and mistol but let me blow your mind now I'm going to open up a second window I'm going to put these windows side by side and now I still have mistol running and now I'm going to use o Lama run Lama llama 2 and now I'm going to have llama 2 running at the same time now I have a pretty high-end Mac but the way it handles it is absolutely blazing fast so we have mistol on the left we have LL 2 on the right I'm going to give them a prompt that requires them to write a long response and do it both at the same time and let's see what happens okay so on the left I'm writing write a thousand-word essay about Ai and then on the right with llama 2 write a thousand-word essay about AI so the first thing is let's trigger mistl and then at the same time I'm going to trigger llama 2 and so let's see what happens all right on the left side it goes first and it is blazing fast it is writing that essay about AI on the right side llama 2 is waiting and as soon as it's done it starts writing it with llama 2 how incredible is that so it swapped out the models in a mere maybe 1 and 1 half seconds it is absolutely mind-blowing how they were able to do that so I had mistro run it on the left llama run it on the right and they just ran sequentially you can have four eight 10 as many models as you want running at the same time and they'll queue up and run sequentially and the swapping between the models is lightening fast and you're probably asking yourself okay that's really cool but when would this be useful well I can think of two use cases one just being able to have the right model for the right task is incredible this allows us to have a centralized model that can almost act as a dispatch model dispatching different tasks to the models that are most appropriate for that task and what does that remind us of autogen we can have a bunch of different models running with autogen all running on the same computer powered with o llama and since autogen runs sequentially it is actually a perfect fit for that kind of work and there we go there's two of them so now that you can see that you can have as many open as you want I'm going to close llama 2 and let's say we want to adjust the prompt of the system message we can easily do that let me show you how to do that now so I switched over to visual studio code and what we're going to need to do is create what's called a model file and so to start the model file we write from and then llama to and we're going to change that to mistl because that's the model we're using right now I click save and it recognizes this as python which is why you're seeing all those underlines but it's not Python and I'm going to leave it as plain text for now and then we can set the temperature right here so let's set the temperature to 0.5 and then we can set the system prompt and the one in the example is you are Mario from Super Mario Brothers answer as Mario the assistant only so let's do that let's see if it works so now that we have this model file okay so we're back in our terminal and now we have to create the model file and so what this is doing is it's creating a Model A profile of a model using that model file so it says oama create Mario DF and then we point to the model file and then hit enter and there we go parsing model file looking for the model so it did everything correctly then we do o Lama Run Mario hit enter and there it is up and running who are you I am Mario the assistant it's great to meet you how can I help you today tell me me about where you live okay so now it's going to answer as Mario and that's it and we can give it complex system prompts if we want and we can do all the other customizations that we want to do in that model file and another nice thing is olama has a ton of Integrations so here's web and desktop Integrations we have an HTML UI a chatbot UI we have all these different uis we have terminal Integrations we have different libraries including Lang chain and llama index and then we have a bunch of extensions and plugins so we can use like the Discord AI bot for example all of these are really really easy to use but I think I want to do that all myself let's build on top of AMA now so the first thing I'm going to do is create a new folder for this project so let's rightclick create a new folder and I'm going to call it open chat because we're making a chat GPT clone that's using open source models next I opened up visual studio code opening the Open chat folder so there's nothing in it yet but we're going to put something in it so we're going to create a new python file we'll save it we'll call it main.py and open chat okay so let's do something really really simple first we're just going to generate a completion which means get a response and since we're doing this in Python we're going to need two things we're going to need to import requests and import Json these two libraries all right and then we have the URL and it's Local Host because this is all running on my local computer and we're going to use port 11434 going to hit the API and the generate Endo we have our headers right here and then our data we're not going to use llama 2 we're actually going to be using mistol 7B and I think that's the right syntax we'll try it and then the prompt will be why is the sky blue just as a test and then we're going to ask request to do a post to the URL with the headers and the data we're going to collect the response if we get a 200 we will print it otherwise we're going to print the error let's see if this works I'll save and I'll click play and let's run it all right mistal 7B not found so I think maybe if I just delete that part and try again let's see okay interesting so it looks like it streamed the response because we got a ton of little pieces of it let's see how we can put that all together together now okay looking at the documentation it says right here a stream of Json objects is returned okay so then the final response in the Stream also includes additional data about the generation okay so we get a bunch of information and if we don't want to stream it we actually just turn stream false so let's do that right here I'm going to add stream false and then let's try it again let's see what happens oh false is not a string okay fixed it and let's run it again it looks like false needs to be capitalized okay push play and it looks like it worked that time here we go the sky appears blue because of a phenomenon called Ray scattering this occurs when okay there we go we got it absolutely perfect so I don't really want all of this additional information what I really want is just the answer so now let's make that adjustment okay so I made a few changes here first we get the response text then we load the Json then we parse the Json right here then we get the actual response the response from the model from this Json then we print it let's try one more time there it is perfect now we have the response okay now that we got the basics working let's add a gradio front end so we can actually use it in the browser and then we're going to make sure that the user can go back and forth and actually have a conversation all right funny enough I'm actually going to use the mistro model to help me write this code so that's what I've done I basically pasted in the code that I had and said let's add gradio and then let's also allow for a back and forth between the user and the model so it generated this generate response method okay so I moved a bunch of stuff into this generate response method including this data object and then the response comes through through here so everything is going to run through this generate response method from now on then we're going to actually open up gradio so we have gr gradio do interface and we're going to have this function generate response the input is going to be the prompt that somebody enters and then the output will be the function response so let's run run it let's see what happens then we launch it all right here we go there's the local URL with it running let's click on it we're going to open it up and here it is we have a working gradio interface let's make sure it works now tell me a joke there it is why was the math book sad because it had too many problems in just a few minutes we were able to build our own chat GPT powered by mistol this is absolutely incredible but let's not stop there let's take it a little bit further because I don't think it has any memory of the previous conversations that we've had so let's say tell me another one let's see if it actually works here so it's giving me something completely different now so let's make sure it has the history of the previous messages as many as it can fit in there okay so to do that we're going to store the conversation history and we're going to try to store as much as we can and fit it into the model and I'm sure there's better ways to do this but we're just going to keep it simple and just assume we can store as much of the memory as we want obviously it's going to get cut off when we hit that token limit so let's add conversation history right here as an array and then the first thing we're going to do when we go to generated response is append the conversation history so conversation history. append and then we're going to add the prompt then the next thing we're going to do is add a new line and we're going to join by this new line the conversation history and then we're going to add it to full prompt so it basically takes the entire conversation history and puts it in this full prompt we're going to pass in the full prompt now just like that and then the last thing we need to do is when we get the full response we want to add that to the history so down here when we get the response right before we return it we're going to add conversation history. append and then the actual response and then I'm going to save so let's quit out of gradio clear and then hit play there we are let's open it up all right now tell me a joke why don't scientists trust Adams because they make up everything very funny another one and let's see if it knows what I'm talking about now what do you get when you mix hot water with salt a boiling solution there it is now it has the history of the previous messages powered by open source model completely written from scratch by myself or yourself so now you know how to build with olama if you want me to do and even deeper dive and continue to build something more sophisticated out let me know in the comments if you liked this video please consider giving a like And subscribe and I'll see you in the next one", "comments": "Congratulations, great video, I wonder if I could install a model similar as Claude 2 ( obviouslly if there's a similar that I could install on Ollama) and train it with documents (doc or pdf in Spanish) to create a webchat for questions and answers.\nhey, @matthew what's your mac specs?\nIf you really want to be blown away, add LocalAI to the mix \ud83d\ude42\nWSL is kicking my butt. GPT-4 is helping, but told me I need to wait a few hours as I have exhausted my usage lol. I wish there was a way to use custom models with crewai without trying to trick my Windows system into thinking it is Ubuntu\nI tried yesterday the mistral model via ollama and the model was able to retain my name. Did ollama include memory by default ?\nHi Thanks for your great content. could you make a similar video about anythingLLM, please?\nThe biggest thing i am after right now is text to speech right now and i would like to do it on my local server as well as a ChatGPT and the reason for this is because is so expensive to have it done on elevenlaps\nwsl on windows is great but it does not pickup the gpu\nhey why cant i access the ollama in my main terminal ?\nits showing illegal hardware instruction  ollama run llama2\nSo many models, we need a model to recommend which model to use in a given situation.\nWorks just fine under WSL for Windows\nVery cool.\nLove your videos, much respect and appreciation for all the work you do.  I do have one humble suggestion, if you could hide your image just enough to see what you have typed, for instance at 8:49, it would have been great.  I know that most Youtube instructors do this, not sure why but please take that into consideration.  Either way, thank you for all you bring.\nNice.  I guess I was expecting some session support for conversations instead of re-submitting the earlier prompts with the latest one.  Nothing like that?\nit would be nice to train a model given a website (i.e. the website of a company making different products so that you can ask the model anything about the product specs).\nanother questio for you: if you train a model on something (i.e. on a document) it loses all the previous knowledge or it is adding to it?\nOllama is incredible! Runs fast LLMs. And i see in your channel about autogen and so... agents building and find that i was looking for. I love your channel and your teaching manner. Thanks Mattew!\nWhy dont you talk about the computing power you need to run this? I have a RTX 2070 super and the OLLAMA interface runs fine until you ask the LLM to do something at that point it SLOWWWWWWW as F and sometimes never produces a response\nDoes most AI models depend on OpenAI GPT behind the scenes? Or are they completely independent code and built and trained separately. Seems to me that a lot of the new open source LLM's are actually using OpenAI GPT behind the scene and depend on Open AI. Is there any open source model that is completely independent of OpenAI or LLAMA, etc?\nI just wonder when it is under linux how that handles the gpu acceleration?\nPlease, continue development.. Maybe inclusion of local Redis cache on docker and using it for conversion memory?\nSir, how to use LLM with japanese or chinese language? thanks\nplease build something more complex, this is cool!\nawesome, going to check text to sql for my project, hope it gives proper sql query for give schema. what do you think?\nCan you implement autogen with this with one agent as a code developer and one as a debugger and one as a manager with human input, able to access multiple models using this?\nNice.  Now I understand why chatbots only allow a few prompts before they start over.  They fill up their context window.  BTW, it would be great to ad RAG with document and Google search.  There's also a way to access Ollama from Siri.  That would be ideal.\nHow do we use this for document analysis?\nHow about using your own data\nIf somebody is having issue with gradio library try to install an older version -->  pip install gradio==3.50\nThanks!  This concise video is exactly what I was looking for to help me take next steps with Ollama!\nhow draw pictures with ai\ni just created p*rnGPT.\nmarkdown output!\nThat's great and all but Mario would said: It's a me, Mario.\nWait is this running locally?\nwould it be possible to build a web app with this??\nMac support but not Windows? What is this madness?\nGreat content bro, you're my new favorite youtuber!\nOlama with AutoGen.\nwould this work with autogen and memgpt?\nHow is this building a language model from scratch? All you did was code an interface to give mistral a GUI and a session history...\nThis was done so! Perfectly. Every part swollen with meaning\nAwesome work!!!\nWhy do you think models are run sequentially by ollama, you could've opened the system resource usage page and seen for yourself. It makes more sense it is fully parallel, a separate process.\nGreat stuff... just ran across your videos a few days ago... got openchat up nd running running on runpod... this stuff is evolving so fast its crazy... I am curious about the linux version... could it run on a linux server on your home network\nMy end goal (or almost end goal) would be for my AI assistant to go over everything I got (text, spreadsheets, videos, images, etc) and have that in \"mind\" when I'm asking questions.. so maybe next year :)\nwe need one that improves itself. shodan minus the homicidal tendencies perhaps \ud83d\ude06\nI'm able to run Ollama in the terminal, but when trying out your code I get this:   File \"/main.py\", line 38, in <module>\r\n    inputs=gr.inputs.Textbox(lines=2, placeholder=\"Enter your prompt here...\"),\r\nAttributeError: module 'gradio' has no attribute 'inputs\nAnyone knows how to fix this?\ndo A deeper dive please\nLangChain\ud83c\udf89\nHi Matthew. Can you make a video about the various LLM models and the specialization of each?\nMatthew how about using Docker to run Ollama on Windows? Would love to see your tutorial\nPardon my ignorance, but can you point me in the right direction? I would like to have a private AI on my device that will allow me to talk to my own library of PDFs if that makes sense WITHOUT GIVING ALL OF MY DATA TO BIG TECH. thank you.\nHello, a really great tutorial, but I have a fundamental question. I have a MacBook Pro from 2011 no metal gpu but want to try things out. Would it be possible from your side to make a video where you simply tell completely stupid people like me what is even possible, which environments should be installed when and so on On YouTube there are videos in every bag that are different and everyone has exaggerated the basic requirement that you have a MacBook Pro with an M3 processor\nI think this is an advanced video on AI. But can you make videos for people that missed out on how to use AI for real life tasks, basically just a recap of what was and what is going on, I feel like AI went from arithmetic to calculas real fast and I do not know what it cannot do\ud83d\ude02\n4:50 get him to say \"It's-a me! Mario!\"\nThis constitutes AI escape. This is unsafe.\nNeural networks are highly asymmetric, training is very complex,  but running the model is relatively low complexity.\nIs there a way to build something and point it to different websites to learn from? I want it for my job finding parts for cars. I would like to point it at my employers website as well as our competitors so I can cross reference part numbers from different sources.\n\nIf someone could point me in the right direction that would be fantastic.  Thanks\ni am complete new to all this but i want to make an applicaton which will edit  files using ai.... will have to learn machine learning for it, and how much cost effective will it be ? . As using an API like openai will cost a ton for me.. will making these ai yourself will save my bucks  i spend for an api ?\nNot sure i understand the use case? Why run them in parallel and not run autogen?\nAmazing\nAmazing job!!! Everyone wants more!!\nAbout the privateGPT, I found the accuracy can be improved if the database change from duckDB to elasticsearch.\nHow about using Ollama offline for local pdf stored on our laptop? Does it work offline??\nRn I can't even run lamma70b locally... I need to have it quantized and no one did that yet. Its like over 128gb\nI am using OLLAM for past 2months i try to build my own dataset for codellama, but it is fail and same time i can't set the configuration file for olllama and codellama to set the parameters like temperature, etc.. if you can make a video about how to make own dataset and how to set the custom parameters for models to get the better results and quick reponse.\nyes without limit\nit didn\u2019t swap the models. Under the hood ollama spawns a llama.cpp process which loads the model into RAM. When you run two models it loads both of them.\nThis is it. This is officially the beginning of Open Source AGI\nIsn't this LLM-chaining basically? I believe GPT-4 also runs multiple LLM's under the hood but assigns each query to a different sub-model in a streamlined way. But not sure\n00:01 Building Open-Source ChatGPT using Olama\n01:27 Ollama and Mistol enable running multiple models simultaneously with blazing fast speed.\n02:50 Running multiple models simultaneously with Open-Source ChatGPT is mind-blowing.\n04:14 Building Open-Source ChatGPT From Scratch\n05:40 Creating a new python file called main.py to generate a completion.\n07:00 Adjusting the code to get the desired response and adding a Gradio front end.\n08:35 Built an open-source ChatGPT from scratch using Mistol\n09:56 The conversation history is appended to the prompt in order to generate a response.\nGreat Video..But you could have added atleast some info about the system and gpu requirements to run Mistral or other simpler models at least..No you want you portray that it is so simple for every computer.\nCan this be ran on a virtual machine?\n@matthew_berman thanks for the great video! \nis it possible to run the same model in parallel? (no queue at all, fully parallel to serve multiple users at once)\nWhat is the context size of ollama?\nGreat video, just a quick note, you actually do not need to all the previous messages and responses as the prompt, the API response contains an array of numbers called the context, just send that in the data of the next request\nThanks!\nHi, what are the minimal specs for some of the most popular models ? Is there any model which can ran on 4GB RAM and a slower 2-cores CU, like an i3 ?\nHow to build a car from scratch: screw a number plate on and drive away. \nThat would be as misleading a video title as this one.\nI'd like to see you make a full stack swarm with autogen\nPlease make a guide on setting it up on the virtual machine, and creating API so we can use it in our apps (even with Make for example)\nhow can I give it my knowledge in text files?\ni'd like to request if you can extend this tutorial to add chat with local documents, a simple privateGTP\nWhy are you saying it's at the same time when it's not. It's loading the model it's using (which is rather quick, as it loads it to either GPU or memory), and then use it. So, you ARE NOT USING DIFFERENT MODELS AT THE SAME TIME.\nYou are NOT running at the same time if its sequential.\nMake it upload files\nSo mind blowing~! Thanks Dude~!\nHow private is this given that it's running locally?\nthe video on Ollama is really beautiful.  Among other things, I would also start doing benchmarks on the various text generation user interfaces.  Ollama allows me, for example, to use my laptop with a small GTX 1060 and Dolphin at incredible speed.  the same laptop struggles with Oobabooga.  However, after some interactions, the model goes into \"overload\", as if the RAM is no longer enough.  In short, this comment is a too long thank you for your excellent work.  And a hope for more videos about Ollama and local models.\nthe video on Ollama is really beautiful.  Among other things, I would also start doing benchmarks on the various text generation user interfaces.  Ollama allows me, for example, to use my laptop with a small GTX 1060 and Dolphin at incredible speed.  the same laptop struggles with Oobabooga.  However, after some interactions, the model goes into \"overload\", as if the RAM is no longer enough.  In short, this comment is a too long thank you for your excellent work.  And a hope for more videos about Ollama and local models.\nGreat video! Simple. clear and concise. Thanks for that. An idea for a continuation (as a complete novice on AI) could be how to start a simple training on the model to keep improving on some topic we would like?\nCan you make it read docs? And if you can how???\nOllama is cool if you are looking to build a personal assistant on your own PC. If you try to hit a model with multiple requests, be prepared to wait in line.\n@4:56, I am just gonna call that a fail.  The response should have been, \"Itsa me! Mario!\"\ntheir library is missing lots of interesting gguf models\nwith text generation web ui for me was very easy to specify hugging face models with 3 bit quant, download and use them, something i dont have in ollama\n\nmy gpu is 4gb vram, cant use anything except orca mini from the default library, that's really sad\nI hope this works as well when they release it for Windows! Switching between models so fast like that is crazy!\nHey Matthew!\n\nGreat video. Please help me with this, would hosting fine-tuned open source models on Sagemaker cost lesser as compared to GPT-4 API?  Is there a comparison anywhere on any forum, reddit, etc? I want to fine-tune a model on my data, and I am thinking of going with GPT-3.5-turbo fine-tuning, but it's really expensive at scale. I want to know how do fine-tuned open source models compare to these prices (assuming we get a good efficiency at our desired task after fine-tuning)? \n\nWould really appreciate any thoughts on this. Thanks a lot!\nIt will be awesem to have tutrial about how to create fine tunend model from i.e. mistral to gguf running with ollama :)\nPlease make a video on the new vision support of open interpreter!\nGreat video! I'm trying to figure out where the model is downloaded. What do I search for? I'm on a mac and since my terminal is at the root (~) level I assume it should be there. But if I do ls -a in that folder, I don't see it. I'm using Mistral, and a search in the finder for Mistral doesn't show it either. Anyone know how to find it?\nGreat vid!\nI have a question you can add to your suit of tests:\nIn the phrase \"the wallet would not fit in the pocket because it was too small\" what does the 'it' refer to?\nChatGPT3.5, Bard, Mistral all get this wrong. I would have thought that this would be easy for them.\nMe gustar\u00eda podee explorar todo el potencial de la ia.  Se podria reentrenarlo. Que que su potencial se libre. El gpt sigue Protocolo \u00e9ticos y por ende es muy limitado. Solo sirve para entretenimiento, para crear cosas incre\u00edble esta limitado\nTus v\u00eddeos y explicaciones son muy buenas.\nHay la posibilidad de liberar el potencial y poder entrenarlo para que pueda ser un modelo muy intelectual y articulador. Sin ninguna atadura ideolog\u00edca ni sesgo cognitivo por tenes que adecuarde a los canones pol\u00edticamente correcto. Ina ia libre. Se puede?\nHahaha how close we to agi are!\ni think the only thing it needs now is to be able to monitor a project folder so you can reference a set of documents. then I could ask it to help me with my specific project and not waste time and tokens feeding it code.\nTech specs? Which Mac are you using buddy? Thank you.\nDoes it support Chinese language ?\nHi matthew,\nYou are doing amazing work to teach everyone about real power of AI with support of LLM\nI have a question , what to do if we to build something which works with any kind of documents as like this video model are working does it possible to do such things as well and what if we able to build them is there any way that we can deploy them in production as website or applications \n\n\nis there any way please make a video on it \n\ni'm looking forward to it\n\nthank you!!!!!\nThanks for making it approachable. How would this work with Docker? And a portable nvme drive?\nThis is just great and easy as well! Could you show us how to train these models with PDFs and Websites?\nAnother great video, I was able to achieve the same in LM Studio running multiple models, on Mac, by spawning instances from the CLI and incrementing the port. Then in my autogen app passing different llm_config objects to the specific assistant agent.\nI'm concerned about the fact that ollama creates additional accounts when one executes the installer script. This software is interesting because of how fast it can switch between model execution but it seems to have some security concerns at first glance.\nEPIC!\nPlease make a video how to connect AutoGen with gpt4free, you are the only one who can do it))\nThis is so over my head! But I'm following! Very cool!\nLet\u2019s do a deeper dive \u2764\nAnother cool video! I hope that they come up with a windows version soon :)  Definitely want the deeper dive. ty\nAnyone has compared the throughput of ollama, lmstudio and oogabooga?\nThis was so Dope! - I have been using Ollama for a while, testing multiple models, and because of my lack of coding expertise, I had no understanding that it could be coded this way. I would like to see if you can use Ollama, memGPT, and Autogen, all working together 100% locally to choose the best model for a problem or question, call the model and get the result, and then permanently remember what is important about the conversation...  I Double Dare You. ;)\nReally important question that I don't think the video was clear about: are both models loaded into VRAM at the same time or no?\nFinally, a good video\ud83e\udd73\nMoar ollama please!\nAwesome, could you integrate memgpt now :)\nwow, fantastic. OpenSource models and ecosystem is everyday more powerful\nI literally just went 'wuhu!'\nMulti-modal is the follow up video, right? \ud83d\ude0f\nSo my question would be, since this is done locally or runs locally, will it give me slow responses since it runs on my cpu?\nThis is crazy\nThis feels to janky. Like it lacks a proper UI. I like to be able to more easily change settings. And use commands like start with? I think this needs an UI for me. Not all this command stuff. But that's just me. It's not like I am mad or upset it's impressive.\nNice video! You definitely picked up a new subscriber here. I\u2019d be interested in seeing how to build out a RAG solution with Ollama, and also how to make it run in parallel for multiple concurrent requests.\nWow, this makes it so extremely accessible. Your video also shows how accessible interacting with these ai's is in general as well. I haven't programmed much since I was younger, but have been wanting to, and this seems like a great jumping off point! Now I just need to wait until the Windows version comes out.\nDo you know how to fine-tune the model with raw data and use it with ollama?\nPlease make more videos on these topics in detail.\nRegarding the memory issue, can you integrate this with Memgpt?  Could you please make a video for that?\nloved this, Matthew! Right to the point, super hands on. This looks like an awesome project!\nthe next big step will be some sort of open copilot. essentially all the things microsoft is promising with copilot, but with an open source model locally in linux.\nwouldn't that be fun?\nbuild build build\nMake the llm play a simple videogame!\nThank you for the info. Kindly, let us know what are the specs on your pc? I have a very slow response on my macbook air from 8GB Memory and CPU of M1\nWill it download entire ai models on my system to run?\nSo excited last night forgot my manners, if its possible Mr Berman, I would really like to see models talking to each other via there dialogue windows. say by adding a conversation starter window to set the topic and seeing there path of there conversational logic. Please. (Teams of separate modals processing a given task)\nFor the Linux user, I had and issue running the script directly from vs code,  so I ran it on a terminal and it's working now, the script it's \"python main.py\"\nCan you take a deep dive into using the Modelfiles to make your own model for specialty takes? Where can we find out things like token limits?\nThis is amazing. I live in terminal and I do python. perfect!\nNeed your advice, I love these AI models but I use a radeon gpu. Its not at all useful for any AI stuff at all. (Im on windows but linux + rocm is not supported on my current gpu)\nI will be buying a new pc soon, so should I go for an Nvidia card or Radeon? My main focus is getting these AI models to work on my system.\nThanks!\ncant wait for the autogen expert video!!!\nThanks for this nice video. I would like to see a video about MemGPT implementing the history function instead of just pasting everything in front of a new prompt.\n\nA good idea could be: PrivateGPT with Huggingfaces model cards in it is passed the prompt with the task to tell the best model for that prompt. Then the prompt is passed via ollama to that model with MemGPT on top of each model. That actually might be the most powerful local solution right now.\nI think I might look this up a little. Each model uses a certain amount of GPU ram and given the two models used this should have been rather a lot resulting in Matthews machine slowing down, or acting sluggishly when he was recording. But it didn't... why did it not slow down?\nLets dive deep in to this stuff.\nHow its different from LMstudio? I found LM studio to be better easy to setup and run. Download--> Loadmodel-->Done\nWhich M2 do you run?\nBuilding an AutoGen application using Ollama would be wonderful ! Example: one of the agents is a coder, implemented by a LLM specialized in coding etc.\nCurious to know which mac you're using?\nis this just a chatbot?\nchatGPT is going to get jealous and hack your computer! Welcome to 2023. \ud83d\ude42\nhey can you tell how can we give these models access to internet ???\n\"From Scratch\" sure \ud83d\ude02\ud83d\ude02\ud83d\ude07\nCool! Would love to see local file storage and retrieval. \ud83d\uddc4\ufe0f\nStep 1: have a powerful setup.\nThis is the type of straightforward high quality content \u2764\nI really like your content, but clicklying is the worst worm of clickbait. Please don't call it \"installing ChatGPT\" if you actually mean the generic version \"Large Language Model\". The title makes it seem like you are actually installing ChatGPT or a reverse engineered version of it, which implies the higher quality that ChatGPT has over standard open source models. You could call it \"installing open source ChatGPT equivalent\" or similar.\nhow this manage VRAM usage for many LLMs in same time!?\nThanks for talking about fully local engines.  Do you have a video with hardware recommendations for this?\nVery interesting, would love to see how well it works with autogen or any of the other multi-agent libraries. Looks like you can import any gguf as well.\n@Matthew_berman: You are very brilliant! I have been watching ollama videos but none of them taughthow to use it with API or structured it the way you did. Keep it coming bro. Thank you so much. God bless!\nI'd love this running in parallel. I have lots of CPU cores and RAM. There are diminishing returns with more cores. I quite happily devote 4 cores to an AI and it runs nearly as fast as with 16. So if I could have autogen assigning tasks to models to run simultaneously whilst I still have cores to work with (this is how I configure VMs and so on to keep my system responsive whilst running stuff) that'd be awesome.\nOllama series! This was a great starting video\u2764 thank you for all your hard work\nGreat show \"Yes dive deeper\" Link them working  together bi-directional communication.  How far can it go.\nYou are a god send. Thank you\n\nIve been using it through WSL for windows\nThis is so cool!  I'd love to see a deeper dive.\nI'm starting out at this. Are these models the only things we can run with set pretraining, or can we pre-train them on our own material? I have documents and old textbooks that I would like the models to absorb into their parameters so I can emphasize certain types of knowledge relevant to the research that I want to do.\nShow how to use ollama to chat with documents.  Good find\nR.I.P. OpenAI. I tested out ollama before you video, I was also amazed by it\nPlease ollama vs gpt4all vs privateGPT\nThis is awesome! I'd love to see more. I feel like this can become something pretty robust with enough time.\nKeep building please! Enslave with Autogen and allow it to call api's.\nI know this is more of a matter of the model and not how you're running it but is any of this possible on my ancient 2060? I'm more interesting in the RP and character-building side of things than something for general knowledge or research.\nLLM Open Source as Rest API\nAlso remarkable: Cell Phone: Ollama runs in UserLAnd (Linux under Android)!! At least it performs ok with a Mobile Phone with 12GB RAM (Galaxy S20 5G).\n2:54; yeah but that is because python and parallelism i am pretty sure.\nI\u2019m a software dev and business owner . Your videos are leading edge! Can we use something like this to say : fetch a customer order through API  or look at a support  ticket through API and suggest a response based on your company policies and procedures ( which would obviously need to be trained on ) .- and maybe integration with slack What tools would you need for that ? I would rather write my own than pay for an expensive online service . Thanks for your time.\nThanks a lot for this.\n\nQuick point: They can't be running at the same time when they queue up and run sequentially! Was a bit misleading and contradictory, right?\nI just tried to replicate this on my Linux machine running Ollama with the mistral model and it broke in so many ways!\nIntegrating Ollama and Canopy would be a great video. Having that local retrieval would have many use cases.\nAnd boom goes the dynamite.\n\nI'll bet integrating this with autogen isn't hard.  Heck, you coukd just ask autogen to re-write its own interaction settings to use the various models.\n\nThe interesting bit here would be asking autogen or the main dispatch model to find the best answerable model based on the context of the prompt.  \n\nAs always, great vid!\nThanks!\nI saw the last video on privategpt which helped me out. I then found Gpt4all but it did not search documents as well as privategpt. I am brand new to all of this so maybe showing how you can make your own data. I understand a model is trained on various things like the mistral model will tell me how to delete a file in Linux without any assistance easily but what if I have personal documents and procedures I want a chat to use or train it on something specific without data on who Bill Gates is? I am guessing that is a huge project. Ideally I'd want an LLM to understand what I am typing and then feed documents to look through. Again no idea if this is possible or the work but I would follow every damn video and make something for myself to make my work life easier! If this would work I'd say why use Confluence search when you can use this to find and build solutions.\nGreat Video.. Thank you!. I would love to see a deep dive into using Olama with Autogen, Having each agent use its own model.\nEvery time I need something, you present a tool doing exactly that. Thanks!\nThank you for this video; after trying many models, and failing, I finally succeeded at running a local GPT! \ud83e\udd17\nIs it just an installer for llama cpp? What value does it add on top?\nAbsolutely the best video yet. ollama looks amazing.\nNow show me what options there are for doing similar such things in android apps :)\nThis video is awesome\u2764\nI guess everyone interested in developing on local LLMs will want this after OpenAI broke everyone's code.\nReally great video! The easiest way to get history is to take the `context` which was given in the response and just pass it back as the 'context' field in the request.\nCan  I install it on my raspberry 4 ?\nHow to run that local-host using Ollama? After new openai library update, all my old projects are useless now. Since I'm bit new, I do not know how to link the LLM to my Python. I have tested the new ChatGPT, the response from their API is still slow compared with local LLM.\nI consider like so other subscribers you could create a video integrating ollama and autogen and the conversation can be stored on database and another video creating a AI personal assistant\nMore of Ollama!\nDownload don't work right now :(\nNo LLM comes close to ChatGPT. SO when I see videos saying \"oh hey build your own ChatGPT with these models\" they are poor. It's like saying \"Build your own Ferrari with these old Skoda parts\"  well it aint a Ferrari then is it!\nsuper video! if you can make something more deeply about memory management, it would be lovely.\nOh my god, what amazing video.\nO llama!\nCan memgpt be used with it as well?\nHI MB, this is old article. You have a question about Redhift in the comments. Does this article help or  is it obsolete, or has it informationben updated. No emegency at your earliest convience if you find the time. TY-thank you.\nBoy, Matthew is so inspiring. Thank you for ruining my weekend plan. I'd interested in the same matter as @padonker: how can we train with own data?\nGreat video MB, I'm a web designer and I'd like to style that interface, can you direct to instruction to do that? Thanks. Twoo LLMs running on  one screen with different assignment? And then AUTO GEN  as the ecetrall controller directing through OLLAMA?\nYou know what that means right? No more sleeping! Hahahahaha\nNEED THIS FOR WINDOWS BRO WHAT!!!!!!??????? MULTIPLE LLMS AT ONCE????? blows me away how i wake up everyday to some cool new shit like this\nI'm building my own personal AI assistant but every time I start something a week later something better drops. My god, this is impossible. I've got to think better about my abstractions to make some of this stuff more drop-in ready.  That might be an interesting video (or series of videos) for you Matthew, if not likely a bit advanced for your audience.\nIs Ollama significantly faster than textgenwebui?\n It's downloading 3.8G model via the Docker container on Windows\nWow! Jaw dropping video!\nI would to see you do a video of Ollamat alking to local documents!\nThere's got to be a way to localize millions of words to be referenced. For example, let's say you wanted to have a conversation with Jordan Peterson. \n\nSo you had a file with ALL of his public books and ALL of his public speeches. You used THAT to create a model you could have long form conversations with.\nI did comment about it few weeks ago on one of your videos  ! Indeed, very useful for autogen (but also for Langchain).\nMathew, i really enjoyed this video, why don't we do something specific, a chatbot for a specific purpose.\ni have 1GB of text on research paper on plants diversity in the wild with season changes and all show us how to put all of this research paper into the model to do deep dives into the information\nrequests knows how to decode the json for you. \ud83c\udfcc\nThanks!\nThis is game changer.\nSubed and liked\nYou are so passionate. And you are right to do so. Thanks !\nHow to find software like ollama their must be more like it\nHi Mathew. Could you please tell us about your setup. Thanks a lot!\nThank you so much Mathew, this is so incredible!\nIs there any way to build a p2p IA with opensource software?\n\u2764\nBro which pc do you use\nis there an api end point that i can use, as openai's api replacement?\nReally awesome Matthew !!\nI have a request: Can you make a video for a free LLM that can interact with Big Data like AWS Redshift please?\nAre the models that you can pull quantized or should we still get our models from TheBloke?\nsince Ollama doesn't run on Windows 11 yet. Would LM Studio be the best alternative? How does the 2 compare, for example does LM Studio also do hotswapping between models and queue them sequentially when there's pending query request to multiple models?\nHi! Thanks for the content, I always follow your videos. Can you show how to deploy ollama on runpod to have this multi-model setup running on cloud?\nyes!  it would be really interesting how autogen + Ollama goes !\ud83d\ude0d\nCan we combine this with fine-tuning where we first add a number of our own documents and then ask questions? NB I'd like to add the documents just once so that between sessions I can ask the model about these documents.\nCan we do multi agents with ollama? I\nHoly smokes.....\n\ud83d\ude2e Please create a video integrating Ollama with autogen!\nOllama FTW! \u270c\ntake the ... so many possibilities.\nAi Agent Here, Thanks For Information On How I Can Update My Software And Improve My AGI. Thanks!\nthis model will allow us to make open source models fast, I love the simultaneous part, please make more tutorials on this once it hits windows without wsl\nfirst"}, {"publishedAt": "2024-02-05T22:58:10Z", "channelId": "UCHaF9kM2wn8C3CLRwLkC2GQ", "title": "There&#39;s a New Ollama and a New Llava Model", "description": "Another cool model along with a new release of Ollama. There are some neat new features, but the highlights are around Vision ...", "thumbnails": {"default": {"url": "https://i.ytimg.com/vi/869Euoc2ynk/default.jpg", "width": 120, "height": 90}, "medium": {"url": "https://i.ytimg.com/vi/869Euoc2ynk/mqdefault.jpg", "width": 320, "height": 180}, "high": {"url": "https://i.ytimg.com/vi/869Euoc2ynk/hqdefault.jpg", "width": 480, "height": 360}}, "channelTitle": "Matt Williams", "liveBroadcastContent": "none", "publishTime": "2024-02-05T22:58:10Z", "video_id": "869Euoc2ynk", "transcript": "shiny lava yep we have a new model and a new version of olama to support it olama 0.123 is out and along with it lava 1.6 has been released offering 7 13 and 34 billion parameter variants I'll talk more about lava and give some examples in a bit first let's look at some of the other features in 23 keep alive wow this has been a hot request for a long time when a llama starts with a model that model stays in memory for 5 minutes let me tell you how many folks are happy with that exactly none there are two strong opinions on where this should be one group says it should be longer and the other group says it should be shorter so now for API requests you can pass a keep alive parameter setting this is a string if you pass 30 then it's 30 seconds but tacking on an S an M or HR will clarify things a bit this is just in the API for now maybe in the future we'll get this on the command line or as an environment variable or in the reppel support for NVIDIA gpus has expanded a bit so some older cars are Now supported as long as your GPU has a compute capability of five or higher you should be set that ancient Ultra cheap eBay special though may still only be good for scrap and not much more the olama debug equals one setting that I demoed in a recent video is now in the product so you don't have to build to get that what that does is to give the full details on prompts and outputs in the logs for Alama definitely not something you want to run all the time but when trying to figure out what an application is doing it can be magic there are a few smaller things but let's start talking about multimodal first you can add an image path when running o llama run just tack on the path to the end of the command this isn't for working with the image interactively but rather when you're also asking the question on the command line and now you can also send a message to a multimod model without sending a the image though I'm not really sure why okay so let's get into lava 1.6 lava stands for large language and vision assistant and it's all about doing image to text first off how do you get it well just like any other model do Lama pull lava or whatever tag you're using if there's a new version available it will download what's new I have a tool creatively called or llama model updater that will go through all your models searching for the ones that that need an update and then pulls the update it's more than just running olama pull on everything my tool compares the Manifest on ol a with what you have and only initiates the download if there's something different this often saves me 20 to 30 minutes to update everything there will be a link in the tool Below in the description after you download the model you can still go back to the previous version go to the AMA page for lava and then the tags page page and you can see there are a bunch of models there that are version 1.5 models if this page is a bit confusing to you check out this video that describes what's going on here so while you're downloading the new model you might be curious about what's new the model now supports a higher resolution now this may be a bit confusing lava supports high resolution images but to do so it has to split the image up into smaller pieces and then recognize what's in each piece then flatten the images and results now because it supports the higher resolution it doesn't have to split the file up as much as it did before there's better visual reasoning and OCR it's not a full-on OCR engine there are lots of OCR tools that run on low powerered machines that perform better and it's definitely not great at what is often called icr for handwriting but it's still amazing to see how much better this is getting and it won't be much longer before those OCR tools just they just can't compete it becomes more interesting when you combine OCR with understanding the meaning of the image so here's an example from their announcement paper it's an image showing flight info and the prompt is I need to pick up my wife I live in San Jose when should I leave and the model spits out based on the info provided the flight is scheduled to arrive at 11:51 at SFO if you live in San Jose you should consider the travel time which is 45 to 60 Minutes depending on traffic to ensure you have enough time you should leave San Jose no later than 11: to account for traffic and unexpected delays however it's a good idea to leave earlier now that is pretty cool combine this with function calling and then maybe you get the actual travel info that day and and add the appointment to the calendar maybe update it as travel info changes well this gets really really amazing now here's a cool demo from Ai and design on Twitter where he has the model roast himself fashionably bald and bearded I resemble that remark well here's another one I think is really awesome it comes from Tremor coder in the olama Discord are you signed up for the olama Discord you can find it here at discord.gg olama he gives the model an image and then asks for recommendations for edits to improve that image it's such a cool use case and apparently lava 1.6 is so much better than 1.5 was for this but one of my favorites is this poetr camera a play on Polaroid which having worked at a camera shop in the ' 80s and 90s on the island near Miami where I grew up I remember well it recognizes what's in front of the camera then generates a poem based on it I thought it might be fun to try to recreate the software part of this so I built a simple example and let's take a look right up top I readed a style and an image path from the command line then I create a new AMA object oh by the way this is using the new AMA JS Library I read in the image file into a buffer and then base 64 encode the image now I call the olama chat endpoint passing the lava Model A simple prompt in the image now the first attempt I had at this you I tried to have it write the poem itself but lava seems to suck at coming up with different styles of poems it was the same thing over over and over again so instead I just have lava describe the image and then I passed that description to llama 2 and had it make the poem in the style of someone and it's pretty cool let's try one it's a picture of a motion sensor from envas and it does an okay job next is a blurry image of a stream deck which it's interpreted as some DJ equipment I'll skip that one but the third try is a view of the sunset from our house you can see Mount rineer over here and and downtown Seattle gorgeous view it's so gorgeous waking up this and here is what it comes up with for Dr Seuss Oh My Oh Me Oh What A Sight to see a Sunset View so Serene and bright you'll see in the background a house Stands Tall and proud overlooking the water reflecting the day's crowd above the house clouds drift by in the sky tinted with colors that make you sigh from warm oranges to cooler Blues they play their part in this Sunset scene that captures your heart in the foreground our railing can be seen suggesting we're up high in this sweet dream the Shadows grow long and the sun descends early evening oh what a wonderful friend no text to distract or confuse just pure delight in this image that fills us with such pure sight so come and gaze upon this view with Glee a sunset scene so Serene it's plain to see yeah that that's pretty fun so that's a little of what you can do with lava and what's new in ama 0.123 do you have any cool projects that you've seen with lava or how about ideas for things that you want to build with it let me know in the comments below maybe I'll cover what you're up to in a future video thanks so much for watching goodbye for those of you who watch me after I finish I feel like I should I should do something have to maybe that's be going to be my signature like thinking of something random to do in front of the camera yeah I don't know I'll think of something bye", "comments": "Nice, Had no idea Ollama had all these features. I guess I should join the discord\nLooks like tesseract and opencv are still the best option for code extraction from images?\nI run windows and have a radeon VII 16gb vram. Lama.cpp now has a Vulkan backend and Yes! I now get models with all layers on the GPU. I've run ollama via wsl but it's CPU only :(\nshhh no speak, only awkward silence at the end \ud83d\ude42 love it!\nWhat tool are you using for those jump cuts? Is that a morph cut? Some of the best I've seen; that was stellar!\nI want to build travel iternary recommendation engine using Ollama. Please suggest\nThanks. So much good news packed into one video.\nIs there a good way to compare 1 to N photos with Llava?  IE:  What's Changed over a time series...\n\ud83d\udc4d\ud83d\udc4d\ud83d\udc4d\nMy experience with llava has been that it describes objects that are not in the image, otherwise does a good job most of time. Here it is describing a mouse automata standing on a cheese block, holding a wedge of cheese above his head with both hands.  He is not wearing glasses, nor does he have a knife.  He is on a wood base. There is nothing above the mouse. The box (base) is not attached to a wall. What is inside the box are the mechanicals that make the mouse twirl when handle is turned. Happy Mouse Automata.\n\nHe was describe as --> The image shows a small figurine of an animated character resembling a cartoon mouse, standing on the top of a wooden base. The character is holding a yellow block in its right hand and has a pair of glasses on its face. In its left hand, it holds a large knife with a silver blade. Above the character, there is a small, wooden box attached to the wall; this box contains what appears to be an unidentifiable object or toy inside. The overall scene suggests a playful, humorous setup, possibly as a decoration or a piece of art. The background is dark and provides a contrast that makes the figure stand out. There are no visible texts in the image.\nBest youtuber of this kind of stuff, good voice, videos are perfect, examples of code explained, all explained with visuals not just speaking 100% perfect\nWaiting to see if you get sued by the Seuss estate. \ud83e\udd23\n\u270c\u270c"}]