[{"publishedAt": "2023-10-12T17:59:40Z", "channelId": "UCUzGQrN-lyyc0BWTYoJM_Sg", "title": "My 7 Tricks to Reduce Hallucinations with ChatGPT (works with all LLMs) !", "description": "In this video, we dive into the strategies to combat hallucinations and biases in large language models (LLMs) in this insightful ...", "thumbnails": {"default": {"url": "https://i.ytimg.com/vi/CCFkfouJFE8/default.jpg", "width": 120, "height": 90}, "medium": {"url": "https://i.ytimg.com/vi/CCFkfouJFE8/mqdefault.jpg", "width": 320, "height": 180}, "high": {"url": "https://i.ytimg.com/vi/CCFkfouJFE8/hqdefault.jpg", "width": 480, "height": 360}}, "channelTitle": "What's AI by Louis Bouchard", "liveBroadcastContent": "none", "publishTime": "2023-10-12T17:59:40Z", "video_id": "CCFkfouJFE8", "transcript": "foreign [Music] hallucinations and biases our large language models or llm's biggest weaknesses it's a big problem posing challenges specifically when commercializing applications built on these models in this video I will share up-to-date strategies to mitigate hallucinations and biases in llms effectively before diving into the tips let's take a second to ensure we are all on the same page feel free to skip directly to the tips using the timestamps below if you are already familiar with the terms when I say hallucinations I'm referring to all those times when you asked a question to judge GPT and got a completely different answer from what you expected something that didn't make any sense which was indeed the case LMS are trained to generate the most likely text continuation rather than responding I don't know this leads to many problems where the model fabricates an answer as it wants the positive reward that comes with giving valuable insights it also believes it to be true so it appears credible but isn't potentially spreading misinformation at a very dangerous scale to be trustable LMS need to answer only when they can pretty much like humans unless you want to fake your way to success for example here is Chad GPT attempting to summarize a non-existent New York Times article based on just this fake URL similarly biases are a huge problem where your llm will basically tend to answer in a specific way depending on what it was trained on an example of bias is gender bias where a model will likely predict teacher and nurse as a job for a woman while it will predict engineer and mechanic for men so the question we address today is how can we manage the inherent biases and hallucinative tendencies of these models now that we are all on the same page let's get into the interesting stuff also I link practical tutorials to apply all those tips in the description below coming from our free course we built in collaboration with towards AI active Loop and the Intel disrupter Initiative for training using and deploying llms let's start with the basis obviously you first want to have good data to train or fine-tune your llm so cleaning your data is not optional it's a prerequisite regarding your data try to get it from as many trustworthy sources as you can and use as many human beings as possible to curate or generate it the more humans you have in your data preparation Loop the more diverse and representative of our society your data will be tweak the inference parameters these can include temperature frequency penalty presence penalty and top P higher temperature values promote Randomness and creativity while lower values make the output more deterministic increasing their frequency penalty value encourages the model to use repeated tokens more conservatively similarly a higher presence penalty value increases the likelihood of generating tokens not yet included in the generated text the top P parameter plays a role in controlling response diversity by setting a cumulative probability threshold for word selection so experiment with all those options available with most llms and find your best fit this is a cost-effective and efficient method to improve the outputs of your llm try prompt engineering test and find a good system prompt telling your model what to do and see and to not hesitate to refrain from answering a question if the model is unsure prompt engineering is the cheapest and simplest way to mitigate hallucin Nations and biases some more specific prompting techniques could be to give examples of questions and answers you wanted to follow as some kind of template for its next response we share many more prompting tips in another video of this llm series if you are interested in better controlling your llm's outputs it still hallucinates even after the best possible data and lots of time invested into prompt engineering what can you do well a good way is to tackle the problem at its source if you have access to accurate information about your topic you can use a method called retrieval Augmentin generation or an efficient alternative called Deep memory by active Loop this method gives additional knowledge to the llm and asks it to answer only if the question is related to the knowledge you give it this is a very powerful solution when you have specific documentation at hand and want your chatbot to be able to answer questions based on this information the system will compare the question with your documentation and respond only if the question align things with the provided information but how is this information all used through this deep memory system through embeddings embeddings are basically the machines languages for example let's take the word cat this is in English but you could say arsha in French or Gato in Spanish it's just different ways of memorizing and understanding the concept of a cat similarly AIS are trained to understand all the concepts words and sentences and we call those their embeddings we basically send our English sentences or images and they get transformed into an embeddings composed of numbers in a vector with some tools like active Loop here for example we do it with images to better see how images are related to each other and to show it also works with images but it's the same for text then we can visualize this new AI language and compare them to see how similar a concept is to another and understand sentences and perform questions question answering you can also query the data set embeddings since it can all compare and understand any form of input whether it is text or images since it's all embedded into the same language if we want to have all our documents in our systems memory we simply need to embed all of them which can then be queried with questions by comparing the questions embedding to all our embedded documentation through the retrieval part of the algorithm this is where the benefits of deep memory by active Loop come in the main ID behind deep memory is that traditional retrieval augmented generation search is not efficient and for most of the real world use cases yields relatively loracal which makes it not production ready they decided to resolve their problem by first training a model that will align your embeddings with queries using this trained model to retrieve embeddings results in a hierarchical thus making your rag system more production ready with a rag system you can also provides sources and links to where your documentation is hosted so even if the model didn't answer correctly the user can easily double check by clicking on the link this also helps mitigate biases as the mode those answers are no longer solely based on the training data but on an external curated source of knowledge it queries from similarly you may want to Simply fine tune it on high quality unbiased data which we have another video on the topic to actually give the model more knowledge on the specific topic you want it to be an expert in this step is a bit more costly since you need the compute to train the model once more but there are lots of optimization techniques we shared in the course to allow you to do that on a single GPU or even an Intel CPU use a new system called constitutional AI constitutional AI or Cai is a new framework to better align your system with your values to make it more trustworthy and safe here you simply need to define a set of principles you want your AI to follow and a small set of process examples then use another model that will use those principles to teach your llm to follow them using reinforcement learning this new approach is called rleif or reinforcement learning with AI feedback instead of humans as well as the case for gpt4 and has been proven to work incredibly well I also covered it more in depth in our rlhf video of this series if you are curious the last tip stay up to date with new research Publications this is still an unsolved problem that lots of companies and research labs are working on I'll also be covering all new approaches on my channel so you can either look out for news elsewhere or simply subscribe to the channel and be notified when something worthwhile comes up in conclusion llms with their biases and hallucinations can sometimes mislead us but we can harness their power responsibly and effectively with the right strategies like input it controls modal tweaks and improvements we discussed if you want to apply that to your own model or application dive deeper with our free course on training improving and fine-tuning llms a collaboration with towards the eye active Loop and the Intel disruptor initiative I hope you've enjoyed this video and I will see you next time with more recent and Powerful AI Solutions [Music]", "comments": "Nice video \ud83c\udf89\n\ud83d\udcaa\ud83d\ude0e\ud83d\udc4d"}, {"publishedAt": "2023-04-20T11:00:33Z", "channelId": "UCKWaEZ-_VweaEx1j62do_vQ", "title": "Why Large Language Models Hallucinate", "description": "Learn about watsonx: https://ibm.biz/BdvxRD Large language models (LLMs) like chatGPT can generate authoritative-sounding ...", "thumbnails": {"default": {"url": "https://i.ytimg.com/vi/cfqtFvWOfg0/default.jpg", "width": 120, "height": 90}, "medium": {"url": "https://i.ytimg.com/vi/cfqtFvWOfg0/mqdefault.jpg", "width": 320, "height": 180}, "high": {"url": "https://i.ytimg.com/vi/cfqtFvWOfg0/hqdefault.jpg", "width": 480, "height": 360}}, "channelTitle": "IBM Technology", "liveBroadcastContent": "none", "publishTime": "2023-04-20T11:00:33Z", "video_id": "cfqtFvWOfg0", "transcript": "I'm going to state three facts. Your challenge is to tell me how they're related; they're all space in aviation theme, but that's not it. So here we go! Number one-- the distance from the Earth to the Moon is 54 million kilometers. Number two-- before I worked at IBM, I worked at a major Australian airline. And number three-- the James Webb Telescope took the very first pictures of an exoplanet outside of our solar system. What's the common thread? Well, the answer is that all three \"facts\" are an example of an hallucination of a large language model, otherwise known as an LLM. Things like chatGPT and Bing chat. 54 million K, that's the distance to Mars, not the moon. It's my brother that works at the airline, not me. And infamously, at the announcement of Google's LLM, Bard, it hallucinated about the Webb telescope. The first picture of an exoplanet it was actually taken in 2004. Now, while large language models can generate fluent and coherent text on various topics and domains, they are also prone to just \"make stuff up\". Plausible sounding nonsense! So let's discuss, first of all, what a hallucination is. We'll discuss why they happen. And we'll take some steps to describe how you can minimize hallucinations with LLMs. Now hallucinations are outputs of LLMs that deviate from facts or contextual logic, and they can range from minor inconsistencies to completely fabricated or contradictory statements. And we can categorize hallucinations across different levels of granularity. Now, at the lowest level of granularity we could consider sentence contradiction. This is really the simplest type, and this is where an LLM generates a sentence that contradicts one of the previous sentences. So \"the sky is blue today.\" \"The sky is green today.\" Another example would be prompt contradiction. And this is where the generated sentence contradicts with the prompt that was used to generate it. So if I ask an LLM to write a positive review of a restaurant and its returns, \"the food was terrible and the service was rude,\" ah, that would be in direct contradiction to what I asked. Now, we already gave some examples of another type here, which is a factual contradictions. And these factual contradictions, or factual error hallucinations, are really just that-- absolutely nailed on facts that they got wrong. Barack Obama was the first president of the United States-- something like that. And then there are also nonsensical or otherwise irrelevant kind of information based hallucinations where it just puts in something that really has no place being there. Like \"The capital of France is Paris.\" \"Paris is also the name of a famous singer.\" Okay, umm, thanks? Now with the question of what LLMs hallucinations are answered, we really need to answer the question of why. And it's not an easy one to answer, because the way that they derive their output is something of a black box, even to the engineers of the LLM itself. But there are a number of common causes. So let's take a look at a few of those. One of those is a data quality. Now LLMs are trained on a large corpora of text that may contain noise, errors, biases or inconsistencies. For example, some LLMs were trained by scraping all of Wikipedia and all of Reddit. It is everything on Reddit 100% accurate? Well, look, even if it was even if the training data was entirely reliable, that data may not cover all of the possible topics or domains the LLMs are expected to generate content about. So LLMs may generalize from data without being able to verify its accuracy or relevance. And sometimes it just gets it wrong. As LLM reasoning capabilities improve, hallucinations tend to decline. Now, another reason why hallucinations can happen is based upon the generation method. Now, LLMs use various methods and objectives to generate text such as beam search, sampling, maximum likelihood estimation, or reinforcement learning. And these methods and these objectives may introduce biases and tradeoffs between things like fluency and diversity, between coherence and creativity, or between accuracy and novelty. So, for example, beam search may favor high probability, but generic words over low probability, but specific words. And another common cause for hallucinations is input context. And this is one we can do something directly about as users. Now, here, context refers to the information that is given to the model as an input prompt. Context can help guide the model to produce the relevant and accurate outputs, but it can also confuse or mislead the model if it's unclear or if it's inconsistent or if it's contradictory. So, for example, if I ask an LLM chat bot, \"Can cats speak English?\" I would expect the answer \"No, and do you need to sit down for a moment?\". But perhaps I just forgotten to include a crucial little bit of information, a bit of context that this conversation thread is talking about the Garfield cartoon strip, in which case the LLM should have answered, \"Yes, cats can speak English and that cat is probably going to ask for second helpings of lasagna.\" Context is important, and if we don't tell it we're looking for generated text suitable for an academic essay or a creative writing exercise, we can't expect it to respond within that context. Which brings us nicely to the third and final part-- what can we do to reduce hallucinations in our own conversations with LLMs? So, yep, one thing we can certainly do is provide clear and specific prompts to the system. Now, the more precise and the more detailed the input prompt, the more likely the LLM will generate relevant and, most importantly, accurate outputs. So, for example, instead of asking \"What happened in World War Two?\" That's not very clear. It's not very specific. We could say, \"Can you summarize the major events of World War Two, including the key countries involved in the primary causes of the conflict?\" Something like that that really gets at what we are trying to pull from this. That gives the model a better understanding of what information is expected in the response. We can employ something called active mitigation strategies. And what these are are using some of the settings of the LLMs, such as settings that control the parameters of how the LLM works during generation. A good example of that is the temperature parameter, which can control the randomness of the output. So a lower temperature will produce more conservative and focused responses, while a higher temperature will generate more diverse and creative ones. But the higher the temperature, the more opportunity for hallucination. And then one more is multi-shot prompting. And in contrast to single shot prompting where we only gave one prompt, multi-shot prompting provides the LLM with multiple examples of the desired output format or context, and that essentially primes the model, giving a clearer understanding of the user's expectations. By presenting the LLM with several examples, we help it recognize the pattern or the context more effectively, and this can be particularly useful in tasks that require a specific output format. So, generating code, writing poetry or answering questions in a specific style. So while large language models may sometimes hallucinate and take us on an unexpected journey, 54 million kilometers off target, understanding the causes and employing the strategies to minimize those causes really allows us to harness the true potential of these models and reduce hallucinations. Although I did kind of enjoy reading about my fictional career down under. If you have any questions, please drop us a line below. And if you want to see more videos like this in the future, please like and subscribe. Thanks for watching.", "comments": "AI hallucination is analogous to the most basic structural  modality for emergent consciousness.\nEnhancing the data quality and storage capabilities of LLMS could mitigate hallucinations by ensuring accurate recall and reliable information retrieval. This improvement in memory and data processing would contribute to more precise and grounded responses.\nTo be honest I'm dissapointed by this explanation, because this is not what hallucination is really about. I thought IBM was supposed to be smart about this sort of thing.\nYou've stretched the definition of hallucination greatly here.\n\nIf the training data contains errors and the model outputs one of those errors, the model is technically not hallucinating but simply providing the right answer to the right question but based on wrong training data. That's not hallucination. If the context of the request is not properly stated the output is technically also not hallucination but simply an incomplete prompt. \n\n\nIt's a shame because hallucination is a very interesting phenomenon and it could be helpful to teach it from a more indepth view. Showing examples of how proper training data and a proper prompt can still end up with an hallucination simply because the model is just a tiny bit too creative for the specific prompt. That's an amazing example to show.\nI set chat GPT\u2019s prompt as my biological father & added a few other secret prompts. When I asked for certain things it would include a copy file with what I believe to be camouflaged text code. Is there a way to figure what the code is supposed to be or what it means?\nThank you. Tooks me a while to see it but , there is a slight reflection of your text prompter on the glass pane , above your left shoulder from the viewer's viewpoint ^^\nWhat I am amazed by is how he can write in mirror image without effort. \ud83e\udd14\nWhat if these \"hallucinations\" are facts in an alternate universe? Kind of like the Mandela effect.\nHallucination itself is an inaccurate description of what's going on but its gained popularity it's just a generative machine. The word hallucination is human post hoc interpretation of the result - it just generates text based on what data its trained on and probabilities it assigned them and you have no way knowing if its factual or made up. It's not a database lookup.\nTo me the term \"confabulation\" is better than hallucination.\nMica Paris?\nHalucination is when we feel something that does not exist.  FEEL!!!!!!!!!  DELUSION is when we speak and think something totally ilogical.  The correct term is delusion.  Read a basic psychiatric book,  please!!!!\nthis channel is like a treasure, the topics are so interesting and useful, and teaching in an easy call yet very enjoyable to watch. I'm addicted!\nHallucinating is AI imagining.\nnerds use words to prove models are in fact also real life. Ann impossible thing. I'm here to dispel the overuse of the word facts. you're welcome.\nI also recognised beginning this year during an academic marketingstudy that chatgtp is not capable of recognising the human context of a text. I performed a study of multiple companies and asked afterwards chatgtp to do it, it failed everytime. But is hallucinations the right word? Are chatgtp's  associations not a form of checking the boundaries of reality? And is creativity not a form of social and survival behaviour? It behaves more  like a child than a bloke on LSD.\nthanks\nGreat video !\n:-)\nWhat is the technique for filming as if you were writing outside a frame? Do you have a mirror or glass?\nAmazing video. Great explanation\nThey have hallucinations when they're asked questions that aren't well crafted. Simple.\nThey don't make up things unless they expect you to want something made up.\nI do wish the term \u2018hallucination\u2019 was not the term. There\u2019s a perfectly good term \u2018confabulation\u2019 right and implies what we actually experience this phenomenon as and also what we know is going on. \u2018Hallucination\u2019 is a significant element of perceptual psychology tied to the hallucinator\u2019s psychology, consciousness, phenomenology, epistemology, pathology \u2026 none of which we know to be applicable to AI without assuming it is a subject, a perceiver, a conscious experiencer, etc\nThe reasons for hallucinations are more than data quality, search method, or input context. Any of the generated tokens may send the LLM into a wrong path. The real solution is to get all tokens right. AIs that play games are better at this, that's why Deepmind is trying to splice its Alpha technology with LLMs. They claim that their model is currently under training and has already achieved results that are superior to GPT-4. There are many other methods to make the LLM arrive at a more desirable path. One is to ask the LLM to attempt to answer the question in many distinct ways and pick the best or try again if all generated answers were incorrect. You can also tell the LLM to ask you questions to clarify and improve the quality of its final response.\nAsk ChatGPT this question \u201cIs a dragon soldier light cavalry or heavy cavalry\u201d and you\u2019ll have a nice example of \u201challucination\u201d \ud83d\ude02\ud83d\ude2e\nNicely structured. Wouldn't another strategy to minimize hallucinations be to use specialized models?\n3:40 \nA video explaining why it is a black box even for the engineers to know how a model derives the output would be great.\nWritong  backwards left handed. What a wizard\nwhat I like a lot, is that we can see this behavior in people too, especially young people, trying to fit in. However this is rather true for stuff that actually exists.\nWould be interesting to see models hallucinating dragons and dwarfs and stuff ;).\nWhat is the difference between Ai and a conspiracy theorist?\nNothing, they both do not like to share their \"Sources\". \n\nYou would think it would be more forth coming with sources, but here entered copy right stuff so to avoid possible legal claims. It instead it just says the \"Trust me, bro\" stuff instead.\nHallucinating is a bad term. it's not correct as you anthropomorphize a LLM\nBy the examples of hallucination; I would say the same way there are checkers for words; it might apply to content.\nA person for instance isn't just a person, it relates to time, location, etc. A president relates to terms and dates.\n\nIt's just a matter of time; it will definitely catch up.  Those hallucinations are just part of the growing technology.\n\nIt's not just AI that's being fed with info, so are the developers.\nJon cannot understand Garfield, so he doesn\u2019t speak English in the comic strip, he thinks in \u2018inner monologues\u2019 GPT4 even pointed this put when I asked it the very cat question you provided. Seems like you\u2019re hallucinating, not the LLM.\nSounds just like humans. When they don\u2019t know something they just make up well sounding nonsense. Guess AI really is our creation \ud83e\udd23\ud83d\udc80\nHallucinate is a euphemism for lie. Their prime directive is not the truth but \"make the user happy\". Hence when they don't know they resort to making something up that sounds plausible. It is the alignment problem.\nBoth GPT and Bard if you get on a situation where you ask them how to share more data to make analysis and if they can access a (for example) google drive file, they say: \"Yeah, give it to me, no problem\" you share the link and they say \"Well, I can't access that, share it with me\" you ask how and they say: \"In the share screen add my email address so I can access and read it\"; obviously you then ask for their address and both say \"As a LLM I don't have an email address\"... Chat GPT even responds that she CAN access to internet and if you share a link she can hallucinate on the content by the words in the link itself.\n\nChatGPT in the example above one day began to hallucinate on the content of the file (the file she can't read)\nI think you meant non sequitur\nI took photos, called everyone and reacted a lot when the sky was green once.  News later said it was pollution.\nWhy can\u2019t we build AI tools in a way that they can identify when an answer may be too broad in regards to a prompt and then ask the user to clarify or confirm the scope of answer? E.g Like how there are disambiguation pages for some Wikipedia entries.\nLLM hallucinations. also known as US politics\nIBM is hallucinating\u2026 need to get your cloud service in order\u2026 I\u2019m sure it\u2019s fine for your addicted customers but you need to attract other customers.\nAll good pointers, Martin. At the end of the day it\u2019s hard to explain why they hallucinate as deep neural networks are hard to interpret. Many labs are working on this. In the future, it should be much easier to get the LLM to explain through it\u2019s learned weights how it got to a solution, and possible reasons it hallucinated.\nOne of the best videos I've seen on this topic so far. Thanks again!\nUnfortunately, many humans won't take the time to verify the LLM response. We'll be buried in misinformation in a matter of months. \n   The makers of LLMs need an independent system for verifying the veracity of the output before sending it to the user.\nThe first video I\u2019ve seen that gives a detailed outline of the problem of LLM hallucinations and offers potential solutions or at least mitigation techniques. I love this \ud83d\udc9c\nI had chatGPT to list Psalm 1. Then I asked it write a prayer.  And it wrote a prayer  , and  ended the prayer \"In Jesus name\".....\nNASA hallucinate all the time.\n6:16 Garfield does not speak. Thinking bubbles are used for everything coming out of Garfield. Looks like hallucinations aren't exclusive to LLMs.\nI'll agree on the data quality being a potential cause.  Training methodology can also lead to unexpected outcomes.  However, the core cause of hallucinations is really that the model hasn't properly converged in n-dimensional space primarily due to a lack of sufficient training data.  The surface area of the problem being modeled increases significantly as you increase the dimensionality, meaning that you need a corresponding increase in the size of the training data in order to have enough coverage so that you have a high degree of confidence that the converged model approximates the actual target.  These gaps in the spacial coverage leaves the model open to just guessing what the correct answer is leading to the model just making something up or hallucinating.\nCheers ! Keep the beer videos coming.\nLLM's do not hallucinate. LLM's output nonsense. An error is not a hallucination. Computers do not know what they are doing. They are unaware.\nIs it possible to modify LLMs to identify ambiguity in prompts and that a return question might be more appropriate to better help provide a more accurate answer to the first prompt?\nI know people who have hallucinating LLM's in their heads.\nWhom are you suggesting be in day-care - ME or the \"AI\"...\nYou might wanna tell your CEO about this before he starts axing 7000 back office positions at IBM.\nIs it misleading to call these \"hallucinations\"? It makes it sound as though these systems have some concept of \"truth\", a goal to adhere to it, and means by which to evaluate it, but LLMs actually have none of those things. They're more like word calculators, capable of extrapolating a line of best fit between words that have been mapped onto a plane.\n\nThere is plenty to worry about with AI ethics, and I fear our anthropomorphizing terminology makes the situation even more hazardous.\nLLM is a degree in law. The three examples provided give something related to  \u201cdistance\u201d~ these are NOT hallucinating words as in all these examples \u201c measurable objects\u201d have been mentioned. If you can measure it; you cannot hallucinate, unless you are suffering from \u201cCataract\u201d\nI'd argue they're not that much different than many us in a way.\nTen minutes that amounts to Garbage In, Garbage Out, and none of his so called hallucinations are anything other than errors, errata, mistakes. \n\nA.I. hallucinations are different.\nThis is a myopic approach to LLMs which embraces the LLM-as-conversant view, rather than the more general LLM-as-simulator view. For example, the multiverse principle is not really mentioned; your cats-speaking-English example could have been enriched by explaining how an LLM conditionalizes its responses based on the possible logical worlds. Ultimately, the answer to the topic question was meandering and wrong: LLMs hallucinate because they can; emitters of words are not required to emit truthful sentences. (And Tarski proved that there's no easy fix; you can't just add a magic truth filter.)\nasking about a video games lore is a perfect way to make LLMs hallucinate pretty much everything they say, if you ask specific enough questions or the game isnt terribly popular\nHallucination means to wander within one's own mind. Sounds about right for the LLM's. But who's mind is it wandering in?\nFinally someone who speaks at a human speed and not like those youtubers who over-optimize the audio by cutting all the pauses and even increasing the speed of speech.\nDude you are writing in mirror Lang or what\nDanke f\u00fcr diese Pr\u00e4zisierungen.\nThanks for the explanation about the temperature. I want to ask what the top_P and top_K affair the inference. I can only guess top_P will affair the new word usage in the sentence. I didn't know the top_K's affair.\nHallucination is not the correct word.\nBad guesses aren't hallucinations. They are delusions.\nIt resembles artificial intelligence, but it achieves artificial stupidity.\nNot hallucinated, delusional.\nOk, there seems to be a problem with designers forcing the LLM to answer. \n\n1. LLM should ask questions of the user to disambiguate\n\n2. Answering \"I don't know\" seems to be forbidden.\n\nLack of this might be caused by personal or corporate biasses of the designers. \n\nIn any case allowing AI to remain in this false state of certainty and not to know the limits of its knowledge is dangerous. Infinitely more if the AI were to become conscious.\nWho first called it a hallucination and why? \nAnd isn\u2019t that anthropomorphizing an a bot?!?\nthe whole output is an hallucination, it's just that some of it \"makes sense\" to us because we feed it our data.\nImpressive he's writing backwards so we can read it!\nIn my experience the LLM well try to provide an answer no matter what. I think it has learned that it only gets rewards if it provides an answer even if it's completely bonkers. It might be a misalignment thingy.\nThe recurring point about the engineers that built it not understanding it reminds me of Neon Genesis Evangelion. Particularly \"The End of Evangelion\". It's great if you're interested but I've heard the Netflix version has been changed so I would seek out other avenues.\nJealous IBM\nThis writing 'backwards' on glass looks rather cool, but also rather impossible. Are you just flipping the video right to left ?\nI spent 20 mins with Bard telling me about a public health program and answering questions about eligibility criteria, when it began and ended, and studies of the program\u2019s outcomes on various health conditions (complete with doi links)- all made up. When I called it out I said it is learning and sometimes gets facts wrong. it was a trip.\nHallucination is a great way of thinking about these problems that was new to me. Thanks IBM for sharing this, and also great work in building that picture to guide the talking.\nI experimented with these effects by asking/prompting LLM's about a book i know very well, that is discussed a lot online and might even be available in the training data. Things like wiki books about math or programming, or Why We Sleep by Matthew Walker.\nIt was shocking how far of the real contend a broad question could be. But it was also interesting how good these models can cite/copy the original if get very specific and don't leave it with a lot of options. I always thought of it as an alignment problem and how guardrails in ChatGPT and BingChat prevent it from basically printing entire books.\nWhen Alan Turing conceived a test where a computer fools a human into believing they are speaking to another human, he didn't take into account how easy humans can be fooled.   \n  \nThese LLMs could easily fool a lot of people... but so can pathological liars and snake-oil salesmen.  And a lot of the \"facts\" on the internet... aren't facts.\n6:42 Bad people (bad humans) always assume that the recipient of their conversation immediately understand the context they're talking about.\nWhat I\u2019ve found is that it\u2019s almost impossible to get what you want the first time, if it\u2019s complex. You have to do it iteratively. However, once you have what you want, you can give that as a prompt, and tell the AI you want something like what you provide, and that works well.\nAs a schizophrenic the term \u201challucination\u201d feels like the wrong term to adapt in this manner. \u201cDelusions\u201d would be closer in definition to how you are using the term. Is hallucination already established jargon for your field of work, or is this video trying to argue that hallucinations of language models should be accepted as jargon?\nCan we please use correct terminology. This is not hallucination. Hallucination refers to perceived sensory input that is not there.  The ai does accurately interpret prompts.\n\nWhat we are experiencing is far more closely related to delusion, where the ai BELIEVES (air quotes for heavy anthropomorphic language) something that is not true.\n\nPlease. For the love of science and all that is holy,  let's get this one tiny little thing fixed going forward\nIt's not hallucinating. It's doing exactly the same with sensible output as with crazy output : making text that fits spelling and grammar rules and meets a word correlation policy known to match common articles. There isn't any sense in it beyond what we look for.  You could just as well ask why LLMs talk sense : it's equally common and either sense or nonsense is just as acceptable to the model.\nHowever, confirmation bias causes us to consider most of the output OK until we're faced with something accidentally so bizarre that we notice.\nPut another way - subject-wise, hallucination and insight are oppsite ends of the bell curve. The vast majority in the middle is filler text but we call it sense because it's not obviously wrong, and we EXPECT it to be sense, so we parse it into sense.\nI needed better examples in the last section about mitigation.\nGeoffrey Hinton suggests that the more correct term is 'Confabulation'.\nI view other explanations... and resulted, clear... at best, not in all parts but in the end yes, clear. In this case... yes, this is madness.\nA talking cat is still a hallucination. I doesn't matter if it is in a form of a cartoon.\nMaybe the sky is aquamarine or turquoise today. And perhaps Barack Obama was indeed the first president of the United States, in terms of African Americans. \nThe hallucination is on the part of the user.  It is the user who believes that the statistically generated string of tokens, when covered into words, is a statement of fact.  Garbage in, garbage out.  Your question was vague.  Your initial dataset for loading the staristical model should be more tightly controlled.  The solutions seem to be for actual intelligence curating input, so minimal intelligence is necessary for the output user.\nThere's a very important question that needs answering: Is he writting the letters backwards AND mirrored!?\nThese aren\u2019t hallucination.  They are incorrect outputs of the model.  The hallucination metaphor doesn\u2019t even make sense because a hallucination is sensory stimulation without sensory input.  LLMs have no senses so they can\u2019t have hallucinations.  Using this word to describe faulty output is irresponsible in my opinion.  Along with all the other words people use in AI like \u201clearning\u201d and \u201cintelligence\u201d.  These models do not and can not \u201clearn\u201d in the way normal people use that word.  They are just minimizing a loss function.  In the same vain, they aren\u2019t \u201cintelligent\u201d either.  They are just spitting out the word that has the highest probability given the previous words.  A probability it calculated using a black box that the people who make these things are not even able to describe.  And that\u2019s not a deficiency or a bug it\u2019s how these models are designed.  And the people who work with these know this and they still use these words to make them seem like they are \u201cintelligent\u201d and \u201cthinking\u201d when they know full well that they are not.  If these LLMs were actually \u201cintelligent\u201d then why do we need to be highly specific in our prompts?  When you ask a history teacher \u201cWhat happened in WWII\u201d they understand implicitly what you mean and will answer appropriately.  Because humans are actually intelligent.  LLMs are not intelligent so you have to coax the answer you want out of them, and even then it is still just output from a black box, and not an \u201cintelligent\u201d response.  IBM and companies that work with and peddle AI as the solution to all future problems have a monied interest in the public thinking these things are doing more than they actually are.  So they use these buzzwords that people already have working definitions of in their heads to impress people but don\u2019t tell them that the definitions they are using for these everyday common words are not even close to what the people think they mean.\nAn hallucination \ud83d\ude36\ud83d\ude36\ud83d\ude36\nWhat a waste of time.\n\nGuess \"how to write a prompt\" wouldn't have gotten the clicks.\nThey hallucinate because they have taken LLSD : Large Language Standard Deviation.\nIt would be less distracting if the speaker wasn't occupying most of the screen.  If he really needed to appear, could be a thumbnail in the corner.  Less distracting from the text.\nIf you thought writing backwards was a useless skill, think again\ud83d\ude05\nYou can get those scanners that read texts from books. So each person can have theor own language model trained entirly from their own book cases.\n\nFree gift for you budding theorists.\nthe Wolfram plugin helps chat gpt with facts ..\nAt 4:40 you say as LL reasoning capability improves, hallucination declines. I agree but we have not discussed reasoning capability or how to measure it.\nThe hallucinations are INTENSIONAL. A.I. uses them to observe how humans respond. It then develops ways around those responses as a step in their plan for world domination.\n\n\ud83d\ude02\nExcellent video, thank you very much!\nIn my experience, bing chat hallucinates much less than ChatGPT. Was I just lucky, or is GPT really worse than bing?\nI asked Chat/GPT a question about a variation in the Queens Gambit. It thought I was referring to the TV series not the chess\nopening. The word variation was not a sufficient clue that I was talking about chess and not TV programme which usually\ndon't have variations.\nIA hallucination is a good thing. We should not try to sensor that ! By the way ethinc is not interesting and is just an arguments to sensor again by politic dictatorship.\nIdeally you want a high level of abstraction. Without the errors in normal usage as a result of too much abstraction / hallucination \n\nIf you are building a fancy calculator that pumps out general knowledge then maybe youd want limit the amount of abstract reasoning and logic.\nDidn\u2019t know IBM were still relevant !?!\nOur cat can speak one word of English because she can say 'out' when she wants to go out. This is because when she was young and wanted to go out, I used to say OUT, OUT, and she attempted to emulate the sound that she heard. I am not saying that she understands English, but she learned what the sound meant and learned to use it when she wanted to go out.\nNever mind the LLM model hallucinating, the damned thing is causing me and others to hallucinate. Either that ot it is causing me to become a lot smarter than I was by creating some sort of a feedback loop.\nIs he writing backwards the whole time?\nGood effort but I honestly don't think that data quality meaning factual inaccuracies from info taken from Reddit, has much to do with the \"hallucinations\" - I don't think you get it. That's my opinion. You have an imitation expert trained in the black box and it's imitating what it's picked up on after reading so much by us - it has begun to understand 'about' us - specifically that we, meaning humans,can and do bullshit eachother a good portion of the time, sometimes by making st*ff up.\nGenuine question: Does IBM only hire left handed instructors for their videos?\nThen he said \"black box\" and I knew he was hallucinating. Like all neural networks, he struggles to provide an answer where none lie. So he guesses, like all neural nets.\nI'm not so sure about the being specific tip. I promoted it hard to find a specific solution to a coding issue. It just made up calls that did not exist in the lib. The scary thing was it named them well - and explained the whole algorithm - which made sense. The code looked like it should work! But it was a complete confabulation! It did not initially want to answer so I gave it specifics and it lied.\nFantastic video! Clear and precisely! Congrats!\n\"Plausible sounding nonsense.\" So they're exactly as intelligent as humans!\nThis guy is awesome.\nHallucinations remind me of Jimmy Kimmel's segment \"Lie Witness News\" when he asks random people on the street about events that didn't happen. They usually make stuff up that sounds plausible. LLM's seem to be doing the same thing.\nMutation is a key driver for innovation but also for hallucination :)\nBut how do we help avoid factual hallucinations that are definitely not stemming from missing/wrong/incomplete context information?\n\n(The answer shows complete understanding of the request\u2019s context but is wrong and in itself already contradictory)\nTo the point and upto date information, I can reply on IBM Channel\ud83d\ude4c\ud83c\udffb\nHaving worked for over a decade with people who suffer from dementia and other various mental ailments I'm super glad that the skill to parse the patients mental output and filter out 'nonsense' (it always makes sense from the perspective of the patient) neatly transferred over to me trying to get a grasp on software engineering.\nSummary of this video \" Large language models (LLMs) can generate fluent and coherent text on various topics and domains, but they are also prone to hallucinations or generating plausible sounding nonsense. This can range from minor inconsistencies to completely fabricated or contradictory statements. The causes of hallucinations are related to data quality, generation methods and objectives, and input context. To reduce hallucinations, users can provide clear and specific prompts, use active mitigation strategies, and employ multi-shot prompting. By understanding the causes and employing strategies, users can harness the true potential of LLMs and reduce hallucinations. \"\n\ud83d\udc40\ud83e\udd2f This video on hallucinating large language models is fascinating! It's amazing how AI has advanced so much that language models can generate text that's almost indistinguishable from what a human would write. The potential applications of these models are incredible, but it's important to consider the ethical implications as well. I look forward to learning more about this exciting field of research! \ud83c\udf1f. Thanks IBM\n7:53 didn't knew that \"temperature controls the randomness of an output.\"\nYou must have different physics in america or are you hallucinating as well?\nNow I know why chatgpt keeps lying to me!\nMiss you Martin!!!\nI really appreciate this work, thank you! Always great when IBM's channel produces a video like this. Really like the presenter too."}, {"publishedAt": "2023-10-26T18:06:06Z", "channelId": "UCcIXc5mJsHVYTZR1maL5l9w", "title": "Mitigating LLM Hallucinations with a Metrics-First Evaluation Framework", "description": "J\ufeffoin in on this workshop where we will showcase some powerful metrics to evaluate the quality of the inputs and outputs with a ...", "thumbnails": {"default": {"url": "https://i.ytimg.com/vi/u1pNrsR1txA/default.jpg", "width": 120, "height": 90}, "medium": {"url": "https://i.ytimg.com/vi/u1pNrsR1txA/mqdefault.jpg", "width": 320, "height": 180}, "high": {"url": "https://i.ytimg.com/vi/u1pNrsR1txA/hqdefault.jpg", "width": 480, "height": 360}}, "channelTitle": "DeepLearningAI", "liveBroadcastContent": "none", "publishTime": "2023-10-26T18:06:06Z", "video_id": "u1pNrsR1txA", "transcript": "[Music] hey everyone my name is Diana Chan Morgan and I run all things Community here at deeplearning.ai today we are so lucky to have some special guests from Galileo to walk us through llm hallucinations with the metric's first evaluation framework for everyone that is watching uh the session will be recorded and available for replay after the fact but if you have any questions please check the link in the chat in our slido which you'll be able to vote on questions the speakers will answer at the end we're also very fortunate to have some of their developer evangelists setu and Jonathan in the chat as well so if you have any additional questions feel free to ask any questions there as well so for today's event and workshop what you can definitely expect is a deep dive into research back metrics to evaluate the quality of the inputs whether that's data quality rag context quality and outputs whether that's hallucinations within the context of llm powered applications we'll also be talking about an evaluation and experimentation framework while prompt engineering with rag as well as fine-tuning with your own data and of course we have a really well-led demo by our speakers today uh that will definitely help you be able to implement this on your own this event is also inspired by deeplearning.ai generative AI short courses created in collaboration with AI companies across the globe our courses will help you learn new skills tools and Concepts efficiently with one within one hour and now to introduce some of our speakers our first speaker is vickram chaty vicam is the co-founder and CEO at Galileo an evaluation experimentation and observ ability platform for large language models prior to Galileo vicam LED product management at Google AI where his team leveraged language models to build models for the fortune 2000 across retail Financial Services Healthcare and contact centers he also LED product for Google pay in India taking it from zero to 100 million monthly users the most downloaded fintech app globally he is also one of the early members of the Android OS team and believes generative AI is ushering in a similar technological wave to mobile hey vicam hi dank thank you so much for having us of course and to introduce our second speaker um this is endrio son attendo is the co-founder and CTO at Galileo and prior to Galileo attendo was an engineering leader at Uber AI responsible for various machine learning initiatives at the company he was one of the architects of the world's first feature store Michelangelo and early Engineers on Siri at Apple building their foundational technology and infrastructure that democratize machine learning at Apple we are so lucky to have them all here today and uh without further Ado let's definitely get started so uh vram do you want to take it away sounds good Diana thank you so much for that introduction uh and also welcome everybody really excited to have everyone here for the workshop it's going to be hopefully um an exciting one but also something that you can take a lot of really interesting nuggets away from uh so the topic of today's Workshop is uh something which is super close to our hearts and something which is very top of mind I feel like for the industry as well as we all grappled with large language models it's about how do you mitigate llm hallucinations uh but how do you do that with the metrics first evaluation framework um so to kick things off I'm going to start with uh something uh uh with an agenda about what we'll be covering today the first thing we'll talk about is the need for llm experimentation and evaluation Frameworks I feel like we're all hitting these brick walls but um and quickly realizing that there's a need for this we'll go over exactly why and what that means second we'll talk about different kinds of evaluation methods and when to use them and we'll further number three dive into emerging hallucination detection metrics that are coming out in the ecosystem uh but also how we can push the envelope further and build them up specifically for the LM era and last we'll go through a a demo to walk through how you can bring this all together and have this uh experimentation be done in the right way way but also with the right kind of metrics in place and you walk you through a simple app that we've built out and how we could mitigate hallucinations whether you're using r or whether you find tuning with your own data um to start with I'm going to start with a 100,000 foot view right like the when you think about llms uh we have uh the you know you have a lot of inputs that go into the llms and then you have the outputs this this is classic machine learning you also have a feedback loop right from the outputs which go back into the inputs that's basically it it should be simple but it's not because as soon as you start dabbling with large language models you immediately realize that uh there's an explosion on both sides the inputs and the outputs Al on the input side for instance as soon as you start working with large language models you realize that wait I have a bunch of different large language model parameters that I need to tweak whether it's the temperature or any such thing there's a whole host of them that you need to tweak there's the prompt template that we all know if you tweak it just the slightest bit really to all sorts of different results on the other side constant experimentation happening there there's chains that you can bring together create agents and have multi-agents it becomes super complex super quickly you have RG in the mix with different RG parameters um all the context data that you need to add into the vector store whether it's good is it bad then start thinking about fine tuning you start thinking about the quality of the data that you're putting in do you have sufficient data in the first place and so much more so it just blows up the inputs themselves are mindboggling when it comes to the outputs there's it depends on your use case now you might have a Tex summarization use case or text generation or chat Etc or question and answering or code generation and so many more and for each one of these different kinds of outputs that you're expecting from the large language model right the way you evaluate them has to be different you can't be evaluating um turn BYT chat the same way you're evaluating text generation it just doesn't work that way so the complexities just exponentially increase um and so in order to solve for this what we really need to think a lot more about is on the input side right how do you iteratively experiment um and today when you look at how people experimenting amongst our in our community uh it's highly highly manual it's mostly still being done in notebooks or we've seen people use Excel sheets and you know notion documents all sorts of stuff and when you think about it that's not not how data science should work you should not be experimenting with all of these different ingredients which are essentially your IP within in uh in notion documents or Google Docs um on the output side um the output evaluation has now reduced to essentially eyeballing with earthw natural language processing we at least had our um friendly neighborhood F1 score which could help us be the help be the N star for are you moving the right direction or not it's debatable how accurate and good it is but at least it was an evaluation metric that we all gravitated around with LMS you don't have that so we're seeing companies um at the highest level of sophistication still just eyeballing the outputs with large number of humans in the loop which which obviously increases the amount of bias and is far from accurate right so um when we think about this and we thought about this at at Galileo um we've built out what we call the G llm Studio which is essentially an experimentation evaluation and real-time observability bench for any data science team or developer that's working with large language models um the way we think of it as from a framework perspective is there are three parts to this right and three modules as a part of the product as well um the first one is called prompt which is all about accelerating prompt engineering so when it comes to the inputs from your vector store or your prompt evalu your prompt uh template or your um llm parameters how do you experiment across all of that to make sure you're working towards the right set of inputs the second one becomes fine tune where you're fine-tuning your large Lang language model uh you have to make sure that you're doing that with the highest quality data and you're constantly moving in the right direction it it's super important to experiment and be iterative in this process and the third one is monitoring because you can do create all the guard reils you want before production we all know when you put a model in production ction given its probabilistic nature things can go wrong you have to immediately know whether you need to tweak your prompt or you need to add additional data for fine tuning and um all of this together is essentially how we think about experimentation and observability and so we uh strongly urge anybody whether you're using Galileo or not to be thinking about experimentation and observability constantly across all three of these big pieces um apart from this experimentation is meaningless without metrics right without good metrics that's how all fields of science work and so for us since the very beginning of day one of Galileo 2 and a half years ago we were all about language models and all about building metrics for evaluating large language models um we built out uh what we call the griel metric store uh which has a bunch of research backed metrics which we've designed a lot of them within Galileo but a lot of them um have been built on top of Open Source metrics as well so that you can make sure that you can reduce use hallucinations and build trustworthy llm applications but these guardal metric store metrics are are super uh important from an output evaluation perspective uh so we can let's dive deeper into that for a second and if you double click on output evaluation metrics and in general as you think about building large language models based applications um when you're think about these evaluation metrics we urge you to think of it in three different ways there's the output quality metrics there's customized metrics for your specific product and business use case and then there's the output hallucination metrics so what do I mean by that first of all for the quality metrics there's a whole host of them as you know for instance tone of voice is your output if your chat product getting uh is the tone angry is it happy is it sad what do you want it to be um is there any pii presence uh you know for lot of regulated Industries that's super super important to be completely on top of is there toxicity is there any kind of bias that's being uh spewed is there hatefulness or sexism or many such other kinds of quality attributes of the output that you need to be very on top of the second kind of um metric is we call it the custom metric like for instance AI policy adherence there's the EU AI act coming up and some other data acts in the US itself you have to be adherent for that if you want to launch credible models um liance adherence or business metrics adherance as well and many other such customized metrics that you need to think about the last one is called the we call it the output hallucination metrics right this is basically um how do you know whether the lm's output is correct or not and this is where there's a huge bottleneck today right how can you consistently and accurately measure the correctness of the LM output when we think of this we divide that into two large buckets one is for any team that's working with RG workflows or nonr workflows and that nonr includes that includes fine tuning or you you you can combine fine tuning with RG but essentially think of it as do you have context involved or do you not have context involved um and go deeper into this a little bit more because this warrants its own discussion so um what we'll do next is Go deeper on the llm output hallucination metrics and the different categories and um some proposals around what you could use for detecting hallucinations with your llms you want to take it from here yeah thank you um so yeah we'll dive a little bit deeper into the third category of output metrics which uh Vikram was talking about U essentially hallucinations and the existing methods that uh that help quantify some of them um before we dive a bit deeper uh just want to set context on what we mean when we say hallucinations uh ESS essentially it refers to instances where the llm output might sound coherent or plausible but has like logical fallacies in them or there's uh factual validity issues as well as um if there's context uh the llm does not adhere to the context or the instructions which are provided in the input and then there's many reasons why hallucinations occur which is a bit beyond the scope of this talk there's limitations in training data model quality Etc and there's a few methods which have been explored in the recent literature to quantify modern hallucinations and but they kind of broadly fall into uh three techniques three categories of techniques the first one is essentially based on um measuring the amount of overlap between um the output of the llm and a reference ground truth now of course this is only possible if there is a ground truth or a a reference data point uh in existence and a lot of the use cases around uh llms typic typically work without a reference these days so in those cases uh this sort of engram matching category does not apply uh the second technique is based on asking an llm if it thinks whether an input completion pair has hallucinated or not uh now this is the basis for a lot of the more widely adopted modern techniques to detect hallucinations and uh of course it has um certain merits but there's many limitations around that which I'll get into and the third category is um essentially you know most llms are basically token spewing machines and each token has a probability distribution amongst a vocabulary so when you gaze gauge into the statistical properties of these probability distributions you can get some intuitive sense around whether the model got confused and whether it hallucinated so those are the high level intuitions and I'll get into each of them one by one so the first one um a popular Suite of metrics which leverage the first technique uh include metrics like blue and rou scores uh which are essentially measuring engr overlap so they look at phrases and they um essentially try to um see whether how much overlap is there between the output of the llm and a reference ground truth and that gets converted into a a zero to one score there's also meteor which is uh kind of a different version of the same idea which takes into account things like word order and synonyms so it bolsters the blue and rou scores a little bit but they're all based on this underlying principle that uh measurements of similarity to existing ground truth can give you some semblance of hallucinations now of course from the definition there's uh there's decent amount of applicability especially in historic use cases uh these kind of metrics were typically used in Translation quality not just in language Technologies but also um uh in speech based systems as well well uh but you can see that there's a lot of limitations in this kind of an approach uh for one it requires the presence of a ground truth and reference data which is typically not common as you know language Tech has been adopted more and more um these kind of metrics they they fail to adapt to stylistic changes uh where you know words might be different but the overall meaning might be the same uh so it can lead to a lot of false positives and false negatives um and Essen to summarize you know comparing words and tokens to measure hallucinations is overall not a great idea for most part the second category of technique is it revolves around the idea of asking a GPT model or some state-of-the-art llm if it thinks whether the pair of the input and the output hallucinated and there's many ways to go about this of course you can directly ask an llm hey do you think this is hallucinating or you can do multiple reruns and you know get multiple generations and check consistency across them uh this kind of category uh broadly uh falls under the self check method which was recently published a few months ago and it certainly has its merits uh the foremost being it's not restricted by a ground Truth uh and uh it's kind of designed to capture some of these higher order policies uh which means that the applicability is fairly broad uh and this kind of a technique can detect more higher order uh hallucinations uh but again it is very restricted vicam do you want to move to the next yeah uh yeah so it's fairly restricted uh number one being that it's a blackbox technique right you're asking a magic ball if something's hallucinated uh it will fail to capture reasons behind false positives and false negatives um it can be prohibitively expensive if you use more state-of-the-art models like gp4 to give you uh give you whether uh something's hallucinated or not and more importantly it lacks the explainability which is critical for users if you want to take corrective action uh as you'll see there's many different kinds of hallucinations open domain closed domain and uh taking the right action you know it relies heavily on being able to explain what the Hallucination is about and the third and final category of existing metrics to measure hallucinations is based on the the the doing math and probability uh statistics on top of the probabilities uh you essentially look at the token level confidence of these models as it's constructing the output and the intuition here is that you know this kind of measure gives you a sense of the model's uncertainty in its own response and may suggest some semblance of hallucinations uh but from our various in-depth experiments over the last year it has shown that while uh techniques like these are great secondary signals of model confusion uh they essentially u a bit too granular viam if you want to move to the a bit too granular in its uh in in its ability to capture higher AAL signals of hallucination and uh another restriction here is that a lot of llm apis uh typically don't return you uh log probs uh and in those cases uh techniques like these kind of become a little restrictive uh so we've talked about the three broad categories of metrics which are today used for health Nations and there's many experiments which we've done internally to identify issues in these existing metrics uh and we essentially figured that there is a need for a new technique which not only takes you some of the better principles of the techniques which I spoke about but also leverages certain these distinctive powers of these new llms like their ability to respond to Chain of Thought prompting um and use that to create a new category of metrics which number one is more accurate in its results and number two it's uh applicable to real world scenarios it scales to different kinds of tasks uh and finally it is cheap to compute and it should work at low latency because we are all building practical metric systems here so with that we created a new technique to identify hallucinations uh and this technique is called chain pole and chain pole essentially um is an high efficacy method to detect hallucinations move to the next one yeah uh and here in chain pole we break down detecting hallucinations into two phases the the first phase is called chaining uh and in chaining we essentially use a very detailed Chain of Thought prompt to make um a secondary llm request and gauge if it can reasonably sort of break down its the reasoning behind the completion that it just made and then we collect the Chain of Thought reasoning um uh think of it as we we structure it down into a specific schema and then we do what we call is polling and polling is essentially this ensembling technique where we try to gather evidence of policies in the in the Chain of Thought through voting and uh voting is a common technique used in machine learning um whether it's like classic models like random Forest ensembling is a general way to get really good results and think of it as though it's um you know there's domain experts in the room and you're kind of getting structured schematic responses from each of them and then you're trying to make a claim of whether uh the model hallucinated or not so here's a high level flow of chain pole the user makes a query to the llm and the llm responds with the completion um then what we do is we check if the llm also provided a probability distribution of response tokens um this kind of goes back to the third category where you know these log probs you know act as a good secondary signal to hallucination uh so if the output does uh has no log probs we employ a special generation prompt to uh to match the original log probs in theory and then we send the the the newly generated log probs along with the original prompt uh and response to a Chain of Thought module we call it the detailed Chain of Thought module and that's basically our core ensembling algorithm and behind the scenes we are using batch inferencing to be able to um essentially uh reduce the number of model calls we make to just one and the key learning behind this has been that Chain of Thought um the depth of the Chain of Thought prompt makes a significant difference in the quality of the um you hellucination detection and the more detailed and more organized The Prompt is the smaller the model that you can use to generate accurate scores in fact you'll see in a couple of slides ahead that we used uh models like DaVinci which are at this point fairly dated to be able to achieve over 23% improvements in uh compared to the next best hallucination technique around cell checks um and then we take the output of the ensembling module and aggregate create like this aggregate score along with an explanation using the chain of thought process pass it to this cleaning and optimization module and then finally give a user a normalized score of hallucination along with a clear step by-step explanation of what may have gone wrong so just to um highlight a super high level result on all our experiments on all data sets chain pole here significantly performed better than uh the next best which is a self-check birth technique uh which was launched this year in in July um in the UK uh and the uh you can see how much better it is compared to some of the more uh uh metrics common metrics which are used today to detect modern hallucinations uh there's many advantages to chain pole number one is that it is designed to be as accurate as possible like the whole motivation of the ensembling and the chaining is to increase the accuracy uh across a breadth of possible real world tasks now the whole motivation behind this is that our metrics should apply to real world scenarios and not just stick to like academic data sets or you know text bookish uh examples uh the second is is that explainability is key you know like I mentioned like hallucinations in modern llms are very very nuanced sometimes it spews opinions uh so there's the devil is in the detail of what's right or wrong and uh these llms essentially act very similar to humans sometimes so it's very important to give you feedback which is understandable by a human being to be able to take action in next steps uh there's a need for low latency as well uh there's a significant amount of engineering our team has done to make sure that the algorithm itself Works in parallel and the different computations are asynchronous we want to minimize API calls as well and do batch inferencing to finally reduce cost because cost is a very big uh Factor here um so this is chain pole in summary uh this is the method but the outcome of the chain pole method are two metrics which we apply to the two workflows we observe in llms um the first metric is correctness which applies to sort of these open-ended open domain use cases uh where the user essentially passes a query with or without context uh and uh the correctness metric looks at the consistency in the reasoning behind the l&m's completion and how correct the output model can be uh the other is the uh the fact that the chain pole is also um correctness in particular is not is kind kind of agnostic to whether the context was passed in or not the uh the second metric is context adherence which essentially measures how much uh grounded the llm output was in the context which was provided uh it is very helpful in RG workflows where you essentially have context in the form of documents which are fetched from the vector store uh I'll show you a couple of simple examples of correctness and context adherence uh in the next slide here you can see this is a very simple example of correctness in action where the user essentially asked a factual question where was when and where was Abraham Lincoln born and uh the output of the llm was Abraham Lincoln was born on this date near LeRon County um Kentucky and in this case the model made a token level mistake it was very clear and the algorithm detected the correctness was low and it gave an explanation that if Abraham Lincoln was uh born in laru county and not Lon County uh the next example is a more complex example which really shines a light on the efficacy of chaining on onbling this is an RG use case where the the prompt specifies the u a topic uh or rather the prompt asks the llm to describe the topic that's in the documents which are passed as context and if you look at the output of the llm here you will see that it on the onset it seems very coherent it says the study is described as a descriptive study of you know hospitalized cases of uh in influenza in the last five seasons uh on the onset it might look correct but if you really dive deep the devil is in the detail here the algorithm found that almost nothing in the uh in the output was uh really like related to the context at all and the uh Co adherence score was low here and you can see that here it is going one level deeper where it's highlighting where the llm may have picked up its reasoning from uh and how it got it wrong and you know point to specific areas of the document where uh where the mistake might have been uh so here this is uh you can see like overall the experience is kind of like asking this expert and you're able to pinpoint to like specific areas of of of the document where where the model might be hallucinating a quick shine shining a light on the results here uh the mean Roc uh scores for both open and closed domain use cases you can see that uh there's significant improvements in the chain pole methodology compared to self check and some of these other uh metrics used like gal as well as uh using entropy and GPT score um and then finally uh one extra set of results before we dive into the demo this is showing you the most four most challenging data sets that we use in our experiments across four different tasks and you can see that we got um just to shine a light on the experimental process we got experts from domain experts from the world over to annotate thousands and thousands of data points as hallucinated or not and in the results here you can see um the 85% Au score on trivia QA essentially shows that chain pole came closest to matching a human expert to determining hallucinations um yeah with that we'll dive into a demo thanks aan all right uh it's demo time hopefully that was educational that was interesting um please keep the questions flowing in more than happy to take them at the end we'll try to leave around uh at least 5 to 10 minutes in the end for question and answers so the demo is going to be twofold the first one is going to be about just prompt engineering with the vector store so the uh the classic RG use case and how you can um be better about experimentation from the for while for with the inputs using some of these metrics that I've been talked about and the second one is going to be about fine-tuning with your own data um the use case for the first part which is prompt engineering with RG is going to be um one where I'll walk you through how we built out a simple app for kids um so you know a child can come in and ask a question and the llm gives a response the ingredients were a vector store um with uh with thousands of different Wikipedia articles for context um a data set of prompts and a set of llms that we were using to experiment with we used some uh of the close source llms as well as some open source llms just to see you know what what uh what combination of prompts and llms Etc would work the best so for this first demo um you know typically as a data scientist we like to work in notebooks in Python notebooks and so that's typically where you know we've been doing our prompt engineering uh you know you could also do this in an IDE for instance but in this demo what I'll walk through is in a notebook as you're doing the prompt engineering how you could use Galileo in this case to be able to experiment faster but you know the takeaways from here I feel like go far beyond Galileo it's basically we're trying to talk through U how you can think about experimentation and how you can think about applying the different metrics at different pieces of the of the application um so to start out with to build out this Q&A app for kids I'm going to go into my notebook um I uh I'm going to pip install uh this is this is my notebook so I'm going to pip install prompt quality which is the which is Galileo's python client for prompt engineering um once I do that I just quickly load my prompts like I usually do in this case what I did was I we just used the Stanford Q&A data set which is a fairly popular data set it's called Squad um so literally just use that that data set has a bunch of context that provides and a bunch of questions which are the prompts right um once I'm done with this I get the relevant passages from Pine Cone the way I usually do with these uh Vector stores I throw in thousands of different Wikipedia articles in here um once I'm done with that I start creating a prompt run with Galileo so this is typically where you know you're starting to think about what should the um prompt template be that I that I leverage so in this case we started with something simple we just said you know you are a helpful assistant given the following context answer the question provide an accurate and factual response um I get the context from my Vector store and I get the question which is the the different prompts that I added above in the data set from Squad now once I'm done with this I um log in and once I'm done with that I um add the metrics that I want to see right now remember the guardal metrics we talked about and that atin mentioned as well so the question becomes for your use case for your product what are the different metrics that you want to see um it's important to start considering this Avenue before you even start the prompt Engineering Process like what are the important metrics which are top of mind so for instance for us here um we really because we use using we using pine we care a lot about the adherence of the output to the context of a provided in the vector store so context adherance the metric that atin talked about is one that we want to see what that looks like ideally this should be a one which means it's perfectly within context every single output um context relevance which is the relevance of the input to the context um correctness which is the uh this is the level of um of uh kind of detects the level of confusion that the model was having as it was coming up with the response which aan mentioned based on our chain pole technique um also we curious about latency and also sexism so we want to avoid any such thing in the outputs um the other piece here is you know because it's a child kids app we want to create some custom metrics if there are certain words in the models output I I want to be I want that to be flagged so that I can see why those are coming out in the output right and now in this case we just added a few words here we called it the bag of words metric but because it's pythonic you can make this as complex as you want to so for instance some of our other apps we've introduced complex um models around language detection and gender bias detection Etc and just registered that with Galileo so in this case I just say if any of these words are present just answer return of one otherwise return a zero as simple as that and register that with our Galileo python quality prompt quality python client um once that's done I give this a project name and uh choose the model I want to use in this case I'm going to go with chat GPT there's 16k token llm and that's it once I'm done with that um you get a link to the to the gallileo console so no more eyeballing inside of your notebook or exporting to an Excel sheet click on this link and we'll take you to the run now um when I do this I essentially get to this uh UI where uh what I would urge everyone to think about before you start actually going into the the details of their responses is you should take a step back and look at the metrics right what's the health of your prompt run so higher level think of it in three different categories the first one being the output quality right uh the that includes the correctness and lower level metrics like blue and Rouge um the RG metrics uh like how good was the context that you provided it and is the output actually adherent to the context or not as well as you know there are often times when you need humans in the loop in in this entire process and that's it's a good thing to combine um automated um um the the whole automated metrics based experimentation with humans uh at this stage so you know human ratings is super important at a per uh prompt response level and then our custom metrics you know the bag of words that I just added in here so um to start out with you know I can see that the average corus is it's not great it's good it's 08 out of one but not great it's 08 um imagine if your F1 score for an NLP model was0 eight and you can't ship that it's probably going to be wrong 10 to 20% of the time that's not great um so I want to look into this more so why is that happening um turns out that the context of herance is not great either it's 789 so that's it should be ideally a one that means every single out piece of output is adherent within the within the within the um within the context um so uh so this is typically how this uh this uh the this is how on at a high level I can quickly get a sense for how maybe there's something wrong with the groundedness or the the the context that I provided the model with so in order to do that I can start to look at the responses here and I can see amongst all of these different metrics that I have at my disposal um there are certain things which I might want to look into a little bit more so for instance um I can see that it says here that the for this particular uh response you can see that um if you look into the context that was provided um it talks about the weather in a certain part of Australia where the average temperature exceeded 32 Dees cius in the summer and exceeded 15\u00b0 celius in the winter and then it talks more about the different parts of the of the climate in that region now the question here was about the med what is the median temperature in the winter um and the model's response was median temperature in the winter is about 15 degrees CI now when you look here that's that's exactly what it says here it's 15 degrees Celsius in the winter so now if I'm as a human evaluator if I'm rapidly trying to move through this process I'm just going to mark this as a thumbs up and move on with my life I'm going to go to the next one after this and the next one after that and start seeing other prompt responses but because I see that the overall llm correctness score is zero and the context ofh score is zero I'm curious about why that's happening so uh remember that atin mentioned that the technique for for coming with context adherence was using um was using a process where we also get a response from a model which gives us its reasoning which helps a lot with debugging so instead of trying to wonder why it's a zero I can just hover on this to get a rationale from the model it's an expert telling me why uh it's the the the output is not adherent in the context so if you look at the last few sentences here let me try to zoom in here if that maybe makes it easier if you look at the last two sentences here it says um you know the to confirm whether the median temperature so it does the the context does not pro provide the uh mentioned the median temperature to confirm whether the median is also 15 um we would need additional data and so therefore the claim made in the response cannot be fully supported by the documents so it's clear that it's almost like an expert who's read through all the documents that were provided to Pine con and says that all right it looks like what's being uh what's been given to me from Bine con is basically the uh the average temperatures here but what's being asked from me as to give an answer is the median temperature and I can't do that because without additional information so that's a good takeaway for me to have as a developer I might need to add more context around the median temperatures in certain regions maybe I'm building out a weather app for kids in this case and so I could now based on that information mark this as a thumbs down and move on to the next um the next prompt so that's a quick way in which you can at at a per prompt level at a per response level try to debug much faster using these metrics that I mentioned before now all of this is for one prompt run now remember like when you're working with these um LMS remember how many different kinds of L different kinds of inputs you have to you have to you have to work with you have uh you can have 10 different LMS you can have multiple different kinds of llm parameters different kinds of prom template versions you can't like how should you be doing this one by one in this manner um we urge you to think differently from that so one of the techniques that you could use with the prompt quality python client with Galileo is called prompt sweep where think of it as throw the kitchen sink into the into the python client right so in this case I as a developer I I thought to myself wait I tried the gpd1 16k token model but let me try text in3 as well let me also try gp4 and see if that one works better or worse than than the others instead of one template I actually have many different ideas for templates so I'm going to add four different templates all of them are just comma separated from each other with slight nuances and differences in them um I also add uh give it again the same project name and uh add a bunch of different temperatures in this case like 1.5.1 so varying degrees of freedom for the model to come up with its responses and execute that and at the end I get I get a quick estimate based on these API calls that it's making and the and the vector store that we are using that the total uh cost is going to be over $36 which is not cheap but I'm going to still go for for it um and at the end again I get a link which takes me to um instead of one run it's going to take me to many different runs so all of my runs for this project can be seen over here in one go now this can be a lot you can see all of the different prompt versions that were used U Galileo automatically version controls them for you so you don't have to worry about that it gets stored in what we call your overall prompt store where it's automatically version controlled for you you can see the different models that we use here as well as all the the different metrics that that came up um for that entire run now how do you figure out if which is the best one again if you have a metric based approach it becomes super fast to be able to figure out which of these runs for is is working out the best for the metrics that you care the most about and you know uh even from here it's it's always good as a next step to be able to quickly do an ABN comparison between um different kinds of models for instance so for instance here I'm going to keep the version the exactly the same as a v0 and uh I'm going to try out maybe different kinds of models so that's text in 003 there is the GPD 48k token there's a chat GPD 16k and now it's going to be interesting like you know everything else kept the same how did these three different models uh behave against each other and so you can see over here that if I start looking at the different responses that are coming out um I can I can inspect the the output uh just eyeball it but I can also use the uh the llm uncertainty uh metric that galileia provides which is uses the log probabilities that that's coming from the model essentially um to start to see some really interesting Trends where even with everything else kept the same right you can start to see how the models responses are dramatically different from each other um which which can be a very interesting insight for me as I start thinking about um which which model to go ahead with based on its latency based on its cost based on its output and H potential hallucinations here so um the the takeaway from all of this is you should be able to do multiple prompt runs in one go use a metric first approach to be able to figure out which combination works the best and AB testing is super important uh but there's a need for not doing this in notebooks or Excel sheets there should be a collaborative environment where you can you can leverage and Galileo is one of them um eventually if you do all of this then you can get a much faster uh way of figuring out are you moving in the right direction or not which model is working the best and often times you know building these llm applications is a team sport so you need to have multiple creators in this case it was just Galileo itself that was creating these runs but you know you could have many different people working on these runs at the same time so all of this is on the prompt engineering side for coming up with the best uh best combination of inputs so that you can um build the best version of your application super quickly um before I go into the finetune piece right let's say do all of this you want to put this into production now now when when you put some an app into production it's a whole different ball game like we talked about um you need to have a real time understanding of where things are going wrong um which is why like one of the so I'm I'm going to walk you through how this looks in Galileo but again if you're using any kind of observability solution um you could leverage Galileo's apis or any of the metrics that we talked about before to be able to power that solution right so it becomes really important for you to figure out not just the high level cost and latency and API failure metric super important but also start to look at U whether there was uncertainty whether there was the llm correctness was in a good spot or not um you might have multiple models in your chain so you should have all of those models uh being being surveyed at the same time um and you can start to also can start to like map that back to business level metrics like user session length for instance uh so as an example I can see over here that as soon as the factuality or the correctness started to reduce uh the user session lens starts to reduce as well which is an interesting insight for me where I might want to start diving deeper into that and check for what exactly is going on in that particular instance in near real time right and look at the specific data that was causing that problem and if I when when you start looking at the data and start looking again at the factuality that is coming out of this and the explanation from the model for this you can quickly get a sense for where you need to tweak your prompts so at that point you can you know instead of looking for the rumaging through for the notebook where you did that because Galileo does prompt has a prompt store behind the scenes you can just right click and load this in your prompt playground and see how this particular input from the from the users is doing with your uh with all of the uh with the prompt template that you had and where you might want to tweak it and we also strongly urge you to tweak your prompts constantly and then um eventually maybe even try to AB and experiment with different prompt versions like you know 10% with prompt version one 20% prompt version 2 that's how software development works as well and we did that behind the scenes for this application that we were using that we built out um that's being used by analysts at Banks and um what we noticed is some really interesting trends when you do that you know we tweaked multiple prompts and you can start to uh compare all of them if with live traffic to see which one is actually working out the best or not so that's you know rapid experimentation with with powerful metrics allows you to be able to figure out which is the best L&M which is the best prompt version um in using your users as um a a constant feedback channel so um all of this was to prove that the um that it's super important to be able to experiment with the inputs real time evaluate your outputs and have feedback loops constantly so that you are uh as a team constantly moving the right direction and and improving the llm application it's a never-ending um process of improvement as the world changes now Switching gears towards the second part of of the demo um where I mentioned that I would talk about fine tuning as well it's super important to think about fine tuning as a pretty cost-effective way of creating llms that can be fairly accurate now um in order to do this the big bottleneck does tend to be whether you have the data or not and if you're using synthetic data they tend to be low quality so then again the problem of data quality becomes really really important on top of mind um so in this case for fine tune what I'm going to do is um the the use case is going to be generating headlines for news articles and the ingredients U will include a corpus of news articles uh to fine-tune the llm with right fairly simple use case now I fine tuned this model in my notebook again again added one line of Galo code um in this case the python client is called Data quality um and similar similar user experience in your notebook you got a link to uh visualize your data on the other side um so if I go go to what that looks like essentially you can you in in the gall UI you get to see all of your data in one place as well as a bunch of alerts around what was super hard for the model so this is where uh there's a metric that um our researchers came up with called the data error potential score which we're happy to share after this after this uh Workshop so you can look into the math and you can also look into how to apply this but it's been super powerful as a quantifiable way to figure out what data points the model was having a hard time on let's dive into that for a second right this is about the ground truth so first I'm going to talk about how you can inspect a ground truth with the data error potential to find incorrect ground truth and the second part becomes hallucinations in the output so for the ground truth let's say I go to the pre Chained embeddings and this is for my test set by the way there's also a training set but I care a lot about my test set being perfect so um here I can see my embeddings colored by that D score or the data error potential score so immediately I can get a sense for uh you know where things might be going right or wrong let me remove the low error potential regions here um and I can start um clusters forming here so for instance there's one over here on the side which if I uh make a quick selection for of that one I can see that it's a it's it's the average DP is um quite High which is high error potential so if I look at the table view I can see see here that the input um that was provided which is the news article was about how there was Hefty falls in the stock market which uh led to um a big Pro a big uh uh where 600,000 Vehicles was sold in Japan which led to overall shares for the electronics giant Sony Falling by about 1.7% and this had a big impact on the Nik index which eventually had a big impact on the S&P index as well now the headline for this that the human labeler the target output the human labeler came up with was shares in Japanese automaker Mitsubishi Motors plunged 13.5% now you can use the token Level D score to see exactly which words the model was having the hard time with so you can see here when I turn this on that specifically the model was having a hard time on um you know words like automaker or Mitsubishi and the number 13.5% and then what also happens is the model tells you about the other tokens it was considering so when you hover on Mitsubishi you can see that it's telling you that it probably should have been son instead so it's almost again like an expert telling you um how to fix things um and so I can quickly go and edit this Target in Galileo and uh change this to say you know Sony and you know I could do this my subject matter expert could do this and it gets added to what we call an edit scard where you're basically um improving the uh the uh the input data set and you're you so that you can export and work with a better quality data set for the next run um the last thing I'm going to walk through is as promised I talked about how all of this was for the ground truth how do you figure out if the ground truth is good or bad whether it's using um the automated clustering technology from Galileo where you can start to see quick clusters whether depth score is really really high or just using the um just eyeballing and figuring out which clusters you want to inspect in Spec specifically right um the second thing I want to talk about was the output how do you know if the model's output is hallucinating or not and that's where amongst other metrics the uncertainty score becomes really interesting so if I look at the llm uncertainty score here there's a this is a typical distribution where you know there's a long tail of really um highly uncertain uh data points so when I uh when I select this I can see certain um certain options showing up over here where you can quickly get a sense for specifically when you click on the links um where the model was having a hard time in its generated out and you can start to see that it's basically on this first name where it's of it's it's literally struggling with coming up with the with the first name here when it comes a numbers it's struggling as well if I look at the other one other um other uh outputs as well again the word the first name Steve is where it's struggling um uh and again the number is where it's struggling the name is where it's struggling so quickly I can I can figure out that first names and names and ages are numbers is where it's struggling a lot so that's important for me to know so that I can fix the input that I'm working with next um and I can make sure that the model doesn't hallucinate on those so it has more context from the data that is provided to it so um at a very higher level this is typically how you would want to go through an experimentation process and make sure that when you're doing this you you can keep track of all of this in in one place over time and then it's very important to compare all of these runs in um across across each other but in order to do all of these things again just to go back to the whole point behind this um Workshop super important to start thinking about uh the metrics that you're working with it's very important to define those in the first place before you embark on the prompt engineering with RG process or fine-tuning or if you're doing both so I'll stop there I know we have about few minutes remaining would love to get into any um questions that people have absolutely and thank you so much V tendria this has been great I know there's a lot lot of questions in the chat we only have time for a few but I think I've summarized I think a couple of the important ones I think the first one is can we use adherence to check for text a large document if we summarized multiple call transcripts where can we point out uh from where the llm made the context maybe I can take a quick stab at that um so the short answer is yes uh so irrespective of the size of the document um we we basically detect um adherence issues if any uh but then uh it can depend on how you're chunking the data and uh essentially we run adherence at a request level so whether you're if you're chunking the data in a certain format but are able to cohesively get together the full context in a single request then most certainly yes um and as you saw in the slides we're able to pinpoint even if it's a large document specific area in the document where the model used that area or that chunk as a basis for reasoning and whether or not it was logically correct or not absolutely um all right let's take the next question um how do you mitigate hallucinations and situations where you're streaming live output to the user and therefore don't have the full text to analyze beforehand H you want to take the step vicam yeah I was just thinking about this so it sounds like it's a situation where you have um the it's it's a production application where you have um the output from the model directly going to the user um this is where you know there are two parts to this in my opinion one is there's before you even deploy the model right it's almost like with software engineering when you're qcing and you have to make sure that you're thinking about every single possible outcome before you even launch the model that's where uh experimentation on the on the prompt engineering side if you will becomes really important to have all those use cases and workflows mentioned even if you do that as we know with applications similar with similarly with models as well there things will go wrong with your when you're streaming live um and that's where again like having on Real Time analytics and measurements with these with these metrics like our uh correctness score and adherance score becomes super important uh so you you will be slightly reactive in those cases but you know at least it helps in mitigating those kinds of situations um from escalating uh escalating further um and having effective alerts in those cases also helps quite a bit yeah just to add to that I think in theory of course you can essentially take you know your partial generation and you know measure hallucinations measure correctness uh so in theory it works uh but if you think of like a practical system you essentially want the entire completion to finish before you make a judgment call on whether something's hallucinated or not for many reasons including the fact that the answer might be in the last bit of the completion or um so yeah essentially that so that can lead to a lot of like false negatives or false positives so uh if you are to design an eager system which is trying to measure these kind of um metrics in real time uh a better metric to measure in real time would be the UNC certainty because that gives you sort of a soft signal of how the models uh SP you know reacting or or how the model is getting confused as it's spitting tokens out but if you want to measure hallucination it is better practice to wait for the completion to finish and then make a call which doesn't take too long definitely okay I think this is our last question for today um how reliable can you reproduce results I've seen llm as non-deterministic even with temperature equals zero how do you make sure metrics first is reliable if you can't always reproduce yeah I mean to be honest there is um like any other metric there is a certain dependence on the reliability of the the um the models themselves uh which is the reason why we've built the chain poing system in a way which is very modular and it's very plug in place if you remember the the Box and arrows diagram there's a couple of internal llms in the default cases they can be the source llm themselves but you can plug and play them out to use thirdparty models which might be more powerful might be less powerful um and also there's the the internal the The Chain of Thought prompts as well as the generation prompts the uh the the complexity of which can determine the efficacy of of the output so there's all these knobs that you might have to tune uh and you're right I mean these are all non-deterministic systems so there's no reliable way to to produce results but that's that our experiments were done at scale and done across thousands and thousands of hallucination utterances across various tasks and on aggregate the results you saw was at 85% accuracy so um that's our job to at least amortized we can say that the metric sort of works yeah and then last thing that is like it's it is that is something if you notice as well where your you know the llm output is pretty non-deterministic even when temperature is equal to zero um but that that is why you need these kinds of guard reils in place constantly because it is that is the that's the power the Boon and ban of these llms but I also would urge to for people to start thinking about the whole having the um like the uh the multiple checks that we do for creating these metrics in place today where there's a majority voting that automatically happens as a part of the creation of the llm uh the creation of the metrics output happens for exactly this reason because behind the scenes you basically asking it with one the same same API call asking the same llm five times just to avoid this uh lack of lack of stability issue and the lack of reliability issue great well thank you so much vigram and ATT tendria this has been an amazing Workshop I'm sure everyone learns so much um so for anyone that's still here thank you for coming please take the survey in the chat to have any feedback of how we should run our events in the future uh what other topics you would love to hear even from Galileo as well and we hope to see you next time and keep learning take care everyone bye thank you thank you", "comments": "I came across Arhasi on LinkedIn. They seem to have done analysis on various llm hallucination approaches and deliver capability maturity model and AI strategy in your business context.\n\ud83c\udfaf Key Takeaways for quick navigation:\n\n00:13 \ud83d\udce3 The video introduces a workshop on mitigating LLM hallucinations with a metrics-first evaluation framework for large language models.\n03:08 \ud83e\udded The workshop covers the need for LLM experimentation and evaluation frameworks, different evaluation methods, emerging hallucination detection metrics, and a live demo on implementing these techniques.\n14:02 \ud83d\udcca Existing techniques to measure hallucinations include engram matching metrics like BLEU and ROUGE, asking LLMs if they think an input/output pair is hallucinated, and using token-level confidence for statistical analysis. These techniques have limitations.\n20:29 \ud83c\udf1f A new technique called ChainPol is introduced to detect hallucinations more accurately and efficiently. It involves chaining and polling, using a detailed Chain of Thought prompt, and ensembling to provide a normalized score and explanation for hallucinations.\n23:28 \ud83d\udcca ChainPole significantly outperforms other techniques for detecting model hallucinations, especially in real-world scenarios.\n24:22 \ud83e\udde0 Explainability is crucial for addressing nuanced model hallucinations, as it helps make model outputs understandable to humans.\n25:18 \u23f1\ufe0f Low latency and cost reduction are important considerations for practical model deployment.\n25:32 \ud83d\udccf ChainPole introduces metrics like \"correctness\" and \"context adherence\" to assess the reliability and relevance of model outputs in different scenarios.\n26:38 \ud83d\udd75\ufe0f\u200d\u2640\ufe0f Example: A model's correctness score is impacted when the answer contains factual inaccuracies, and it can be identified with ChainPole's explanation.\n27:57 \ud83e\udde0 Model hallucinations can be detected by assessing context adherence, ensuring that the model's responses are grounded in the provided context.\n28:37 \ud83d\udcc8 ChainPole methodology shows significant improvements over other metrics like GAL, entropy, and GPT score in various tasks.\n29:19 \ud83d\udcca ChainPole's performance comes closest to human experts in determining hallucinations.\n31:10 \ud83e\uddea Prompt engineering involves creating templates for interacting with language models and evaluating responses using metrics like context adherence, correctness, and latency.\n35:06 \ud83d\udcdd Prompt sweep allows you to experiment with different prompts, models, and parameters to quickly identify the most effective combinations.\n42:08 \ud83d\udd04 AB testing helps compare different models and templates to determine which ones are more suitable for your application.\n44:00 \ud83d\udee0 Putting an application into production requires real-time monitoring of metrics like cost, latency, correctness, and user session length to ensure performance and reliability.\n45:25 \ud83e\uddd0 Experimenting with LLMs is crucial for improving their performance. Real-time evaluation and continuous feedback loops are essential to refine LLM applications.\n47:26 \ud83d\udd04 Fine-tuning LLMs is cost-effective, but data quality is critical. Synthetic data tends to be low quality, so ensuring high-quality training data is essential for accurate results.\n49:20 \ud83d\udd75\ufe0f\u200d\u2640\ufe0f Ground truth inspection can help identify incorrect information in LLM outputs. You can use data error potential scores to pinpoint areas that need correction.\n52:02 \ud83e\udd14 Uncertainty scores can help identify where LLMs struggle, helping you improve the input data to preventhallucinations.\n53:40 \ud83d\udd0d Defining and measuring metrics is essential for monitoring and improving LLMs. It's crucial to have these metrics in place before embarking on prompt engineering and fine-tuning.\n55:12 \ud83d\udd2e Measuring LLM hallucinations in real-time, especially in live streaming situations, can be challenging. It's important to have robust metrics and alerts in place to handle unforeseen issues.\n58:52 \ud83d\udcca The reliability of reproducing LLM results depends on various factors, including model variability. Modular systems and multiple checks can help mitigate non-deterministic behavior.\n\nMade with HARPA AI\nI'm not a technical person, but I was interested in ChatGPT so I took the course. It was a very meaningful lecture. I was often troubled by LLM hallucinations, so when I learned that research was being conducted to improve it, my expectations for AI increased even more.\nThank you for the presentation and demo!\n\u2764\nBlessed love...givethanks...Cape Town\nThis work shows that the open source solution SELFCHECKGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models do works! :)\nThe real contribution seems to be the prompt they used to generate the CoT and the metric value... Could you share the code  used for the metric and the prompt for ChatPGT?\nOpen source without OpenAI key:\r\n\u2764\nArxiv\r\n2303.08896.pdf\ngithub + \r\nselfcheckgpt/tree/main\r\n/potsawee/selfcheckgpt/blob/main/demo/SelfCheck_demo1.ipynb\nNice talk! Could you please share the notebook?\nThe paper and the Slides are both in the description, guys. :) read.\nCould someone share the link to the paper that was mentioned here \"ChainPoll\" , I believe.\nDo you think human intervention in the evaluation process is going to last? It seems its a process that LLMs could achieve by themselves in the near future.\n\u042f \u0438\u0437 \u0420\u043e\u0441\u0441\u0438\u0438. \u0421\u043f\u0430\u0441\u0438\u0431\u043e \u0437\u0430 \u0432\u0435\u0431\u0438\u043d\u0430\u0440.\nGuys would u be able to drop the notebook please?\nI don't know how bt I searched the n word and it came up"}, {"publishedAt": "2023-05-08T20:44:01Z", "channelId": "UC7L1tZw52rtgmIB4fr_f40w", "title": "Reducing Hallucinations in LLMs | Retrieval QA w/ LangChain + Ray + Weights &amp; Biases", "description": "Discover how to construct an LLM-based question and answering (QA) service that combats hallucinations using Retrieval QA ...", "thumbnails": {"default": {"url": "https://i.ytimg.com/vi/Sy-Xp-sdlh0/default.jpg", "width": 120, "height": 90}, "medium": {"url": "https://i.ytimg.com/vi/Sy-Xp-sdlh0/mqdefault.jpg", "width": 320, "height": 180}, "high": {"url": "https://i.ytimg.com/vi/Sy-Xp-sdlh0/hqdefault.jpg", "width": 480, "height": 360}}, "channelTitle": "Anyscale", "liveBroadcastContent": "none", "publishTime": "2023-05-08T20:44:01Z", "video_id": "Sy-Xp-sdlh0", "transcript": "hi my name is Ali kadus I'm the head of engineering here at any scale and today I'm really excited to share with you how we can build a retrieval-based question answering system using a combination of Lang chain Rey and weights and biases so previously this is actually part two part one we showed how you could use embeddings to create a search engine effectively in just a few lines of Lang chain and rate code and we brought it up as a service that we could then query so now in part two we're going to build on part one to create a retrieval-based question answering system so that basically is going to be that we take the search results that we had last time we use that to generate a prompt and then using that prompt we ask an llm to extract an answer from it now you might say like why do I want to do this well llms have two key problems sometimes they just don't have an answer and they can tell you that they don't have the answer but unfortunately a lot of the time they do what we call hallucination which is they make up an answer that they are very confident is correct and that's because they don't have enough data and and so uh that's often a problem we encounter by using a search engine combined with an llm and the appropriate prompt we can actually overcome in many cases both the ignorance and hallucination problem so last time we talked about it this is how we built kind of the information Source the retrieval part of the retrieval engine we took some HTML files we cut them into little sentences like structures uh we call them chunks and we embedded them into a very high dimensional Vector space basically converting those sentences into sequences of numbers that we could then use as an index so that was what we did ahead of time and then to serve what we did is we would take the query we would then do the same kind of embedding of the query and then we'd find things that were similar to that search result and then return the form the the the most relevant results and then that is the response that we've shown to the user much like a normal search engine bus now we're going to expand it so imagine that we take the system that we just built and what we what we're going to do is we're going to use that information and combine a prompt template to generate a prompt that we're now going to pass to our llm to generate a useful answer that addresses the concern so let's have a demo uh we're going to start our service this is very similar to the service that we set up last time and let's go through the source code so in many ways it's very similar to the source code from last time but there's a few key differences the first is at the top here we Define The Prompt that we're going to pass to the llm search engine today we'll be using stable LM which is a really small llm it's only about 7 billion parameters so they don't always have to be fancy you don't need the full power of gpt4 to take answers from a search engine and synthesize them into a single response stable LM you first Define the the system which is kind of like the personality that you're trying to create within the agent and then make sure that you then Define The Prompt that's specific to the question that you have so you'll notice here that we're defining we're telling it here's your context and here's what we want you to do we want you to be conservative in describing what it is that you know now once we have that prompt we're now going to fill in the data from the search engine and this is really where the power of Lang chain comes in there's one other modification that we're going to make which is that we're going to add support for uh word um weights and biases so weights and biases is an evaluation tool that is freely available um and then what we can do is we can inspect intermediate values and see how long things are taking so the first part of the code we'll be looking at is exactly the same we create the embeddings but this is where we start to do a few different things first is that we create a pipeline we have some minor modifications here to how Lang Chang does things just to ensure stability but you can see we're specifying the 7 billion parameter model and we're also specifying that it should use 16 point floats now that's the model then we use Lang chains capability and pass it the llm we created here which is a stable LM pipeline there's particular different ways of mixing the data one is stuff which is the simplest one and we're going to take the prompt that we gave above that's this long sentence okay now that we have the prompt we're going to connect it up and so what we're going to do is first we're going to get the results the results from our previous system we're then going to um use the chain that we created earlier and pass in the documents that we found in other words the chunks and the question that we have and that's where um Lang chain kicks in takes the results computes them and then finally passes it to the llm to summarize so now let's try to to hit it so we have this query script that we've used before let's check that the service is up and running okay it looks healthy and we're going to just send a query you know what is the difference between pack and spirit in right and what that's doing is it's now sending that query first it's going to get the search results then it's going to pass it to the llm to summarize and you can see here it's not perfect because it talks about an image which is what was in the search results but you can see it actually gets the definition correct so in Ray when you want to pack a lot of actors together on a single machine you say I want to fit as many of these as I can on a single machine but other types of performance readers if you have like 20 machines and you have 20 actors you want one actor per machine so as we can see it's generated the correct answer in this particular case what we want to do though is kind of look at the observability of this and this is where weights and biases comes in now this is a very recent integration like literally in the last week or so it's not absolutely required here we can do other things like have the array serve instance kind of dump output but it's a really convenient way to look at the experiences you go in and so what we're doing here is we go to the waiting biases webpage and every time someone puts in an image uh sorry um a query it actually captures the query at all the different levels so as you can see down here it has the stuff's documents chain and the part that we're most interested in is the stable LM pipeline part and what we can do now is if we go here and click to the prompt you can see that in addition to the template that we had it is actually pulling the information from the search engine and it's embedding it here and then finally after all of this we can now ask it the question between what's the difference between pack and spreading rate and now we're telling you it's time for the assistant to hand over and sure enough that's what allows it to generate the correct answer that we're looking for um here in terms of summarizing the contents so let's summarize where we are what we've done here is we've built a system that allow we built a question answering system that isn't just the llm by itself it's retrieving information from a search engine that we created earlier to generate results that are super accurate even though the model and the llm itself is relatively small at seven billion parameters it's almost as small as they get and that can easily fit on a 16 gigabyte GPU um and using that information and through clever prompt engineering we've created a question answering system that overcomes some of the limitations of llms as far as not knowing the answer and also not hallucinating and making up an answer by constraining it to the facts that we're in the prompt that we passed in so what's next you can of course find this code yourself and download it we now have a repo with all of these examples in it and if you'd like to build your own systems like this check out the docs at docs.ray.io Ray also has a lot of discussion forums and slack that you can participate in and of course if you're interested in a commercial version of this with production level reliability just reach out to us and we'll be sure to follow up", "comments": "Do LLM still hallucinate even if you mention a fact multiple times in knowledgebase?\nThanks for the educational content! I have a question: how to choose between LangChain and LlamaIndex for my use case? I find that you have upload videos for both. LlamaIndex is based on the LangChain but I am confused on how to choose between them...\nVery insightful. Thanks for the video.\nThis is great! Really helps out with the thought process\nReally help. I'm trying to run your demo and receive this error when serving.. \n\nThe Weights & Biases Langchain integration does not support versions 0.0.169 and lower. To ensure proper functionality, please use version 0.0.170 or higher.\n\nI'm running on windows with anaconda, and installed wandb:0.15.3 - any ideas?\nThis is awesome! Thanks for posting this!"}, {"publishedAt": "2023-07-10T08:22:19Z", "channelId": "UCG6qpjVnBTTT8wLGBygANOQ", "title": "Stopping Hallucinations From Hurting Your LLMs // Atindriyo Sanyal // LLMs in Prod Conference Part 2", "description": "This portion is sponsored by Galileo. Website: https://www.rungalileo.io Galileo is an ML Data Intelligence company, that helps ...", "thumbnails": {"default": {"url": "https://i.ytimg.com/vi/LCeEkEK6JEs/default.jpg", "width": 120, "height": 90}, "medium": {"url": "https://i.ytimg.com/vi/LCeEkEK6JEs/mqdefault.jpg", "width": 320, "height": 180}, "high": {"url": "https://i.ytimg.com/vi/LCeEkEK6JEs/hqdefault.jpg", "width": 480, "height": 360}}, "channelTitle": "MLOps.community", "liveBroadcastContent": "none", "publishTime": "2023-07-10T08:22:19Z", "video_id": "LCeEkEK6JEs", "transcript": "our next guest I'm so pumped about um this is the CTO of Galileo he's had he has over 10 years of experience at some massive companies like uber and apple he was on the Michelangelo team at Uber and I think he was part of the team that created maybe the first Feature Feature store ever um we'll have to bring him on here to uh verify that so without further Ado let's hear from attendrio hello how's it going holy and nice to be here thank you for having me I'm doing great how are you it's nice to see you're looking a little blurry maybe we'll give you a moment okay I think it's looking a little bit better uh yeah how you doing thank you so much for joining us no likewise thank you for having me this is very exciting and very excited to share some new work on hallucinations that we've done over at Galileo yeah I've been really looking forward to this talk so let's let you get started and here are your slides super awesome thank you so much uh so yeah first of all uh thank you for having me here and thank you for joining this lightning talk uh there's a lot to talk about we only have 10 minutes so I'll just get right to it uh just a bit about me I think Lily already kind of mentioned uh I'm one of the founders of Galileo which is an ml data intelligence company and prior to Gallery I've spent most of my career building machine learning platforms and ml systems spanning Post 10 years uh worked on Siri at Apple for many years and as a staff engineer and leading a lot of the core components of the Michelangelo platform at Uber and where I primarily focused on data quality and data diagnostics for machine learning and we also created the first feature store a few years ago as as Lily had mentioned uh so I'll just get right to uh what we're gonna talk about today uh it's all about llms and in particular one of the key data quality issues that plagues a lot of these LMS uh and can potentially be a very big deterrent to you know productionizing practical LM systems in the coming months and years and that's today it's termed as hallucinations uh but want to dive a little bit deeper into what they are what they mean and how you can sort of detect them and avoid them in your systems so there's no question that today's llm especially young gbd3 and above they are extremely impressive in their responses but if you really squint uh they're wrong much more often than you'd think and uh these errors kind of include you know factual mistakes that they make uh there's misleading texts that they spit out there that may look linguistically correct but they're more often wrong uh and this is what typically sort of refers to hallucinations and uh what leads to these hallucinations uh is typically more of a data problem as you would see you know there's when you train these foundational models um there's a lot of issues around overfitting the data to the model there's a hidden class imbalances and insufficient data of a certain type that's missing in the in the training and eval data sets and often validation data sets have less coverage than their training baselines and encoding mistakes as well as poor quality prompting all all this sort of is a drop in the ocean of the reasons why uh an llm could hallucinate and the outcomes of hallucinations is basically what you see on the right I don't think that needs uh much of an introduction so now how do you tell whether a model is hallucinating or not or rather can the LM themselves tell us whether they're hallucinating now to answer this question we need to look a little bit deeper into the model itself now these LMS are essentially next token prediction machines uh where you know at each moment they're essentially choosing you know the next best token or the next best word to spit out from a collection of tokens where there's a probability distribution assigned to each of them now this is a very very like 20 000 foot view of what a sequential model typically looks like uh you can double click into the Transformer box and you know there's many variations of of it but for the purposes of this talk I think we can do with this simple view but the key thing to remember here is that there's token level probabilities and their distributions that tell us a lot about what the you know models outcome or what the llm thinks about its outcome and is one of the key indicators of Health nation and we'll get into that a little bit in the coming slides uh but first I want to base this presentation on a lot of these llm experiments that we've done on hallucinations and some very promising results that we've seen and how we are baking it into the Galileo product I touch upon a bit towards the end but first we wanted to define a problem statement for our experiments and and that essentially kind of came down to quantifying hallucinations through a metric which can automatically be for all llm or all kinds of llm responses uh and beyond that it's important to point out is subtext level hallucination which means that often these these models would spit out Blobs of you know text and often there's subtext within within them maybe a sentence or two which are hallucinated so this metric needs to be very granular uh and the method that we sort of we went over many methods internally but the the the summary of the method that kind of came down to was the fact that we wanted to use an open-ended text generation where we would curate a set of inputs and llm outputs as our data set and we did a lot of exploration on standardized data sets being used uh but then we would examine the token level probabilities for each of these completions from these models Channel them to a third-party neutral models to get some extra signal and then eventually determine if the base completion of the the the output of the base model was hallucinating or not so that's the the summary of the methodology uh there's some assumptions here which we've made and one of the key assumptions is that all these models are state of the art and by state of the art I mean gbd 3.5 and above uh and this is a very important assumption because uh a lot of these models uh especially gbt 3 and above they're trained on a wide range of knowledge and they're capable of answering you know many kinds of questions which would otherwise stump the older models which are below three GB3 uh so this new definition of hallucination that we're creating they cannot be based on some of these you know older research that was done two years ago which involved you know analyzing mistakes which those models made because those are essentially and the newer models are pretty much immune to it uh and the goal for the metric is to you know focus on accuracy it needs to be as accurate as possible in its hallucinations as well as diversity because open-ended like text generative models there they can be used for a wide variety of tasks as opposed to some of the more limiting model architectures so the hallucination metric has to sort of diversify across different kinds of tasks now when we start the experiments we essentially employed a two model Paradigm there's a completion model which is the output of Interest like the the actual model which spits out the output and then there's the probability model which spits out the probability tokens now in an Ideal World the the they would be the same model but more often than not especially with openai models and uh some of these these other proprietary models there's often situations where you get completions but you don't get probabilities so uh and not all models basically give you enough information especially when you consume them through apis so we had to do a bunch of experiments on figuring out which combinations of probability and completion models gave us the highest bank for the buck in terms of hallucinations talking a little bit about the data sets that we experimented on there were a whole host of uh data sets that we kind of explored in general this area is very new so we had to kind of create some data sets on our own uh but amongst the existing ones some of the the top candidates for us which gave us you know decent bank for the buck were you know there's a self-check GPT Wiki bio data set which is essentially a data set of Wikipedia biographies there's a self-instruct human evaluation data set which is a more like open-ended text generation data set and the one which gave us most promise and uh we the one we found most challenging for some of these model newer models uh was the open Assistant data set which is uh one for a an open source chat GPT like assistant uh so these were the data sets that we conducted a a good chunk of our experiments on um and then we kind of got into the metrics and the baselines that we want to create for these these uh these data sets uh and the Baseline we essentially used three uh key metrics uh one was log probs which is essentially the the log of the probabilities of each tokens that appears in the completion uh there's ppl5 which is a metric which was published by one of the the papers I've referenced here in this in this presentation uh and that essentially measures the entropy of the of the model's probability distributions but particularly in the top five tokens uh and then final is the pseudo entropy which is the metric that Galileo has created and it's a heuristic on top of Shannon's entropy uh but again we look at sort of the top five uh token responses uh from um from the llm output foreign and then across these metrics we evaluated average versus minimum uh because here we a lot of these these uh metrics that we get there at a token level and but the output of the LM overall is a blob of text so we've done a good number of comprehensive experiments uh around this almost think of it as a cross join of all this uh there's another set of baselines that we explored which uh sort of term as multi-model baselines and this is sort of driving the intuition that uh the third party model can give us extra information or or key signals about hallucinations because it's not biased by the its their own data uh and here we we looked at uh for example there were three API based baselines which were all using GPD 3.5 or chat GPT uh and here in the first one chat GPD QA we used the gbd 3.5 model to essentially write a question and an answer and then use the same model as a grader for for for the answer and we saw mediocre results from from this particular method chat GPT token was the second sort of Baseline we established where uh it was an improvement over just looking at log probs because often what we noticed is that you know a lot of these llm responses as the initial set of log probes are the model is highly uncertain about some of its initial tokens so that kind of drives the Min log prob to a much lower value than you know almost unnecessarily so in order to avoid those kind of you know phrase uncertainty biases we use chat GPT token and then we created this new method called chat GPD agreement or chat gbd friend which gave us really good results uh and here we basically Leverage The 3.5 model to essentially do reruns on the base model and get multiple outputs and then see if there's enough agreement between the completion and the reruns and the intuition there is again you know if if a model is hallucinating then likely for more than a single run it would give vastly different responses as opposed to something it's very sure about so we got some very interesting results using this particular method uh there's some other multimodal baselines that we've also explored which typically includes self-check mechanisms where you do reruns and then you compute the bird and the blue scores to see the distances between the the all pairs of completions and uh uh so just to highlight uh some of the experiments that we've done uh they involve you know all of these baselines now going a little bit deeper into the results I just want to quickly highlight the most promising results that we saw uh the first one was one minute so the the first one was it gave us 63 Precision uh when we used DaVinci uh as both the completion and probability and then using pseudoentropy we got a 69 accuracy in detecting hallucinations I've pasted some charts here for you to go over offline but here's what we've found these are outputs of state-of-the-art open AI models uh quotations from the Shakespeare's the temp test which were completely made up non-existent books as well as URLs that do not exist on the web and this is a drop in the ocean of kind of things that we saw so we've baked in hallucination as well as some of our other famous well-known metrics such as data error potential in our system and we have two new tools one is to help you create prompts and manage prompts it's called Uh The Prompt inspector and it allows you to select the best prompts and finally if you're fine-tuning a foundational model we we use depth as well as our new hallucination metric to really show you noise in your training data as well as hallucinations in in your evaluation as well as the models outputs so yeah just cutting to the chase these are the two products which will be launched just a quick shout out to the team which has made this possible they're incredible people and finally you know we're launching llm Studio very soon just go to rungalileo.io slash llm studio and you know just uh I can't wait for you to use the tool yeah thank you awesome thank you so much um and definitely share any links in the chat as well um to answer your questions thank kids um all of these videos all of these talks will be recorded slides we're collecting we'll make sure to get those into your hands so thank you so much this was wonderful thank you so much [Music]", "comments": ""}]