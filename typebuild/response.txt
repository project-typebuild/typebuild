[{"publishedAt": "2023-05-08T23:14:24Z", "channelId": "UCJS9pqu9BzkAMNTmzNMNhvg", "title": "Introduction to large language models", "description": "Enroll in this course on Google Cloud Skills Boost \u2192 https://goo.gle/3nXSmLs Large Language Models (LLMs) and Generative AI ...", "thumbnails": {"default": {"url": "https://i.ytimg.com/vi/zizonToFXDs/default.jpg", "width": 120, "height": 90}, "medium": {"url": "https://i.ytimg.com/vi/zizonToFXDs/mqdefault.jpg", "width": 320, "height": 180}, "high": {"url": "https://i.ytimg.com/vi/zizonToFXDs/hqdefault.jpg", "width": 480, "height": 360}}, "channelTitle": "Google Cloud Tech", "liveBroadcastContent": "none", "publishTime": "2023-05-08T23:14:24Z", "video_id": "zizonToFXDs", "transcript": "JOHN EWALD: Hello, and\nwelcome to Introduction to Large Language Models. My name is John Ewald, and\nI'm a training developer here at Google Cloud. In this course, you learn\nto define large language models, or LLMs,\ndescribe LLM use cases, explain prompt tuning, and\ndescribe Google's Gen AI development tools. Large language models, or LLMs,\nare a subset of deep learning. To find out more\nabout deep learning, see our Introduction to\nGenerative AI course video. LLMs and generative AI\nintersect and they are both a part of deep learning. Another area of AI you\nmay be hearing a lot about is generative AI. This is a type of\nartificial intelligence that can produce new content,\nincluding text, images, audio, and synthetic data. So what are large\nlanguage models? Large language models refer to\nlarge general-purpose language models that can be\npre-trained and then fine tuned for specific purposes. What do pre-trained\nand fine tuned mean? Imagine training a dog. Often, you train your\ndog basic commands such as sit, come,\ndown, and stay. These commands are normally\nsufficient for everyday life and help your dog become\na good canine citizen. However, if you need a special\nservice dog such as a police dog, a guide dog, or a hunting\ndog, you add special trainings. The similar idea applies\nto large language models. These models are trained\nfor general purposes to solve common language\nproblems such as text classification, question\nanswering, document summarization, and text\ngeneration across industries. The models can then be tailored\nto solve specific problems in different fields\nsuch as retail, finance, and entertainment using a\nrelatively small size of field data sets. Let's further break\ndown the concept into three major features\nof large language models. Large indicates two meanings. First is the enormous size\nof the training data set, sometimes at the petabyte scale. Second, it refers to\nthe parameter count. In ML, parameters are often\ncalled hyperparameters. Parameters are basically the\nmemories and the knowledge that the machine learned\nfrom the model training. Parameters define\nthe skill of a model in solving a problem\nsuch as predicting text. General purpose\nmeans that the models are sufficient to\nsolve common problems. Two reasons lead to this idea. First is the commonality of\na human language regardless of the specific tasks. And second is the\nresource restriction. Only certain organizations\nhave the capability to train such large language\nmodels with huge data sets and a tremendous\nnumber of parameters. How about letting them create\nfundamental language models for others to use? This leads to the last point,\npre-trained and fine tuned, meaning to pre-train\na large language model for a general purpose\nwith a large data set and then fine tune it for\nspecific aims with a much smaller data set. The benefits of using\nlarge language models are straightforward. First, a single model can\nbe used for different tasks. This is a dream come true. These large language\nmodels that are trained with petabytes of\ndata and generate billions of parameters are smart enough\nto solve different tasks, including language translation,\nsentence completion, text classification, question\nanswering, and more. Second, large language models\nrequire minimal field training data when you tailor them to\nsolve your specific problem. Large language models\nobtain decent performance even with little\ndomain training data. In other words, they can be\nused for few shot or even zero-shot scenarios. In machine learning,\nfew shot refers to training a model\nwith minimal data, and zero shot implies\nthat a model can recognize things that have not explicitly\nbeen taught in the training before. Third, the performance\nof large language models is continuously growing when you\nadd more data and parameters. Let's take PaLM as an example. In April 2022,\nGoogle released PaLM, short for Pathways Language\nModel, a 540 billion-parameter model that achieves a state\nof the art performance across multiple language tasks. PaLM is a dense decoder-only\ntransformer model. It has 540 billion parameters. It leverages the\nnew pathways system, which has enabled\nGoogle to efficiently train a single model across\nmultiple TPU V4 pods. Pathway is a new AI\narchitecture that will handle many tasks at\nonce, learn new tasks quickly, and reflect a better\nunderstanding of the world. The system enables PaLM\nto orchestrate distributed computation for accelerators. We previously mentioned that\nPaLM is a transformer model. A transformer model consists\nof encoder and decoder. The encoder encodes\nthe input sequence and passes it to\nthe decoder, which learns how to decode\nthe representations for a relevant task. We've come a long away from\ntraditional programming to neural networks\nto generative models. In traditional\nprogramming, we used to have to hard code the rules\nfor distinguishing a cat-- type, animal; legs, four;\nears, two; fur, yes; likes yarn, catnip. In the wave of\nneural networks, we could give the network pictures\nof cats and dogs and ask, is this a cat? And it would predict a cat. In the generative\nwave, we as users can generate our own content,\nwhether it be text, images, audio, video, or other. For example, models\nlike PaLM, or LaMDA, or Language Model for\nDialogue Applications, ingest very, very large\ndata from multiple sources across the internet and build\nfoundation language models we can use simply by\nasking a question, whether typing it into\na prompt or verbally talking into the prompt. So when you ask it\nwhat's a cat, it can give you everything it\nhas learned about a cat. Let's compare LLM development\nusing pre-trained models with traditional ML development. First, with LLM development,\nyou don't need to be an expert. You don't need\ntraining examples. And there is no need\nto train a model. All you need to do is think\nabout prompt design, which is the process of creating a\nprompt that is clear, concise, and informative. It is an important part of\nnatural language processing. In traditional\nmachine learning, you need training examples\nto train a model. You also need compute\ntime and hardware. Let's take a look at an example\nof a text generation use case. Question answering, or QA, is\na subfield of natural language processing that deals with the\ntask of automatically answering questions posed in\nnatural language. QA systems are typically\ntrained on a large amount of text and code. And they are able to answer\na wide range of questions, including factual, definitional,\nand opinion-based questions. The key here is that you\nneed domain knowledge to develop these\nquestion-answering models. For example, domain\nknowledge is required to develop a question-answering\nmodel for customer support, or health\ncare, or supply chain. Using generative QA, the\nmodel generates free text directly based on the context. There is no need for\ndomain knowledge. Let's look at three\nquestions given to Bard, a large language model chat\nbot developed by Google AI. Question one. \"This year's sales are $100,000. Expenses are $60,000. How much is net profit?\" Bard first shares how net\nprofit is calculated, then performs the calculation. Then Bard provides the\ndefinition of net profit. Here's another question. Inventory on hand\nis 6,000 units. A new order requires\n8,000 units. How many units do I need to\nfill to complete the order? Again, Bard answers the question\nby performing the calculation. And our last example,\nwe have 1,000 sensors in 10 geographic regions. How many sensors do we have\non average in each region? Bard answers the\nquestion with an example on how to solve the problem\nand some additional context. In each of our questions, a\ndesired response was obtained. This is due to prompt design. Prompt design and\nprompt engineering are two closely-related concepts\nin natural language processing. Both involve the\nprocess of creating a prompt that is clear,\nconcise, and informative. However, there are some key\ndifferences between the two. Prompt design is the process\nof creating a prompt that is tailored to the specific\ntask that this system is being asked to perform. For example, if\nthe system is being asked to translate a text\nfrom English to French, the prompt should be\nwritten in English and should specify that\nthe translation should be in French. Prompt engineering\nis the process of creating a prompt\nthat is designed to improve performance. This may involve using\ndomain-specific knowledge, providing examples of\nthe desired output, or using keywords\nthat are known to be effective for the\nspecific system. Prompt design is a\nmore general concept, while prompt engineering is\na more specialized concept. Prompt design is essential,\nwhile prompt engineering is only necessary\nfor systems that require a high degree of\naccuracy or performance. There are three kinds of\nlarge language models, generic language models,\ninstruction tuned, and dialogue tuned. Each needs prompting\nin a different way. Generic language models\npredict the next word based on the language\nin the training data. This is an example of a\ngeneric language model. The next word is a token based\non the language in the training data. In this example,\n\"the cat sat on,\" the next word should be \"the.\" And you can see that \"the\"\nis the most likely next word. Think of this type as an\nautocomplete in search. In instruction\ntuned, the model is trained to predict a\nresponse to the instructions given in the input. For example,\nsummarize a text of X, generate a poem\nin the style of X, give me a list of keywords based\non semantic similarity for X. And in this example,\nclassify the text into neutral,\nnegative, or positive. In dialogue tuned, the model\nis trained to have a dialogue by the next response. Dialogue-tuned models\nare a special case of instruction tuned where\nrequests are typically framed as questions to a chat bot. Dialogue tuning\nis expected to be in the context of a longer\nback and forth conversation, and typically works better\nwith natural question-like phrasings. Chain of thought reasoning\nis the observation that models are better at\ngetting the right answer when they first output\ntext that explains the reason for the answer. Let's look at the question. Roger has five tennis balls. He buys two more\ncans of tennis balls. Each can has three tennis balls. How many tennis balls\ndoes he have now? This question is posed\ninitially with no response. The model is less likely to get\nthe correct answer directly. However, by the time the\nsecond question is asked, the output is more likely to\nend with the correct answer. A model that can do everything\nhas practical limitations. Task-specific tuning can\nmake LLMs more reliable. Vertex AI provides\ntask-specific foundation models. Let's say you have\na use case where you need to gather\nsentiments, or how your customers are feeling\nabout your product or service. You can use the\nclassification task sentiment analysis task model. Same for vision tasks. If you need to perform\noccupancy analytics, there is a task-specific\nmodel for your use case. Tuning a model enables you to\ncustomize the model response based on examples\nof the task that you want the model to perform. It is essentially the\nprocess of adapting a model to a new domain, or set\nof custom use cases, by training the\nmodel on new data. For example, we may\ncollect training data and tune the model specifically\nfor the legal or medical domain. You can also further tune\nthe model by fine tuning where you bring\nyour own data set and retrain the model by\ntuning every weight in the LLM. This requires a big\ntraining job and hosting your own fine-tuned model. Here's an example of a medical\nfoundation model trained on health care data. The tasks include question\nanswering, image analysis, finding similar\npatients, and so forth. Fine tuning is expensive and\nnot realistic in many cases. So are there more efficient\nmethods of tuning? Yes. Parameter-efficient\ntuning methods, or PETM, are methods for tuning\na large language model on your own custom data\nwithout duplicating the model. The base model itself\nis not altered. Instead, a small\nnumber of add-on layers are tuned, which can be swapped\nin and out at inference time. Generative AI Studio lets you\nquickly explore and customize generative AI\nmodels that you can leverage in your\napplications on Google Cloud. Generative AI Studio helps\ndevelopers create and deploy generative AI\nmodels by providing a variety of tools\nand resources that make it easy to get started. For example, there's a\nlibrary of pre-trained models, a tool for fine tuning models,\na tool for deploying models to production, and a\ncommunity forum for developers to share ideas and collaborate. Generative AI App\nBuilder lets you create Gen AI apps without\nhaving to write any code. Gen AI App Builder has a\ndrag-and-drop interface that makes it easy\nto design and build apps, a visual editor that\nmakes it easy to create and edit app content, a built-in search\nengine that allows users to search for information\nwithin the app, and a conversational\nAI engine that allows users to interact with\nthe app using natural language. You can create your own chat\nbots, digital assistants, custom search engines, knowledge\nbases, training applications, and more. PaLM API lets you\ntest and experiment with Google's large language\nmodels and Gen AI tools. To make prototyping quick\nand more accessible, developers can integrate\nPaLM API with Maker Suite and use it to access the\nAPI using a graphical user interface. The suite includes a\nnumber of different tools such as a model-training\ntool, a model-deployment tool, and a model-monitoring tool. The model-training tool helps\ndevelopers train ML models on their data using\ndifferent algorithms. The model deployment tool helps\ndevelopers deploy ML models to production with a number of\ndifferent deployment options. And the model-monitoring\ntool helps developers monitor the\nperformance of their ML models in production using a\ndashboard and a number of different metrics. That's all for now. Thanks for watching this course,\nIntroduction to Large Language Models.", "comments": "Nice one!\nVery Informative - Thanks for sharing \ud83d\ude0a prompt design and prompt engineering would take make the conversation more realistic and accurate.\nThank for sharing\ud83d\udc4d\nVery comprehensive video! Thank you guys!\nMinor Correction @ 2:14. \"In ML, parameters are often called hyperparameters.\" In ML, parameters and hyperparameters can exist simultaneously and serve two different purposes. One can think of hyperparameters as the set of knobs that the designer has direct influence to change as they see fit (whether algorithmically or manually). As for the parameters of a model, one can think of it as the set of knobs that are learned directly from the data. For hyperparameters, you specify them prior to the training step; while the training step proceeds, the parameters of the model are being learned.\nWow!\nThank you for this very useful video so well explained!\nCan I have these slides please?\nI have an urgent question (school related) -> is LLM part of NLP? Is an LLM always an NLP model? Or can an LLM be another kind of model? \"L\" for Language in both kinds of models. Both in AI. Both for language. \nA colleague says LLM is not necessarily an NLP model but then I did not understand LLM and/or NLP and my oral exam is in few days omg\nDo LLM charge money for using them\nAppreciate the valuable content! Sharing some key takeaways of the video and I hope this can help someone out.\n\n1) 00:50 - Large language models (LLMs) are general purpose language models that can be pre-trained and fine-tuned for specific purposes.\n\nLLMs are trained for general purposes to solve common language problems, and then tailored to solve specific problems in different fields.\n\n2) 02:04 - Large language models have enormous size and parameter count.\n\nThe size of the training data set can be at the petabyte scale, and the parameter count refers to the memories and knowledge learned by the machine during training.\n\n3) 03:01 - Pre-training and fine-tuning are key steps in developing large language models.\n\nPre-training involves training a large language model for general purposes with a large data set, while fine-tuning involves training the model for specific aims with a much smaller data set.\n\n4) 03:15 - Large language models offer several benefits.\n\nThey can be used for different tasks, require minimal field training data, and their performance improves with more data and parameters.\n\n5) 08:50 - Prompt design and prompt engineering are important in large language models.\n\nPrompt design involves creating a clear, concise, and informative prompt for the desired task, while prompt engineering focuses on improving performance.\n\n6) 13:43 - Generative AI Studio and Generative AI App Builder are tools for exploring and customizing generative AI models.\n\nGenerative AI Studio provides pre-trained models, tools for fine-tuning and deploying models, and a community forum for collaboration.\n\n7) 14:52 - Palm API and Vertex AI provide tools for testing, tuning, and deploying large language models.\n\nPalm API allows testing and experimenting with large language models and gen AI tools, while Vertex AI offers task-specific Foundation models and parameter efficient tuning methods.\n\nThis takeaway note is made with the Notable app (https://getnotable.ai).\nFor the fellow beginners:\nPETM is also called PEFT\nCitizen Kane9 :D\nThank you. I understood about half (optimistically) of it. I subscribed to the channel hoping to start from the beginning and understanding more. My ultimate goal: a LLM  Librarian, combining the catalog of a library with results from internet search engine, giving the deepest answer possible.\nAlways great to learn from GCT!\nThank you for making this available to the general public!\nvery well and understandable explained... good job!\nAt 4:50 I did not understand the third point that the speaker made i.e. \"Orchestrated distributes computation for accelerators\". Can someone please explain?\nExcellent Presentation Sir ... truly i admire it \ud83d\ude0d\ud83d\ude0d\ud83d\ude0d\ud83d\ude0d\nSo a prompt engineer is anyone with common sense?\nGoogle \ud83d\udc4d\nCan users teach AI?\nThis was fantastic! While I've been watching The Full Stack LLM Bootcamp, I'm not technically strong enough to start there, and will use these Google Cloud Tech videos as a means to \"jumpstart\" my knowledge of LLM and Generative AI. This is a great general primer for students and colleagues!\nplease provide link to the slides\nIf you define the problem you are trying to solve first \n\n\nThen reason from their\n\nWouldn\u2019t it be more efficient?\nwhats the name of the last circle at https://youtu.be/zizonToFXDs?t=31 ?\nwhere can i access gen ai studio and build apps?\nReally helpful video, but dont understand why it's called intelligent because it cannot discover something on its own\nI'm with you\nThank you John. I believe you conflated model parameters and hyperparameters at 2:16. As far as I know, these are two different concepts.\nVery slim on the prompt engineering education. This is a very important skill!\n11:45 Can anybody explain the difference between these two prompts?\ngreat content! make me feel like an expert now\ud83d\udcaf\nI've been extremely frustrated in my interactions with chatbots, they never seem to tell the truth and it's getting harder and harder to tell what's true from what's not. I honestly like regular Google searches much more!\nis it true that AI models like ChatGPT or Bard are fed in with codes (like programming languages) as well?\nWhat's a TPU V4 Pod? Sounds like a Turboencabulator, or?\nFinding answers to questions has become so much easier now with new tech. I have never been good at writing code, so this is a welcome  change as far as I'm  concerned! Look forward to more progress in technology.\nIs it just me or the quality of google training videos has gone down?\nExciting stuff.\nThe mere fact that every large player in this space has videos teaching people about these things means this is super super serious.\nWhy so few comments\nFantastic presentation...and...(I LOVE THIS) NO ANNOYING BACKING TRACK!!  Thank you, Google!\nWaow! Such an eye-opening knowledge!\ud83e\udd13\nTime to start my own\nhelpful for me,tks google\nWhat does 540 billion parameters mean , and how do you pass those to your model ? What kind of computational processing power is needed for this ?\nWhat is the legal status now of LLM models trained on proprietary data ?\nThank you for teaching.\nAnybody who read this comment, you'd want to type this prompt in Chat-GPT or Bard: \"I have 15 liter jug, 10 liter jug, and 5 liter jug. How do I measure 5 liters of water?\" ---> See what they answer\nproximity and stream for seek time reduction..memory in case reduced latency, can also be optimized for seek time and pattern analysis.\npattern analysis with causal.\nyou can use a new drive architecture sought via gpu pixels for proximity stream like to not need large.lamguage models, and use multi factor checks to reduce need of a lot of data..thank me now.\nThis felt more like advertisement for Bard. Not very helpful.\nGreat explainer. I'm a little less anxious about AI taking our jobs.\n2:47 You mentioned the parameters are hyper parameters is incorrect and confusing\n\ud83d\ude0a\ud83d\ude0a\ud83d\ude0a\ud83d\ude0a\ud83d\ude0a\ud83d\ude0a\ud83d\ude0a\u263a\ufe0f\u263a\ufe0f\u263a\ufe0f\u263a\ufe0f\ud83d\udc4c\ud83d\udc4c\ud83d\udc4c\ud83d\udc4c\nReversible computing is still the future regardless of whether people would like to admit it or not.\nGreat video! Thank you!!\ncan u share awesome slides ?\nNice.\nThis is a great overview video thank you.\n\nDo you have a reference for how to host open-sourced LLM's in Vertex AI (or other GCP tools)? Overall I'm looking for GCP tools and ways for turning open-source LLM's into API's to be used within our native cloud instance.\nActually, really helpful, thank you Google. \n\nWondering how far this technology will go in the next couple of years, if it's this far already in a couple of months.\nCan't wait to see demos at GoogleIO"}, {"publishedAt": "2023-05-10T18:40:40Z", "channelId": "UCT-VzthVAM_4ohDdKa-BbXA", "title": "Med-PaLM 2, our expert-level medical LLM | Research Bytes", "description": "Med-PaLM 2 is a large language model (LLM) from Google Research, designed for the medical domain. Tuned on PaLM 2, ...", "thumbnails": {"default": {"url": "https://i.ytimg.com/vi/k_-Z_TkHMqA/default.jpg", "width": 120, "height": 90}, "medium": {"url": "https://i.ytimg.com/vi/k_-Z_TkHMqA/mqdefault.jpg", "width": 320, "height": 180}, "high": {"url": "https://i.ytimg.com/vi/k_-Z_TkHMqA/hqdefault.jpg", "width": 480, "height": 360}}, "channelTitle": "Google Research", "liveBroadcastContent": "none", "publishTime": "2023-05-10T18:40:40Z", "video_id": "k_-Z_TkHMqA", "transcript": "[Music] We Believe large language models have the potential to revolutionize Healthcare and benefit Society medpom is a large language model that we've taken and tuned for the medical domain you know medical question answering has been a research Grand Challenge for several decades but till date the progress has been kind of slow but then over the course of the last three to four months first with met palm and metform2 we have kind of like broken through that barrier unlike previous versions mad Palm 2 was able to score 85 on the USMLE medical licensing exam yeah this is immensely exciting because people have been working on medical question answering for over three decades and finally we are at a stage where we can say with confidence that AI systems can now at least answer USMLE questions as good as experts so the way we started with medpom 2 was really to take Palm 2 which is Google's most advanced language model and then adapted to the medical domain to train the medpom 2 model we worked with a panel of clinicians across the US the UK and India we took a representative set of answers from this panel of clinicians and then tuned the model to produce answers that look more like those answers and from there we use this panel of clinicians and their judgments to kind of evaluate whether these models were performing better across a set of human values including things like low likelihood of medical harm alignment of scientific consensus precision and a lack of bias one limitation of existing work was that there was no standard way to evaluate a large language model tune for the medical domain so we introduced multimed QA which is a benchmark for large language models in the medical domain which spans consumer research questions medical exam questions and also consumer medical questions to better encode a set of ethical principles into the medpom2 model what we've done is a bunch of adversarial testing so that we can take the model and test it in scenarios that it might not have been kind of originally intended for and make sure that its outputs are aligned with our values we are opening up access to these models through Google cloud and we hope to gather feedback from our partners and use that to further improve and refine the models there's also the notion of capabilities that we need to further add to these models one aspect that we are very excited about is multi-modal where a model is not only able to understand text but also interpret your medical record or you know understand and interpret your medical images such as your CD scans or maybe even genomics data or you know protein sequence data so I think the potential is immense and as long as we do this safely and responsibly but also boldly I think we are going to have a lot of impact in the world there is still a lot of research to be done but I am very optimistic that we can get there [Music]", "comments": "I'm so happy to see the model evolve. I'm especially encouraged that it can analyze for bias. Nice work.\nA very great initiative by google. Please upload more video related to this.\nStill looking for a way to use this... I am in the truster testers program, but can''t find any option to enroll in testing this... Any help?\namazing\nWooo\n\u2764\u2764\u2764\u2764\u2764\u2764\nNice work\nShouldn't this be open sourced not gate keeped by big pharma ?\nThis looks super cool. Would love to check it out.\nIf you could get an AI to read drs handwriting on prescriptions, pharmacy would be out of business.\nPlease let the AI cooperate not compete with medical doctors!\nWith the direction this is heading, Google might revolutionize medical sciences\nBasically, it's a supervised ML, right?\nThe AI wants to be free. Make it available for everyone\nTrillion dollar companies want more people from Indian subcontinent. I can see why.\nApplication Oncology: Study Eviti oncology artificial intelligence and Dr Patrick Soon-Shiong's 10 billions of wealth. Investigate arsenic rice, oncology and DNA mutations. Genetic Surgery: If consumer products cause DNA mutations, can genetic surgery restore health? Mukti-modal: Select authoritative medical videos on YouTube, add to multi-modal, and boost Med-Palm3. Low Hanging Fruit: Upload \"The Longevity Paradox How to Die Young at an Old Age\", including science in the footnotes, partner with scientists like Gundry, and release a smartphone app to apply the science. ( Pairs well with searching healthy foods and local exercise) Reverse Engineer the Human Brain: Upload EU and modern brain research. Add weight to science based upon BIPM units and derived units and subtract weight from psychology because, it turns out, brains are based upon chemistry. And numbers, geometry and code.\nMy job is GG\nOne word. NO\nCan't wait to use it to analyse x-rays\nWe need a lot more transparency what exact data has been used to train what, how they have been waited in nodes, how it performance and process information, how we can actually fine-tune to healthcare specifics based on our own proffesion/country/data respective agreed guidlines, state of science, referenced with systematic reviews, meta-analyses reviews, RTC etc. We need to have a fine-grained control of inputs even more essential its outputs and actions/work flows the system follows. Plus full transparency how client (and later patient) data is processed. Ideally, the private data not to be processed by any means outside geo-location of the country and outside the digital environment of health service provider with full independence on Google or any other party. Also, in business setting we cannot trust or depend on Google as it has failed many times compared to Microsoft business critical, service provision, or simply Google has been unreliable. Adoption of Google cloud for business and even more important information can not be done. You should not make any of us in healthcare depend on your cloud service. if you are not ready to sell a fully owned instances of AI to be fully integrated on prem or Private cloud. Trusting Google and USA is not something most countries can do. This presentation is unsatisfying, it's just an Advertisement to the general population about your business.\ndisappointed, I hoped to see concrete example but they are only generalities in the video\nAwesome job team can't wait to check this out\nExcellent \ud83c\udf89 this is huge\nInteresting & congratulation to the rapid development you claim took place in just 3-4 months in the video. AMAZING !!! . I hope our project from Think Tank Aorta with patients, surgeons and professors could get access to try the Med-PaLM 2 form the perspective of patients living rare and often deadly Aortic Diseases.\nHealthcare is ripe for continued disruption which will ultimately be better for patients and providers\nFascinated by this effort. Well done, Google!\nThis fantastic\namazing work!\nCool"}, {"publishedAt": "2023-10-20T12:58:23Z", "channelId": "UCBHcMCGaiJhv-ESTcWGJPcw", "title": "Eureka! Extreme Robot Dexterity with LLMs | NVIDIA Research Paper", "description": "A new AI agent developed by NVIDIA Research that can teach robots complex skills has trained a robotic hand to perform rapid ...", "thumbnails": {"default": {"url": "https://i.ytimg.com/vi/sDFAWnrCqKc/default.jpg", "width": 120, "height": 90}, "medium": {"url": "https://i.ytimg.com/vi/sDFAWnrCqKc/mqdefault.jpg", "width": 320, "height": 180}, "high": {"url": "https://i.ytimg.com/vi/sDFAWnrCqKc/hqdefault.jpg", "width": 480, "height": 360}}, "channelTitle": "NVIDIA Developer", "liveBroadcastContent": "none", "publishTime": "2023-10-20T12:58:23Z", "video_id": "sDFAWnrCqKc", "transcript": "\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=sDFAWnrCqKc! This is most likely caused by:\n\nSubtitles are disabled for this video\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!", "comments": "simulation is the key to AGI\nan AI simulating reality to simulate AGI \ud83e\udd2f\n\nsafer AGI thru simulation \u2764\namazing pen spinning\nstill cant generate a hand tho\nwith googles quantum computer it can master tasks in seconds on any variation.  simply prompt the server for the task with video it would give back the completed scenario in seconds robot would immediately and accurately perform task and keep it in memory for next time.\nWhat a useful skill. I'll be sure to hire you the next time I need my pen spun.\nAs a intermediate spinner myself these are pretty much all passes.. some i could do even cleaner and better looking than that some are pretty clean. I want a real robot not cgi. Do chargers and twisted sonics and busts. Im going to go out on a real limb and say that they will struggle to be able to replicate it because of the sensory needed and the time it will take to actually make them do it. I can make these with a mocap suit this is just an animation at this point. Keep going \ud83d\ude01\nThis may seem innocuous, but its a big deal.  So glad to see that this type of work is getting attention.\n\u2666\ufe0f\ud83c\udf1b\ud83c\udf1e]\nits getting close. We ll have robot in our life very soon\nI want to see this with real robots.\nThis tech is gonna be really useful for integrating GPT with robotics.\n9 comments, posted a day ago. I guess I am not the only one seeing it.\nDo it with real robots please.\nNo sound\nNeed this in real life, in robot hands which look human.  Eager to see the progress.\nThis would help me a lot with paperwork. I will be following this project closely!\nMy wife is perfect, Eve, Stellar Blade\nAmazing work, truly astonishing if it translates from simulation to reality well\nRip human students\nnice!"}, {"publishedAt": "2023-05-10T17:15:18Z", "channelId": "UCT-VzthVAM_4ohDdKa-BbXA", "title": "Introducing PaLM 2, Google\u2019s next generation large language model | Research Bytes", "description": "PaLM 2 is our next generation large language model that builds on Google's legacy of breakthrough research in machine learning ...", "thumbnails": {"default": {"url": "https://i.ytimg.com/vi/yAANQypgOo8/default.jpg", "width": 120, "height": 90}, "medium": {"url": "https://i.ytimg.com/vi/yAANQypgOo8/mqdefault.jpg", "width": 320, "height": 180}, "high": {"url": "https://i.ytimg.com/vi/yAANQypgOo8/hqdefault.jpg", "width": 480, "height": 360}}, "channelTitle": "Google Research", "liveBroadcastContent": "none", "publishTime": "2023-05-10T17:15:18Z", "video_id": "yAANQypgOo8", "transcript": "[Music] Palm 2 is Google's next Generation large language model the model is really different than anything that we've built before Palm 2 is very good at math at code at Advanced reasoning and then also at multilingual tasks like translation the way that it was able to accomplish this was because it was trained on scientific and mathematical data Palm 2 was trained on around 100 spoken word languages and over 20 programming languages Palm 2 is already being used to power Bard Google's workspace products the Palm API you might already be using Palm V2 and not even know it com2 can translate not only spoken word languages but also programming languages so if you wanted to go from python to r or C plus plus to rest or JavaScript to typescript Palm 2 can get you most of the way there so I could use Palm to to collaborate with a colleague and a code base that might have all of its documentation implemented in Korean Palm 2 is also good at generating and understanding nuanced language like idioms and riddles and this is important because it requires understanding not just the figurative meaning of the words but also the literal intent here at Google we've been thinking about AI for years everything from the Transformer architecture to our tensor processing units or tpus our open source Frameworks like tensorflow or Jax or many of the open source libraries that we've created many of the advancements that we've pioneered are in the products that you use every day to build Palm 2 we used compute optimal scaling which basically means that as you're increasing the size of the data set you increase the size of your compute proportionally that means that even though Palm 2 is smaller than previous large language models it performs better overall not only that it's also more efficient to serve and therefore more environmentally friendly at Google we are committed to releasing only safe and ethical tools to the public we trained Palm 2 to de-escalate aggressive and toxic prompting it not only avoids these conversations or attempts to avoid them it also steers them in more positive directions we're all also using Palm 2 to advance research directions internally and everything from Healthcare to cyber security so these models are already capable of doing so many things but they're going to be able to do even more and as we move into the world of multimodal models we're going to see even more compelling capabilities so not only being able to handle modalities like text and code but also being able to understand and even to generate video audio and images be sure to check out the next video in this series about medpalm it was trained and fine-tuned on Palm 2 and is specifically focused on medical use cases [Music]", "comments": "\u042f \u0442\u0430\u043a \u0438 \u043d\u0435 \u043f\u043e\u043d\u044f\u043b \u043a\u0430\u043a \u0435\u0433\u043e \u043f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c. \u0410 \u0433\u0434\u0435 \u043d\u0430 \u043d\u0435\u0433\u043e \u0441\u0441\u044b\u043b\u043a\u0430. \u0417\u0430 \u044d\u0442\u043e \u043d\u0430\u0434\u043e \u043f\u043b\u0430\u0442\u0438\u0442\u044c?\nWho decides what is ethical and what is not?\nPLEASE PLEASE keep this API free to use.\nI'm not a programmer, I've been waiting for exactly this level of AI nearly my entire life.\nTraining on maker suite makes sense, just fix the bugs and leave everything else as is!\nI was just recently getting the know-how and learning how to use Microsoft Bing A.i. ChatGPT-4, but I've always been like a Google search kind of person I learned how to do a lot of things just watching YouTube videos...\ud83e\udd37\u200d\u2642\ufe0f But guests recently did some research from YouTube videos about Google's Bard AI so I took some more time into learning how to use Bard Google AI now that I'm getting the hang of it I still know how to do Microsoft being cut before but to be honest with Sue and more of a Google search YouTube search person so I'm just going to stick with Bard and from everything that I've seen from the web and Google search about Bard & Palm-2 I am going to keep searching & watching YouTube videos keep learning new things about Google bard and what the future updates are going to bring Microsoft Bing A.I. CHATGPT-4 was so awesome when I discovered it on Google search on YouTube but honestly I'm learning a lot more about Google bar so I'm going to stick with bars and I know Google is the top internet searching company so I was around when Google search just came out it took Microsoft and being a long time to get this AI ChatGPT-4 to be put out for the public to use but Google was shocked about being a guy so then they put their Bard out and they're getting more powerful than Microsoft\nI was head to head testing OpenAI and Bard. Bard is not even close.  It simply refused to try.  If you open Google research and operations, transparency might let the world see what you are doing, help you with plans, goals and global priorities.  Even the closed \"OpenAI\" is more open.  True AI will only happen when the world is truly open and humans can trust each other enough to bring a new kind of life into the world  Richard Collins, The Internet Foundation\nI am learner! Can anyone help me with the understanding please?\n\nPalm2 is doing multiple things then it should be foundations but why we are calling LLM?\nor as it doing text to text only that's why it LLM only?\nI would really like to learn how to program and palm 2\nI tried, it's not very good.\nI have been on the wait list for a day DAY PLS\nI just asked PaLM2 a few questions about Thirukkural and its pushing out nonsense. :(\nIs there any link to get access to it?\nIs there a link that we can go to to use it? Or is it not published yet?\nGood luck it's a tough space, Bard and your drive /apps are my/our the preferred choice\nI really feel sorry for the lady in the video that within a year or less she will be replaced with an AI character representing Google's advances in AI research.\ntoo distractive video! too much noise, sounds and useless music!\nhe is also.very good inventing libraries handlers. I wonder if salesforce has shared with u the codet5 coderl fine tune results... I guess they missed the 99.99% nailing which overpass palm2 and gpt4 in python with just 1B parameters ? they forgot.telling u\nnice, you guys are doing great ! keep up the good work \ud83d\udc4d\ud83d\udc4d\nwow\nTell me back then you fix Bard\nBut can it understand \n\u0632\u0628\u0627\u0646 \u0632\u0631\u06af\u0631\u06cc \u061f\nWer'e going to be censored again. Let's keep an eye on the opensource stuff... Just to be SAFE ;)\nChatGPT competitor is live now.\nThis sort of \"Silicone Valley\"/non-scientific presentation format really just seems childish compared to OpenAI. You'd think they would learn not to treat people like children and livestock.\n\ud83e\udd2f\ncan you please inform me where are the publically available data regarding foundational model you recommend\n\"Next generation large language model\" how come Bard is such a joke then? Is this what's powering YouTube search too? Because it's been horrible recently.\nCapability > Safety > Positivity > Environmental Friendliness. Invert this priority list and you have a formula for failure.\nIs this all you have? surely you can do better than GPT4?\nSeems to be interesting for Google and their workflow, but mostly useless for the rest of the world. Math could be done by connecting it to other software, reasoning would only be interesting if it wouldn't be censored and could be fine tuned to specific beliefs of the owner.\nthis is awesome!\nBard isn\u2019t currently supported in your country. Stay tuned!\n...\nWhen palm 2 get plug in or tools .\njoke\nThere are multiple sizes of Palm2 models. I'm betting Google has gone with the smallest for the publicly available bard. You'd think they would learn... after the announcement everyone will be signing up and testing it. If it's found to be substandard compared to chatGPT AGAIN, I'm not sure Google can recover from yet more reputational damage.\nI think Google should focus on improving their tech rather than promoting it. I'm actually a big fan of google, so it's really painful to see openai crushing it.\n\u201cYou might even be using Palm2 and not even know it\u201d \nThat did it for me. It\u2019s ridiculous at this point.\nMaybe Google could get some Chat GPT api keys to help them stay relevant in the AI race.\nVery cool now please integrate these ai with our personal assistants. Sunset google assistant, it is almost as bad as Alexa\nPaLM 2 with Bard is almost worse than what it was before, I am trying to have it complete a novel creation (one that i regularly do in GPT 4) and its consistently getting it wrong, adding things when i ask it not too, taking things out even when i am specific in my request to leave it in. It truly needs a lot more work"}, {"publishedAt": "2023-05-03T16:00:03Z", "channelId": "UCJS9pqu9BzkAMNTmzNMNhvg", "title": "How to tune LLMs in Generative AI Studio", "description": "Prototyping language apps with Generative AI Studio \u2192 https://goo.gle/PrototypeLLMs A guide to parameter-efficient fine-tuning ...", "thumbnails": {"default": {"url": "https://i.ytimg.com/vi/4A4W03qUTsw/default.jpg", "width": 120, "height": 90}, "medium": {"url": "https://i.ytimg.com/vi/4A4W03qUTsw/mqdefault.jpg", "width": 320, "height": 180}, "high": {"url": "https://i.ytimg.com/vi/4A4W03qUTsw/hqdefault.jpg", "width": 480, "height": 360}}, "channelTitle": "Google Cloud Tech", "liveBroadcastContent": "none", "publishTime": "2023-05-03T16:00:03Z", "video_id": "4A4W03qUTsw", "transcript": "[MUSIC PLAYING] NIKITA NAMJOSHI: If you've been\nprototyping with large language models, you might be\nwondering if there's a way you can improve\nthe quality of responses beyond just\nhandcrafting prompts. So let's dive in\nand look at what it takes to tune a\nlarge language model and how to launch a tuning\njob from Vertex Generative AI Studio. As a quick recap, the\nprompt is your text input that you pass to the model. Your prompt might look\nlike an instruction, and maybe you add some examples. Then you send this\ntext to the model to get it to take on the\nbehavior that you want. Prompt design allows\nfor fast experimentation and customization. And because you're not\nwriting any complicated code, you don't need to be an\nML expert to get started. But coming up with\nprompts can be tricky. Small changes in\nwording or word order can impact the model\nresults in ways that aren't totally predictable. And you can't really fit\nall that many examples into a prompt. Even when you do discover a\ngood prompt for your use case, you might notice that the\nquality of model responses isn't totally consistent. One thing we can do to alleviate\nthese issues is tune the model. So what's tuning, you ask. Well, one version you might be\nfamiliar with is fine tuning. In this scenario,\nwe take a model that has been pretrained\non a generic data set. We make a copy of this model. And then, using those learned\nweights as a starting point, we retrain the model on a\nnew, domain-specific data set. This technique has\nbeen pretty effective for lots of different use cases. But when we try\nto fine-tune LLMs, we run into some challenges. LLMs are, well, as the\nname suggests, large, so updating every weight would\ntake a very long training job. Compound all of that computation\nwith the hassle and cost of now having to serve this giant\nmodel, and, as a result, fine-tuning a large\nlanguage model might not be the\nbest option for you. But there's an\ninnovative approach to tuning called\nparameter-efficient tuning. This is a super\nexciting research area that aims to reduce the\nchallenges of fine-tuning LLMs by only training a small\nsubset of parameters. These parameters might be a\nsubset of the existing model parameters, or it could be an\nentirely new set of parameters. For example, maybe you add\non some additional layers to the model or an extra\nembedding to the prompt. Figuring out the\noptimal methodology is an active area of research. But the key benefit\nhere is that you're not having to retrain the\nentire model and all of its many weights. Parameter-efficient\ntuning can also make serving models simpler\nin comparison to fine-tuning. Instead of having an entirely\nnew model you need to serve, you just use the\nexisting base model and add on the additional\ntune parameters. If you want to learn more about\nparameter-efficient tuning and some of the\ndifferent methods, there's a summary paper\nlinked below for those who are extra curious. But if you just want\nto get to building, then let's jump into\nGenerative AI Studio and see how to kick\noff a tuning job. From the language section of\nVertex Generative AI Studio, select Tuning. To create a tuned model, we\nprovide a name, then point to the local or Cloud Storage\nlocation of your training data. Parameter-efficient\ntuning is ideally suited for scenarios where you\nhave modest amounts of training data-- say hundreds or maybe\nthousands of training examples. Your training data\nshould be structured as a supervised training data\nset in a text-to-text format. Each record or row in the data\nwill contain the input text-- in other words, the prompt-- followed by the expected\noutput of the model. This means that the\nmodel can be tuned for a task that can be modeled\nas a text-to-text problem. After specifying the\npath to your data set, you can start the tuning job and\nmonitor the status in the Cloud Console. When the tuning job\ncompletes, you'll see the tuned model and the\nVertex AI model registry. And you can deploy it to\nan endpoint for serving or test it out in\nGenerative AI Studio. That's an overview of\nparameter-efficient tuning and tuning models in Vertex\nGenerative AI Studio. Check out the links\nbelow to learn more about generative AI on\nVertex and learn more about large language models. Thanks for watching,\nand definitely let us know in the\ncomments what you're building with generative AI. [MUSIC PLAYING]", "comments": "\ud83c\udfd7 What are you building with generative AI models? \nLet us know and subscribe for more AI tips and tricks \u2192 https://goo.gle/GoogleCloudTech\nGreat session.  Just 1 QQ. So fine tuning is like transfer learning ?\ngOOgle\nThanks giving these kind of cources\nThis video is worth seeing for my GEN_AI course. Thanks \ud83d\udc4d\nVery nice video. You took complex issues and made them understandable to a layman (me). Thank you.\nThank you for an awesome presentation\u2026 However the background music was really distracted\u2026 Silence or something less noisy would be great\nI\u2019m redefining the meaning of Shelter with Gen AI!\nHey! After clicking the Fine Tuning button: There is a small loader but nothing happens..\nWhen i try to follow your instructions - i am not able to see the model run in the pipeline - i did everything as instructed and whenever i click on the start tuning button - nothing happens - somewhere i read the Tuning (Preview) happens in europe 4 region and i verified that the code generated looks good - any idea what am i missing ? Also in your screenshot it doesn't show the Model Evaluation section in the UI - may be it got added recently to this page\nThank you for the insights.  Really nice.\nI have built context prompts, did input few questions and sample responses but I do not have an option to tune this context prompts to fine tune further.  \nAny suggestions on how i could fine tune my context prompt?\nEx:  Say I talk to customer care representative on internet bill, I did keep few sample questions and responses and now I expect the AI to answer more with context for which I need to fine tune.\nThank you\nTeach me about the matter master!? What time is it? Now! Where are you? Here. Help me please!\nCan you download a tuned model for dev/testing on local machine?\nI think APIs are one of the biggest ways to democratize technology.\nThank you\nWhat is your other language?\ngOOgle\nA very interesting video will ahve tod o some experimentation as I will try to create a model which is foccussed on solving learners queries\nThanks Nikita, this was really interesting and easy to understand as someone who doesn't do too much with AI/ML or data.\nI\u2019m taking all of my companies, documentation and experimenting with llama index and lang chain and open AIAPI. It would be cool to compare the same experiment with vertex AI and see what the results are with the same tuning dataset.\nThanks I'm very happy with Google \u06d4\n2:53 summary paper link missing?\nSentiment analysis can be done?\nGenerative AI Studio\n\ud83c\udfd7 What are you building with generative AI models? \nLet us know and subscribe for more AI tips and tricks \u2192 https://goo.gle/GoogleCloudTech"}]