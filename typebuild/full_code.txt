File: simple_auth.py

import hashlib
import os
import pickle as pk
import streamlit as st
import time
import pickle as pk
import extra_streamlit_components as stx
import json
import datetime

# This expander will be invoked right when simple auth is imported into main.py
# putting it here will ensure that auth is right on top.


def create_auth_file():
    profile_dict = {'admin': None}
    if not os.path.exists(st.session_state.profile_dict_path):
        os.makedirs(os.path.dirname(st.session_state.profile_dict_path), exist_ok=True)
    with open(st.session_state.profile_dict_path, 'wb') as f:
        pk.dump(profile_dict, f)
    print("Created profile dict")
    return None

def reset_user_password():

    with open(st.session_state.profile_dict_path, 'rb') as f:
        p = pk.load(f)
    users = list(p)
    users.remove('admin')
    users.insert(0, 'Select')
    reset_user = st.selectbox('Select user to reset password', users)

    if st.button("Reset this user's password"):
        p[reset_user] = None

        with open(st.session_state.profile_dict_path, 'wb') as f:
            pk.dump(p, f)
        st.success(f"Reset done. Ask {reset_user} to create a new password")
    return None

def delete_users():
    with open(st.session_state.profile_dict_path, 'rb') as f:
        p = pk.load(f)
    users = list(p)
    users.remove('admin')
    to_delete = st.multiselect("Delete users", users)
    if to_delete:
        if st.button("Confirm deletion"):
            for t in to_delete:
                del p[t]
            with open(st.session_state.profile_dict_path, 'wb') as f:
                pk.dump(p, f)
            st.success("Done")
            time.sleep(2)
            st.rerun()
            
    return None

def get_profile_dict():
    if not os.path.exists(st.session_state.profile_dict_path):
        create_auth_file()
    with open(st.session_state.profile_dict_path, 'rb') as f:
        profile_dict = pk.load(f)
    return profile_dict

def get_key(salt, pwd):
    return hashlib.pbkdf2_hmac('sha256', pwd.encode('utf-8'), salt, 100000)


def set_key(token, pwd):
    salt = os.urandom(32) # A new salt for this user
    key = get_key(salt, pwd)
    profile_dict = get_profile_dict()
    profile_dict[token] = {salt: key}
    with open(st.session_state.profile_dict_path, 'wb') as f:
        pk.dump(profile_dict, f)
    
    return None

def add_user(user):
    profile_dict = get_profile_dict()
    profile_dict[user] = None
    with open(st.session_state.profile_dict_path, 'wb') as f:
        pk.dump(profile_dict, f)
    
    return None

def simple_auth():
    '''
    This tries to authenticate first with session state variable,
    next with a cookie and if both fails, it asks for the user to login. 

    It also creates a logout button.
    '''

    # Get the path to the profile dict
    st.session_state['profile_dict_path'] = os.path.join(os.path.expanduser("~"), '.typebuild', 'users', 'admin', 'profile_dict.pk')
    # Create a logout button right on top.
    if 'new_menu' in st.session_state:
        if st.session_state.new_menu == 'logout':
            st.session_state['logmeout'] = True
    # Default token
    token = None
    # If the logout button was pressed, it will create
    # a session state to log out.  Created this workaround
    # due to the behaviour of the cookie_manager object.
    # If logmeout is there, invoke logout.
    # The user has to close the browser tab to login again.  The new session will not have this variable in session state.
    if 'logmeout' in st.session_state:
        pass
    # If there is authorization in the system, use it.
    elif 'token' in st.session_state:
        token = st.session_state['token']
    else:
        cookie_manager = stx.CookieManager()
        token = get_cookie_token(cookie_manager)

    # If not authorized in this session, do that.
    if token is None:
        if 'logmeout' not in st.session_state:
            token = get_auth(cookie_manager)
            st.session_state['token'] = token
        else:
            st.warning("You just logged out.  Please close the browser tab. Or refresh the page to login again.")
    if token is None:
        st.stop()        
    if token == 'admin': 
        with st.sidebar.expander("Admin"):
            add_user_if_admin(token)
            use_as_user = st.text_input("View as user")
            if use_as_user != '':
                token = use_as_user
            reset_user_password()
            delete_users()

    return token

def get_cookie_token(cookie_manager):
    token = None
    cookies = cookie_manager.get_all()
    if isinstance(cookies, str):
        cookies = json.loads(cookies)
    cookie_token = None if cookies is None else cookies.get('cookie_token', None)
    profile_dict = get_profile_dict()
    if cookie_token is not None:
        if cookie_token in profile_dict:
            token = cookie_token
            st.session_state['token'] = token    
    return token

def logout():
    cookie_manager = stx.CookieManager()
    try:
        cookie_manager.delete('cookie_token')
    except:
        pass
    return None

# Add user
def add_user_if_admin(token):
    user_name_new = st.sidebar.text_input("Add User name")
    if st.sidebar.button("Add user"):
        add_user(user_name_new)
        st.sidebar.success("Added user")

def get_auth(cookie_manager):
    temp_header = st.empty()
    token = st.text_input('username', autocomplete='first_name').lower()
    pwd = st.text_input("What's the password", type='password')

    if token == '':
        temp_header.error("Please sign in")
        st.stop()

    else:
        profile_dict = get_profile_dict()
        # If there is no user, it'll return None
        if token not in profile_dict:
            st.error("Sorry, you do not have access to the system.  Please write to the admin for access.")
            st.stop()
        # If the user is in the system.
        else:
            salt_key = profile_dict[token]

            # They profile will not have the salt and key if the user has not set a password
            # Ask user to set password        
            if salt_key is None:
                st.error("You have not set a password yet, please set one")
                pwd2 = st.text_input("Please repeat your password", type="password")
                if st.button("Set password"):
                    if pwd == pwd2:
                        set_key(token, pwd)
                        st.success("I've set the password for you.  Please contact admin if you have to reset it")
                        time.sleep(3)
                        st.rerun()
                    else:
                        st.warning("The two passwords you typed do not match.  Please correct.")
                st.stop()
            # If the user has set a password.
            else:
                salt = list(salt_key.keys())[0]
                orig_key = list(salt_key.values())[0]

                entered_key = get_key(salt, pwd)
                if entered_key != orig_key:
                    dummy = st.button("Submit", key='pwd_dummy')
                    st.error("Please enter the correct password or contact admin to reset")
                    st.stop()
                else:
                    # Not sure why I needed this...so commenting it out.
                    #if isinstance(st.session_state['cookies'], str):
                        #st.session_state['cookies'] = json.loads(st.session_state['cookies'])
                    # Set it to expire in 2 weeks
                    days_from = datetime.datetime.now() + datetime.timedelta(14)
                    cookie_manager.set('cookie_token', token, expires_at=days_from)
    return token
--------------------File: tb_settings.py
import streamlit as st
from project_management.project_management import ProjectManagement
from llm_access import LLMConfigurator

def settings_main():
    # Add a test menu
    # Get menu object
    menu = st.session_state.menu
    settings_menu_items = [
        ["HOME", "Projects", "projects","tb_settings"],
        ["HOME", "Settings", "settings","tb_settings"],
        ["Settings", "LLM Access", "llm_access_settings","tb_settings"],
    ]
    
    menu.add_edges(settings_menu_items)

def settings():
    st.title("Settings")
    st.info("This is the settings page")
    return None

def projects():
    """
    Activities in TypeBuild are housed under projects.  This function opens a UI 
    that allows users to create new projects and select existing projects.
    Once a project is selected, the user can manage data, create apps, and use LLMs.
    """
    st.title("Projects")
    if 'project_manager' not in st.session_state:
        st.session_state.project_manager = ProjectManagement(st.session_state.user_folder)
    pm = st.session_state.project_manager
    pm.manage_project()
    
    return None

def llm_access_settings():
    st.title("LLM Access")
    if 'llm_configurator' not in st.session_state:
        st.session_state.llm_configurator = LLMConfigurator()
    lc = st.session_state.llm_configurator
    lc.config_project()
    return None
--------------------File: success.py
import streamlit as st

def print_success():
    st.success("Success!")
--------------------File: ideation.py
# JUST CREATED THIS FILE...HAVE TO WORK ON IT.
# Moved functiions from project_management.py to here.

def ideate_project():
    """
    This stores the user requirement for the given view,
    based on the selected menu. 
    """
    file_path = os.path.join(st.session_state.project_folder, 'project_settings', 'project_description.txt')
    key = 'Project Description'
    widget_label = 'Project Description'
    st.subheader('Ideate')
    project_description = text_areas(file=file_path, key=key, widget_label=widget_label)
    # Save to session state
    st.session_state.project_description = project_description

    ideation_chat()
    return None


def ideation_chat():
    """
    A chat on the project description.
    That could be exported to the project description file.
    """
    # If there is no project description chat in the session state, create one
    if 'ideation_chat' not in st.session_state:
        st.session_state.ideation_chat = []
    
    chat_container = st.container()
    prompt = st.chat_input("Enter your message", key='project_description_chat_input')
    if prompt:
        # Create the messages from the prompts file
        prompts.blueprint_prompt_structure(prompt=prompt)
        with st.spinner('Generating response...'):
            res = get_llm_output(
                st.session_state.ideation_chat, 
                model='gpt-3.5-turbo-16k'
                )
            # Add the response to the chat
            st.session_state.ideation_chat.append({'role': 'assistant', 'content': res})
    
    # Display the user and assistant messages
    with chat_container:
        for msg in st.session_state.ideation_chat:
            if msg['role'] in ['user', 'assistant']:
                with st.chat_message(msg['role']):
                    st.markdown(msg['content'])

    return None

--------------------File: __init__.py
from typebuild.app import run
--------------------File: simple_cf.py
import time
import streamlit as st
import requests
import json
from simple_agent_definitions.get_agent_definitions import get_main_instruction
from tools.new_code_agent import tool_main as new_code_agent

def display_messages():
    for message in st.session_state.conversations:
        if message["role"] == "user":
            avatar = "👤"
        elif message["role"] == "assistant":
            avatar = "🤖"
        else:
            avatar = "💻"
        # Show only user and assistant messages

        if message["role"] in ["user", "assistant"]:   
            with st.chat_message(message["role"], avatar=avatar):
                st.write(message["content"])    

def get_tools():
    file = "tools.json"
    with open(file, "r") as f:
        tools = json.load(f)
    return tools

def get_llm_response(messages, functions, model="gpt-4-turbo-preview", max_tokens=2000):
    url = "https://general.viveks.info/generate"
    # url = "https://general.viveks.info/claude"
    model = 'gpt-3.5-turbo'
    # The data to be sent to the endpoint
    data = {
        "messages": messages,
        "model": model,  # Optional: Specify the model you want to use
        "max_tokens": 2000,  # Optional: Specify the maximum number of tokens
        "functions": get_tools()
    }

    # The headers for the request, including the Authorization token
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer llm_token"  # Replace with your actual token
    }

    # Make the POST request
    response = requests.post(url, headers=headers, data=json.dumps(data))
    return response

def manage_llm_interaction():
    """"
    This function calls the llm and gets the response, when required.
    """
    if 'ask_llm' not in st.session_state:
        st.session_state.ask_llm = False
    
    if st.session_state.ask_llm:
        with st.spinner("Thinking..."):
            response = get_llm_response(
                messages=st.session_state.conversations, 
                functions=get_tools(), 
                model='gpt-4-turbo-preview', 
                max_tokens=2000
                )
        if response.status_code == 200:
            response = response.json()
            
            st.session_state.latest_response = response
            message = response.get('message')
            if message:
                st.session_state.conversations.append({"role": "assistant", "content": message})
            
            if response.get('function_call'):
                function_call(response['function_call'])
                # Set the ask_llm to True
                st.session_state.ask_llm = True
                st.rerun()
            # If no function call, set ask_llm to False
            else:
                st.session_state.ask_llm = False
                # Rerun the app
                st.rerun()
        else:
            st.error(f"Error: {response.text}")


def function_call(function_call):
    name = function_call['name']
    arguments = function_call['arguments']
    # If arguments are a string, convert to dictionary
    if isinstance(arguments, str):
        arguments = json.loads(arguments)
    
    # show the arguments
    st.info(f"Calling function: {name}")
    st.info(f"Arguments: {arguments}")
    time.sleep(3)
    if name == "agent_code_creator":
        st.info("Starting code creator...")
        with st.spinner("Generating code..."):
            res = new_code_agent(arguments['objective'], arguments['file_name'], arguments['description'])
        st.code(res)
        st.stop()
    # If the name starts with 'endpoint_', remove it
    if name.startswith("endpoint_"):
        name = name.replace("endpoint_", "")
    # Call the llm endpoint
    url = f"https://general.viveks.info/{name}"
    with st.spinner(f"Calling {name}..."):
        if isinstance(arguments, str):
            arguments = json.loads(arguments)
        response = requests.get(url, params=arguments)
    # If the response is successful, add the response to the conversations
    if response.status_code == 200:
        response = response.text
        st.session_state.conversations.append({"role": "function", "name": name, "content": response})
    else:
        st.session_state.conversations.append({"role": "function", "name": name, "content": f"There was an error processing your request: {response}"})

    return None

def chat():

    # If the conversation is empty, add the main instruction
    if len(st.session_state.conversations) == 0:
        st.session_state.conversations.append({"role": "system", "content": get_main_instruction()})
    # Add last response to the sidebar, if it exists
    if 'latest_response' in st.session_state:
        st.header("Latest response:")
        st.sidebar.write(st.session_state.latest_response)

    prompt = st.chat_input("Type your message here")
    if prompt:
        st.session_state.conversations.append({"role": "user", "content": prompt})
        st.session_state.ask_llm = True
    display_messages()
    manage_llm_interaction()
--------------------File: task.py
import yaml
import os
import streamlit as st
import importlib

from extractors import Extractors

class Task:
    """
    A class for tasks.  Each task is assigned to an agent.
    Some agents can deletgate tasks to other agents. In such cases,
    they know a list of available agents.
    """

    def __init__(self, task_name, agent_name, task_description, available_agents=[], tools=[]):
        self.task_name = task_name
        self.tools = tools
        self.agent_name = agent_name
        self._parse_instructions(agent_name)
        self.task_description = task_description
        
        
        # Some agents can delegate tasks to other agents.
        # This dict contains the names and descriptions of the available agents.
        self.agent_descriptions = {}
        self._set_available_agent_descriptions(available_agents)

        return None
   
    def to_dict(self):
        return {
            'task_name': self.task_name,
            'tools': self.tools,
            'task_description': self.task_description,
            'agent_descriptions': self.agent_descriptions,
        }

    def _replace_placeholder_with_actual_values(self):
        # TODO: DOCUMENT WHAT THIS DOES.  
        # Should we rename this so we understand what context means?
        if hasattr(self, 'get_data_from'):
            module_name = self.get_data_from.get('module_name', '')
            function_name = self.get_data_from.get('function_name', '')
            # import the function from the module and run it
            tool_module = importlib.import_module(module_name)
            tool_function = getattr(tool_module, function_name)
            data_for_system_instruction = tool_function()
        else:
            data_for_system_instruction = None

        # st.sidebar.info(f"Agent name is {self.get_instance_vars()}")
        # Take the data_for_system_instruction and replace the variables in the system_instruction
        instruction = self.system_instruction
        if data_for_system_instruction:
            for key, value in data_for_system_instruction.items():
                instruction = instruction.replace(f"{{{key}}}", value) # We are using triple curly braces to avoid conflicts with the f-strings
            
        
        return instruction

    
    def _parse_instructions(self, agent_name):
        """
        Parse the instructions for a given agent.

        Args:
            agent_name (str): The name of the agent.

        Returns:
            dict: The parsed instructions.

        Raises:
            FileNotFoundError: If no system instruction is found for the given agent.
        """
        
        path = os.path.join(os.path.dirname(__file__), 'agent_definitions', f'{agent_name}.yml')
        
        if os.path.exists(path):
            with open(path, 'r') as f:
                instructions = yaml.load(f, Loader=yaml.FullLoader)
            # Parse the variables
            for key in instructions:
                setattr(self, key, instructions[key])
            
            return instructions
        else:
            st.warning(f"{agent_name} must have been removed from the system.")


    def get_instance_vars(self):
        """
        Returns a dictionary containing all the instance variables of the object.
        """
        return self.__dict__


    def _get_tool_defs(self):
        """
        Returns a formatted string containing the list of available tools and their docstrings.
        
        The method retrieves the docstrings of the tools specified in the `self.tools` attribute.
        It then formats the information into a well-structured string, including the tool name and its docstring.
        
        Returns:
            str: A formatted string with the list of available tools and their docstrings.
        """
        add_to_instruction = ""
        if self.tools:
            add_to_instruction += """THE FOLLOWING IS A LIST OF TOOLS AVAILABLE.  DO NOT MAKE UP OTHER TOOLS.  
            CALL THEM BY THEIR NAME VERBATIM.
            
            TO USE THE TOOL, RETURN A WELL FORMATTED JSON OBJECT WITH the tool_name and kwargs as keys.
            YOU CAN FIND THE DOCSTRING OF THE TOOLS BELOW:

            """
            tools = st.session_state.extractor.get_docstring_of_tools(self.tools)
            
            for tool in tools:
                add_to_instruction += f"\n===\n\n{tool}: {tools[tool]}\n\n"
            
        
        return st.session_state.extractor.remove_indents_in_lines(add_to_instruction)

    # TODO: Deprecate? This is not used anywhere.
    def _available_tools(self):
        """
        Returns a dictionary of available tools.

        The dictionary contains the names of the available tools as keys and their respective docstrings as values.

        Returns:
            dict: A dictionary of available tools with their docstrings.
        """
        extractor = Extractors()
        tools = {}
        for file in os.listdir(os.path.join(os.path.dirname(__file__), 'tools')):
            if file.endswith('.py'):
                tools[file] = extractor.get_docstring_of_tool(file)
        return tools

    def _set_available_agent_descriptions(self, available_agents):
        """
        Set the available agent descriptions.

        Args:
            available_agents (list): A list of available agent names.
        """
        for agent_name in available_agents:
            path = os.path.join(os.path.dirname(__file__), 'agent_definitions', f'{agent_name}.yml')
            with open(path, 'r') as f:
                instructions = yaml.load(f, Loader=yaml.FullLoader)

            description = instructions.get('description', '')
            if description:
                self.agent_descriptions[agent_name] = description
        return None
    
    def get_system_instruction(self):
        """
        Add the agent name and description to the system instructions

        Args:
            agent_name (str): The name of the agent

        Returns:
            str: The system instruction with the agent name and description
        """

        instruction = self._replace_placeholder_with_actual_values()
        # Add tools to the instruction
        instruction += self._get_tool_defs()
        
        # If this agent could delegate, add that information
        # to the instruction.
        if self.agent_descriptions:
            instruction += "\n\nTHE FOLLOWING IS A LIST OF AGENTS AVAILABLE.  DO NOT MAKE UP OTHER AGENTS.  CALL THEM BY THEIR NAME VERBATIM:\n"
            for agent_name, description in self.agent_descriptions.items():
                instruction += f"- {agent_name}: {description}\n"

        instruction += """Your response must always be a valid json.  It could contain different keys based on the task.
        The following keys should always be present in the response:
        - user_message: Your message to the user
        - ask_human: boolean.  If True, the agent will ask for human intervention.        
        """        
        if self.task_name != 'planning':
            instruction += "\n- task_finished: boolean"
        
        instruction += "\n\n===\nTASK NAME: " + self.task_name + "\n\n"
        instruction += "TASK DESCRIPTION: " + self.task_description + "\n\n"

        return instruction



--------------------File: new_chat.py

import streamlit as st
from plugins.llms import get_llm_output
import os
from glob import glob
from task import Task
from task_graph import TaskGraph
import importlib

import inspect

def display_messages(messages, expanded=True):
    """
    Displays the messages in the chat.

    Utilizes Streamlit's expander and chat_message for displaying messages.
    This method iterates through the messages list and displays each one based
    on the role (user, assistant, system).

    Returns:
        None
    """
    if messages:
        with st.expander("View chat", expanded=expanded):
            for i, msg in enumerate(messages):
                # TODO: REMOVE SYSTEM MESSAGES AFTER FIXING BUGS
                
                if msg.role in ['user', 'assistant']:
                    with st.chat_message(msg.role):
                        if msg.content.startswith('{'):
                            msg.content = eval(msg.content)
                            st.json(msg.content)
                        if isinstance(msg.content, dict):
                            st.json(msg.content)
                        else:
                            st.markdown(msg.content.replace('\n', '\n\n'))
                if msg.role == 'system':
                    st.info(msg.content)        
    return None



def test_main():
    # Add a test menu
    # Get menu object
    menu = st.session_state.menu
    test_menu_items = [
        ['HOME', 'Chat', 'chat', 'test']
    ]    
    menu.add_edges(test_menu_items)
    return None

def get_template():
    task_graph = TaskGraph("Create haikus on all seasons")
    # Example Usage
    task_graph.add_task(
        task_name='Fall haiku', 
        agent_name='haiku_agent',
        task_description='Write a haiku about the Fall season',
        )

    task_graph.add_task(
        task_name='Winter haiku', 
        agent_name='haiku_agent',
        task_description='Write a haiku about winter',
        )
    return task_graph

def add_orchestration_to_session_state():
    if 'orchestration' not in st.session_state:
        task_graph = get_template()
        st.session_state.task_graph = task_graph
        # Get all the agents from the agent_definitions folder, in os independent way
        # Current directory plus agent_definitions
        agent_definitions = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'agent_definitions')
        # Get all the files in the agent_definitions folder
        agent_files = glob(os.path.join(agent_definitions, '*.yml'))
        # Get the agent names
        agent_names = [os.path.basename(i).replace('.yml', '') for i in agent_files]
        # Add the agent names to AgentManager
        orchestration_task_desc = f"""
        You are helping the user complete the task: {task_graph.name}.  It has {task_graph.graph.number_of_nodes()}.
        """
        orchestration = Task(
            task_name='orchestration', 
            task_description=orchestration_task_desc,
            agent_name='agent_manager',
            available_agents=agent_names,
            )
        st.session_state.orchestration = orchestration
        st.markdown(task_graph.generate_markdown())
        st.session_state.current_task = 'orchestration'
    return None

def manage_llm_interaction_old():
    """
    This function sends messages to the LLM and gets a response.
    Based on the response, it could create additional specialized agents
    to handle the request.  It also routes the response to the appropriate agent.

    Parameters:
    - agent_manager: The agent manager object.

    Returns (str):
    - The response from the LLM.
    """
    # FETC

    current_task = agent_manager.current_task
    # Get messages for the LLM
    system_instruction = agent_manager.get_system_instruction(agent_manager.current_task)
    # st.success(f"System instruction: {system_instruction}")
    prompt = None
    if agent_manager.current_task == 'orchestration':
        agent = agent_manager
    else:
        task = agent_manager.get_task(agent_manager.current_task)
        agent = task.agent
        task_name = task.task_name
        status = task.status
        if status == 'just_started':
            prompt = task.prompt
            agent_manager.set_task_status(task_name, 'current_task')
            prompt = task.prompt
        else:
            prompt = None

    messages = agent.get_messages_with_instruction(system_instruction, prompt=prompt)
    st.session_state.last_request = messages

    # Get the response from the LLM
    with st.spinner("Getting response from LLM..."):      
        res = get_llm_output(messages, model=agent.default_model)
        st.info(f"LLM output: {res}")
    return res

def manage_llm_interaction():

    orchestration = st.session_state.orchestration
    tg = st.session_state.task_graph
    messages = tg.messages.get_messages_for_task('orchestration')
    system_instruction = orchestration.get_system_instruction()
    messages = tg.messages.get_messages_with_instruction(system_instruction)
    res = get_llm_output(messages, model=orchestration.default_model)
    return res


def populate_res_dict(res_dict, res):
    """
    If res dict is empty, use the string response to populate it.
    """
    if not res_dict:
        res_dict = {
            'output': res, 
            'task_finished': False,
            'ask_human': False,
            'ask_llm': True,
            }
    
    # If there is a final response, set the task_finished flag to true
    if 'final response' in res.lower():
        res_dict['task_finished'] = True
        res_dict['ask_llm'] = False
        res_dict['ask_human'] = False

    return res_dict

def manage_task(agent_manager, res_dict, res):
    
    # Convert the response to a dict even if it is a string
    res_dict = populate_res_dict(res_dict, res)
    
    # If the response is a request for a new agent, create the agent, if it does not exist
    if 'transfer_to_task' in res_dict:
        agent_name = res_dict.get('agent_name', 'agent_manager')
        task_name = res_dict.get('transfer_to_task', 'orchestration')
        task_description = res_dict.get('task_description', 'No description provided.')
        explain_to_user = res_dict.get('explain_to_user', None)
        # Add explanation as content to the messages
        if explain_to_user:
            agent_manager.set_message(
                role="assistant", 
                content=explain_to_user, 
                task=task_name
                )    

        if task_name != agent_manager.current_task and task_name != 'orchestration':
            st.info(f"Changing agent from {agent_manager.current_task} to {task_name}")
            agent_manager.add_task(
                agent_name=agent_name, 
                task_name=task_name, 
                task_description=task_description
                )
            # Add the message to the new task
            agent_manager.set_message(
                role="assistant", 
                content=task_description, 
                task=task_name,
                )

    if res_dict.get('task_finished', False):
        completed_task = agent_manager.current_task
        agent_manager.complete_task(completed_task)
        agent_manager.current_task = 'orchestration'
        st.session_state.ask_llm = False
    elif res_dict.get('ask_human', False):
        # Set the message to the worker agent via the agent manager
        st.session_state.ask_llm = False
    else:
        # If we are not sure, ask LLM.
        st.session_state.ask_llm = res_dict.get('ask_llm', True)
      

    # Add the response to the current task
    content = res_dict.get('output', str(res_dict))
    if content:
        agent_manager.set_message(
            role="assistant", 
            content=content, 
            task=agent_manager.current_task
            )

    # If ask_human is true in the response, set the ask_llm to false
    # If the current task is still orchestration, check for next task, if any.
    if agent_manager.current_task == 'orchestration':
        # Get the next task
        next_task = agent_manager.get_next_task()
        # If there is a next task, set the current task to the next task
        if next_task is not None:
            st.session_state.ask_llm = True

    return res_dict


def manage_tool_interaction(agent_manager, res_dict):
    """
    If an agent requested to use a tool,
    this function will run the tool and return the result to the agent.

    It will also request a response from the LLM.
    """
    tool_name = res_dict['tool_name']
    tool_module = importlib.import_module(f'tools.{tool_name}')
    tool_function = getattr(tool_module, 'tool_main')

    # Get the tool arguments needed by the tool
    tool_args = inspect.getfullargspec(tool_function).args

    # Arguments for tool will be in res_dict under the key kwargs
    args_for_tool = res_dict.get('kwargs', res_dict)

    # select the required arguments from res_dict and pass them to the tool
    kwargs = {k: v for k, v in args_for_tool.items() if k in tool_args}
    tool_result = tool_function(**kwargs)
    # TODO: Some tools like search need to consume the tool results.
    # Others like navigator need not.  Create a system to pass to the
    # correct task. 
    if isinstance(tool_result, str):
        tool_result = {'content': tool_result}

    # Check if the the task is done and can be transferred to orchestration.
    if tool_result.get('task_finished', True):
        # Add a message from the tool to the agent manager
        content = f"""MESSAGE TO THE AGENT MANAGER.  
        The {agent_manager.current_task} task is done.\n"""

        if tool_result.get('content', None):
            content += f"""Here are some notes from the completed task that you can use to help the user:  
            {tool_result['content']}"""
        # Set the current task to orchestration
        agent_manager.current_task = 'orchestration'
        # Message to agent manager goes as system message
        role = 'system'
    else:
        content = tool_result.get('content', '')
        role = 'user'
    # Set ask_llm status
    st.session_state.ask_llm = tool_result.get('ask_llm', True)
    if content:
        # Add this to the agent's messages
        agent_manager.set_message(
            role=role, 
            content=content,
            task=agent_manager.current_task
            )

    with st.spinner("Let me study the results..."):
        st.sidebar.warning(f"Ask llm: {st.session_state.ask_llm}\n\nCurrent task: {agent_manager.current_task}")
    return None




# TODO: MAKE THIS A CHAT FRAMEWORK CLASS
def chat():

    # Add the agent manager to the session state
    add_orchestration_to_session_state()
    tg = st.session_state.task_graph    
    orchestration = st.session_state.orchestration
     
    # Create the chat input and display
    tg.messages.chat_input_method()    
    messages = tg.messages.get_all_messages()
    
    display_messages(messages, expanded=True)
    st.sidebar.info(f"Ask llm: {st.session_state.ask_llm}\n\nCurrent task: {st.session_state.current_task}")


    # ask_llm took can be set to true by agents or by tools
    # that add to the message queue without human input
    # and request a response from the llm
    if st.session_state.ask_llm:
        # Get the response from the llm
        res = manage_llm_interaction()        
        # Extract the response dictionary
        res_dict = st.session_state.extractor.extract_dict_from_response(res)
        st.write(res_dict)
        st.code(res)
        st.stop()
        res_dict = manage_task(agent_manager, res_dict, res)
        # If a tool is used, ask the llm to respond again
        if 'tool_name' in res_dict:
            manage_tool_interaction(agent_manager, res_dict)
            

        st.rerun()

    return None

--------------------File: task_graph.py
"""
- TODO: Assign task to Dhruv to serialize to JSON.
- Add decision nodes and logic.
- Figure out which task will respond to the user.
- How do we model orchestration (between each transition).
- Tasks can be completed via UI or chat.
- How do we add tasks dynamically during a task (not setting a new node under a completed node).
- Allow users to revisit tasks, undo completion.  (Find next only provides the next task to complete, not the next task to visit.)
- Allow users to navigate to previous steps at any time via the UI or chat.
"""

import os
import time
import networkx as nx
import pickle
import dill as dl
import json
import streamlit as st
from messages import Messages
from task import Task
import json
import textwrap
from matplotlib import pyplot as plt

class TaskGraph:
    def __init__(self, name=None, objective=None):
        self.graph = nx.DiGraph()
        self.graph.add_node('root', sequence=0, completed=False)
        self.name = name
        # Description of the objective
        self.objective = objective

        # Templates with name and instruction
        self.templates = {}
        # An instance of the message class to hold all the 
        # messages for this task graph
        self.messages = Messages(name)
        # If true, this conversation is sent to the planner.
        self.send_to_planner = True
    
    def add_task(self, task_name, task_description, agent_name, parent_task=None, sequence=None, available_agents=[], decision_function=None, **kwargs):
        """
        Adds a node to the graph. The node can be a regular task or a decision node,
        depending on the 'decision' flag and additional keyword arguments.

        Args:
            task_name: The name of the task to add.
            task: Text describing the task to be given to the language model.
            sequence: The sequence number of the node.
            completed: Whether the node is completed.
            decision_function: The function to call if this is a decision node.
            **kwargs: Additional keyword arguments for decision nodes.
        """
        if kwargs.get('before_task') and not parent_task:
            parent_task = self.find_parent(kwargs['before_task'])
        
        if kwargs.get('after_task') and not parent_task:
            parent_task = self.find_parent(kwargs['after_task'])

        if not parent_task:
            parent_task = 'root'
        if not sequence:
            sequence = self.calculate_sequence_number(parent_task)
        node_attributes = {
            'sequence': sequence,
            'completed': False,
            'decision_function': decision_function,
            'task': Task(
                task_name=task_name, 
                task_description=task_description,
                agent_name=agent_name, 
                available_agents=available_agents
                ),
        }

        if decision_function and not kwargs.get('branches', None):
            raise Exception("Decision nodes must have branches.")
        # Add additional attributes for decision nodes
        if kwargs:
            node_attributes.update(kwargs)

        self.graph.add_node(task_name, **node_attributes)
        # Add a parent task. If there's no parent, add a root node.
        self.add_parent(task_name, parent_task)
        # Provide confirmation
        # st.success(f"Added task '{task_name}' to the graph.")
        return None

    def calculate_sequence_number(self, parent_task, before_node=None, after_node=None):
        """
        Calculates a non-conflicting sequence number for a new node based on the specified parameters.
        """
        def find_gap(sequences, start, increment):
            seq = start
            while seq in sequences:
                seq += increment
            return seq

        sibling_sequences = [self.graph.nodes[sib]['sequence'] for sib in self.graph.successors(parent_task)]

        if not sibling_sequences:
            return 1
        elif before_node and before_node in self.graph:
            before_sequence = self.graph.nodes[before_node]['sequence']
            return find_gap(sibling_sequences, before_sequence - 0.1, -0.01)
        elif after_node and after_node in self.graph:
            after_sequence = self.graph.nodes[after_node]['sequence']
            return find_gap(sibling_sequences, after_sequence + 0.1, 0.01)
        else:
            return max(sibling_sequences, default=0) + 1


    def find_parent(self, node_name):
        """
        Finds the parent of the given node. Returns the parent node's name,
        or None if the node has no parent or does not exist.
        """
        if node_name in self.graph:
            for parent, child in self.graph.edges():
                if child == node_name:
                    return parent
        return None

    def _get_successors_and_predecessors(self, node_name):
        """
        Returns the successors and predecessors of the given node,
        and that node itself in a list.
        """
        successors = list(self.graph.successors(node_name))
        predecessors = list(self.graph.predecessors(node_name))
        return [node_name] + successors + predecessors

    def get_messages_for_task(self, task_name):
        """
        Returns the messages for the given task.
        """
        return self.messages.get_messages_for_task(task_name)
    
    def get_messages_for_task_family(self, task_name):
        """
        Returns the messages for the given task and its ancestors and descendants.
        """
        if task_name in self.graph:
            task_family = self._get_successors_and_predecessors(task_name)
            return self.messages.get_messages_for_task_family(task_family)
        else:
            return []


    def add_dependency(self, parent_task, child_task):
        if parent_task in self.graph and child_task in self.graph:
            self.graph.add_edge(parent_task, child_task)
        else:
            st.warning("Both tasks must exist in the graph to add a dependency.")
    
    def add_parent(self, child_task, parent_task=None):
        if child_task in self.graph:
            if parent_task:
                self.graph.add_edge(parent_task, child_task)
            else:
                self.graph.add_edge('root', child_task)
        else:
            st.warning("The child task must exist in the graph to add a parent.")
        return None

    def update_task(self, task_name, task_description=None, sequence=None, task_finished=None, completed=None, other_attributes={}):
        """
        This updates the task with the given name.  It can update the sequence, task_finished, task_description, and other attributes.
        the current code uses completed and task_finished interchangeably.  We should standardize on one.
        Until then, this function allows both keywords.
        
        """
        if task_name in self.graph:
            if sequence is not None:
                self.graph.nodes[task_name]['sequence'] = sequence
            if task_finished is not None:
                self.graph.nodes[task_name]['completed'] = task_finished
            if completed is not None:
                self.graph.nodes[task_name]['completed'] = completed


            if task_description:
                self.graph.nodes[task_name]['task'].task_description = task_description
                self.graph.nodes[task_name]['task_description'] = task_description
            for key in other_attributes:
                self.graph.nodes[task_name][key] = other_attributes[key]
        else:
            st.error(f"Unable to update {task_name}. Task not found in the graph.")
            time.sleep(3)

    # TODO: Document why we need this and get_next_task.
    def _find_next_incomplete_task(self, node):
        """
        Helper method to recursively find the next task to complete.
        """
        # If the current node is not completed, check its children
        if not self.graph.nodes[node]['completed']:
            children = sorted(self.graph.neighbors(node), key=lambda x: self.graph.nodes[x]['sequence'])
            for child in children:
                next_task = self._find_next_incomplete_task(child)
                # If a child or its descendant is incomplete, return it
                if next_task:
                    # Return the first incomplete child that is found
                    return next_task
            # If all children are complete, return the current node
            return node
        # If the current node is completed, return None
        return None

    def _find_next_incomplete_task(self, node):
        if not self.graph.nodes[node]['completed']:
            if self.graph.nodes[node].get('decision', None):
                condition = self.graph.nodes[node]['condition']
                outcome = condition()
                next_branch = self.graph.nodes[node]['branches'][outcome]
                return self._find_next_incomplete_task(next_branch)
            else:
                for neighbor in sorted(self.graph.neighbors(node), key=lambda x: self.graph.nodes[x]['sequence']):
                    next_task = self._find_next_incomplete_task(neighbor)
                    if next_task:
                        return next_task
                return node
        return None

    def _get_task_sequence(self):
        """
        Get the sequence of the tasks in the graph
        starting with the root.
        """
        sequence = []
        for node in nx.topological_sort(self.graph):
            sequence.append(node)
        return sequence


    def update_dependent_tasks(self, task_name):
        """
        Sets all the following tasks as incomplete.
        """
        sequence = self._get_task_sequence()
        for node in sequence:
            if node == task_name:
                continue
            self.graph.nodes[node]['completed'] = False
        return None


    def _find_next_incomplete_child(self, current_node=None):
        """
        This checks if the current node is complete. If it is, it checks its children.
        It returns the first incomplete child that it finds, or None if all children are complete.
        Uses a depth-first search.

        Args:
            current_node: The node to start the search from. If not given, start from the root.

        Returns:
            next_node: The next node to complete or None
        """
        if not current_node:
            current_node = 'root'
        
        graph = self.graph
        # Check if current node task is incomplete and is not root
        if not graph.nodes[current_node]['completed'] and current_node != 'root':
            return current_node
        
        # Sort children by sequence
        children = list(graph.successors(current_node))
        children.sort(key=lambda x: graph.nodes[x]['sequence'])
        
        # Check each child recursively
        for child in children:
            next_task = self._find_next_incomplete_child(child)
            if next_task:
                return next_task
        return None

    def get_next_task(self, start_node=None):
        """
        Public method to find the next task to complete. 
        If start_node is given, start from there, else start from the root.

        Args:
            start_node: The node from which to start the search. If not given, start from the root.

        Returns:
            next_node: The next node to complete or None
        """
        if not start_node:
            start_node = 'root'

        if start_node and start_node in self.graph:
            # next_task = self._find_next_incomplete_task(start_node)
            next_task = self._find_next_incomplete_child(start_node)
            if next_task:
                return next_task
            else:
                
                return None

        st.warning("All tasks are completed or no tasks are available.")
        return None

    def get_next_task_node(self, start_node=None):
        """
        Public method to find the next task to complete. 
        If start_node is given, start from there, else start from the root.

        Args:
            start_node: The node from which to start the search. If not given, start from the root.

        Returns:
            next_node: The next node to complete or None
        """
        # next_task = self.get_next_task(start_node)
        next_task = self._find_next_incomplete_child(start_node)
        if next_task:
            return self.graph.nodes[next_task]
        else:
            return None
        

    def convert_dicts_to_graph(self, graph):
        """
        Converts the task dictionaries in the graph to Task instances.
        Then converts the graph to a DiGraph.
        """
        for i,g in enumerate(graph['nodes']):
            if 'task' in g:
                task_dict = g['task']
                if 'description' in task_dict:
                    task_dict['agent_description'] = task_dict['description']
                if 'name' in task_dict:
                    task_dict['agent_name'] = task_dict['name']
                agent_name = task_dict.pop('agent_name')
                task_description = task_dict.pop('task_description')
                init_keys = "task_name, available_agents, tools".split(",")
                non_init_keys = [k for k in task_dict if k not in init_keys]
                init_dict = {k:task_dict[k] for k in init_keys if k in task_dict}
                non_init_dict = {k:task_dict[k] for k in non_init_keys}
                task = Task(task_description=task_description, agent_name=agent_name, **init_dict)
                for key in non_init_dict:
                    value = non_init_dict[key]
                    setattr(task, key, value)
                graph['nodes'][i]['task'] = task

        # Convert the graph to a DiGraph
        new_graph = nx.node_link_graph(graph)        

        # Return the new DiGraph
        return new_graph

    def convert_graph_to_dict(self):
        """
        Converts the graph to a dictionary, making sure that the Task instances
        are converted to dictionaries, so that the graph can be serialized to JSON.
        """
        G = self.graph
        graph = nx.node_link_data(G)
        
        for i,g in enumerate(graph['nodes']):
            if 'task' in g:
                graph['nodes'][i]['task'] = g['task'].__dict__

        return graph

    def _get_file_path(self):
        """
        Returns the file path of the graph.
        """
        path = os.path.join(st.session_state.user_folder, 'objectives', f'{self.name}.dl')
        directory = os.path.dirname(path)
        if not os.path.exists(directory):
            os.makedirs(directory)

        return path
    
    def _get_file_path_json(self):
        """
        Returns the file path of the graph.
        """
        path = os.path.join(st.session_state.user_folder, 'objectives', f'{self.name}.json')
        directory = os.path.dirname(path)
        if not os.path.exists(directory):
            os.makedirs(directory)

        return path
    
    def _save_to_json(self):
        """
        Saves the graph to a json file.
        """
        # Can save only if the graph has a name
        if not self.name:
            st.warning("The graph must have a name to save.  Let's try again once we start the task.")
            
        else:
            file_path = self._get_file_path_json()
            graph_dict = self.convert_graph_to_dict()

            messages_dict = self.messages.export_all_messages()
            attributes = {
                'name': self.name,
                'objective': self.objective,
                'graph': graph_dict,
                'messages': messages_dict,
                'templates': self.templates,
                'send_to_planner': self.send_to_planner
            }

            with open(file_path, 'w') as file:
                # Dump the data to the file
                json.dump(attributes, file)
                
        return None
    
    def _save_to_file(self):
        """
        Saves the graph to a file.
        """
        # Can save only if the graph has a name
        if not self.name:
            st.warning("The graph must have a name to save.  Let's try again once we start the task.")
            
        else:
            file_path = self._get_file_path()
            # Save with dill to preserve functions
            with open(file_path, 'wb') as file:
                # Get all the attributes as a dict
                attributes = self.__dict__
                # Save it as dill
                dl.dump(attributes, file)
                
        return None

    def _load_from_json(self, file=None):
        """
        Loads a graph from a json file.
        """
        
        # Get a list of files in the objectives folder
        path = os.path.join(st.session_state.user_folder, 'objectives')
        # check if the folder exists and create it if it doesn't
        if not os.path.exists(path):
            os.makedirs(path)
        files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]
        # Sort the files from most recent to oldest
        files.sort(key=lambda x: os.path.getmtime(os.path.join(path, x)), reverse=True)
        
        # Filter out files that don't end with .json
        files = [f.replace('.json', '') for f in files if f.endswith('.json')]
        if files:
            st.header("Load task graph")
            # Add a blank option to the list
            files.insert(0, 'SELECT')
            # Display a selectbox to choose the file
            file_name = st.selectbox("Select conversation to load", files)
            if file_name == 'SELECT':
                st.info("Please select a file to load.")
                return None
            else:
                file_name = os.path.join(path, file_name) + '.json'
                if st.button("Load"):
                    # Load with json
                    self.json_to_graph(file_name)
                    st.success(f"Loaded graph from '{file_name}'.")
                    st.rerun()
        # If there is no file, inform the user.
        else:
            st.warning("There is no previous work to load.  Please create a new task.")

        return None
    
    def json_to_graph(self, file_name):
        with open(file_name, 'r') as file:
            attributes = json.load(file)
        # Assign the attributes to the current graph
        for key, value in attributes.items():
            if key == 'graph':
                # Convert the dictionaries in the graph to Task instances
                value = self.convert_dicts_to_graph(value)
            elif key == 'messages':
                # Create a new Messages instance without specifying a task name
                messages = Messages(self.name)

                # Iterate over the list of dictionaries
                dicts = value
                for dict_obj in dicts:
                    # Create a message_tuple instance
                    message = messages.message_tuple(**dict_obj)
                    # Append the message_tuple instance to the messages attribute
                    messages.messages.append(message)
                
                # Assign the messages attribute to the current graph
                value = messages
            setattr(self, key, value)
        
        return None

    # Deprecated
    def _load_from_file(self):
        """
        Loads a graph from a file.
        """
        # Get a list of files in the objectives folder
        path = os.path.join(st.session_state.user_folder, 'objectives')
        # check if the folder exists and create it if it doesn't
        if not os.path.exists(path):
            os.makedirs(path)
        files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]
        if files:
            st.header("Load task graph")
            # Add a blank option to the list
            files.insert(0, 'SELECT')
            # Display a selectbox to choose the file
            file_name = st.selectbox("Please select file to load", files)
            if file_name == 'SELECT':
                st.warning("Please select a file to load.")
                return None
            else:
                file_path = os.path.join(path, file_name)
                if st.button("Load"):
                    # Load with dill to preserve functions
                    with open(file_path, 'rb') as file:
                        attributes = dl.load(file)
                    # Assign the attributes to the current graph
                    self.__dict__.update(attributes)
                    st.success(f"Loaded graph from '{file_name}'.")
                    st.rerun()
        return None

    def rename(self, new_name):
        """
        Renames the graph and the file, if it exists.
        """
        if new_name:
            # Rename the file and graph
            old_file_path = self._get_file_path()
            self.name = new_name
            new_file_path = self._get_file_path()
            if os.path.exists(old_file_path):
                os.rename(old_file_path, new_file_path)
            st.success(f"Renamed graph to '{new_name}'.")
            time.sleep(2)

        return None

    def generate_markdown(self):
        """
        Generates a markdown text from the task graph with hierarchical tasks.
        """
        md_text = f'# List of tasks and subtasks\n\nProject Name: {self.name}\nObjective: {self.objective}\n\n'
        def format_task(node_name, level=0):
            md_line = ""
            node = self.graph.nodes[node_name]
            if node_name != 'root':
                md_line += f"{'#' * level} Task name: {node_name}\n"
                # md_line += f"System instruction: {node['task'].system_instruction.strip()}\n"
                md_line += f"Task description: {node['task'].task_description.strip()}\n"
                output = node.get('content')
                if output:
                    md_line += f"Output: {output}\n"
                # Get task status
                task_status = node.get('completed', False)
                md_line += f"Task status: {'Completed' if task_status else 'Not completed'}\n"
                md_line += "\n"
            for child in sorted(self.graph.successors(node_name), key=lambda x: self.graph.nodes[x]['sequence']):
                md_line += format_task(child, level + 1)
            return md_line

        for node_name in [n for n, d in self.graph.in_degree() if d == 0]:  # Root nodes
            md_text += format_task(node_name)
        return md_text

    def get_attributes_of_ancestors(self, current_task):
        """
        Returns the attributes of the ancestors of the current task
        as a flattened dictionary.
        """
        ancestors = list(nx.ancestors(self.graph, current_task))
        if current_task not in ancestors:
            ancestors.append(current_task)
        attributes = {}
        for ancestor in ancestors:
            attributes.update(self.graph.nodes[ancestor])
        return attributes
    
    
def draw_graph(G, wrap_width=10):
    """
    Draws a family tree from a NetworkX graph with wrapped labels.

    Parameters:
    G (networkx.Graph): A NetworkX graph object representing a family tree.
    wrap_width (int): The maximum line width for wrapped labels.
    """
    # Increase figure size
    plt.figure(figsize=(8, 4))

    # Position nodes using a hierarchical layout
    pos = nx.spring_layout(G, iterations=100)

    # Draw the graph
    nx.draw(G, pos, with_labels=False, node_color='lightblue', node_size=3000, edge_color='gray', linewidths=1)

    # Draw labels with wrapped text
    for node, (x, y) in pos.items():
        label = node.replace('_', ' ')  # Escape underscores
        wrapped_label = textwrap.fill(label, width=wrap_width)
        plt.text(x, y, wrapped_label, fontsize=10, ha='center', va='center', multialignment='center')

    # Adjust layout to avoid clipping of labels
    plt.tight_layout()

    # Show the plot on streamlit
    st.pyplot(plt.gcf())


--------------------File: temp.py

class Vehicle:
    messages = []
    codeword = "secret"
    def __init__(self, make, model, year, color):
        self.make = make
        self.model = model
        self.year = year
        self.color = color

    def print_details(self):
        print(f"Make: {self.make}")
        print(f"Model: {self.model}")
        print(f"Year: {self.year}")
        print(f"Color: {self.color}")
        print(f"Code word: {self.codeword}")

class Car(Vehicle):
    def __init__(self, make, model, year, color):
        super().__init__(make, model, year, color)

class Truck(Vehicle):
    def __init__(self, make, model, year, color, num_wheels):
        super().__init__(make, model, year, color)
        self.num_wheels = num_wheels

    def print_details(self):
        super().print_details()
        print(f"Number of wheels: {self.num_wheels}")

askhok_leyland = Truck("Ashok Leyland", "Dost", 2018, "White", 4)
askhok_leyland.print_details()

maruti_800 = Car("Maruti", "800", 2000, "Red")
maruti_800.print_details()
--------------------File: main.py
import sys
import simple_auth
import os
dir_path = os.path.dirname(os.path.realpath(__file__))
sys.path.append(dir_path)

import streamlit as st
import yaml

# Make it full width
st.set_page_config(layout="wide", page_title='TB Chat Framework')
token = simple_auth.simple_auth()
st.session_state.token = token

from helpers import starter_code
# Starter code has to run early.  Do not move.
starter_code()

# from chat import chat
# from chat_with_planning import chat
from simple_cf import chat
from graphical_menu import GraphicalMenu
from tb_settings import settings_main, llm_access_settings
from function_management import run_current_functions

if 'menu' not in st.session_state:
    menu = GraphicalMenu()
    st.session_state['menu'] = menu
    settings_main()
    st.rerun()
else:
    menu = st.session_state['menu']
    settings_main()

# Show tool_result in session state, if it exists
if 'tool_result' in st.session_state:
    st.header("Tool Result")
    st.write(st.session_state.tool_result)

# If latest request or response are there, show checkbox to show them
please_stop = False
with st.sidebar.expander("Admin"):
    st.checkbox("Show developer options", key='developer_options')
    # Show task graph if it exists
    if 'task_graph' in st.session_state:
        show_task_graph = st.checkbox('Show task graph')
        if show_task_graph:
            from task_graph import draw_graph
            tg = st.session_state.task_graph
            draw_graph(G=tg.graph)
    if 'last_request' in st.session_state:
        show_request = st.checkbox('Show latest request')
        if show_request:
            please_stop = True
            st.write(st.session_state.last_request)
        show_all_messages = st.checkbox('Show all messages')
        if show_all_messages:
            please_stop = True
            st.write(st.session_state.all_messages)
    if 'last_response' in st.session_state:
        show_response = st.checkbox('Show latest response')
        if show_response:
            please_stop = True
            res = st.session_state.last_response
            st.warning(st.session_state.last_response)
    if 'planner_instructions' in st.session_state:
        show_planner_instructions = st.checkbox('Show planner instructions')
        if show_planner_instructions:
            please_stop = True
            st.write(st.session_state.planner_instructions)
    # If there is a message from the agent, show it (temporary)
    if 'agent_messages' in st.session_state:
        if st.checkbox('Show agent messages'):
            please_stop = True
            st.success(st.session_state.agent_messages)
if please_stop:
    st.stop()

if not 'openai_key' in st.session_state:
    llm_access_settings() # LLM access settings

# menu.add_edges(menu_bar_options) # add the edges to the menu in the GraphicalMenu class
# menu.create_menu() # create the meu bar
# run_current_functions() # run the current functions in the session state
# st.sidebar.warning(st.session_state.selected_node)

chat() # chat interface

# Naviagate through the menu using the chat interface
# TODO: Ranu, even though we are not adding anything to sesssion state
# or doing anything with the return value, we still need to call this
# for some reason.  Without it, chat is not opening menus. 
from tools.navigator import get_available_destinations
nodes = get_available_destinations()


--------------------File: test_llm.py
"""
LLM Access without any streamlit use for testing.
"""
from openai import OpenAI

def get_openai_output(messages, max_tokens=3000, temperature=0.4, model='gpt-4', functions=[]):
    """
    Gets the output from GPT models. default is gpt-4. 

    Args:
    - messages (list): A list of messages in the format                 
                messages =[
                {"role": "system", "content": system_instruction},
                {"role": "user", "content": prompt}],

                system_instruction is the instruction given to the system to generate the response using the prompt.

    - model (str): The model to use.  Default is gpt-4.
    - max_tokens (int): The maximum number of tokens to generate, default 800
    - temperature (float): The temperature for the model. The higher the temperature, the more random the output
    """
    # Enable adding the key here.
    client = OpenAI()
    if '4' in model:
        model = "gpt-4-turbo-preview"
    params = {
        "model": model,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "n": 1,
        "messages": messages
    }
    if functions:
        params['functions'] = functions
    
    # If the word "json" is found in the content, ask for a json object as response
    json_found = False
    for i in messages:
        if "json" in i['content'].lower():
            json_found = True
            break
    if json_found:
        params['response_format'] = {"type": "json_object"}

    response = client.chat.completions.create(**params)
    msg = response.choices[0].message.content
    
    return msg

--------------------File: extractors.py
import streamlit as st
import os
import importlib
import inspect
import re
import json


class Extractors:

    """
    This class contains functions to extract information from the response from the LLM.

    Available functions:
    - remove_indents_in_lines (response)
    - extract_dict_from_response (response)
    - extract_json_from_response (response)
    - extract_list_of_dicts_from_string (response)
    - extract_agent_name_and_message (response)
    - extract_message_to_agent (response)
    - extract_func_call_info (response)
    - extract_code_from_response (response)
    - extract_modified_user_requirements_from_response (response)
    - get_docstring_of_tool (tool, function_name='tool_main')
    - get_docstring_of_tools (tool_list)

    """
    
    def __init__(self):
        return None

    def remove_indents_in_lines(self, response):
        """
        Remove leading and trailing whitespace from each line in the given response.

        Args:
            response (str): The response to process.

        Returns:
            str: The response with leading and trailing whitespace removed from each line.
        """
        lines = response.split('\n')
        lines = [i.strip() for i in lines]
        return '\n'.join(lines)

    def extract_dict_from_response(self, response):
        """
        Extracts a dictionary from the given response.

        Args:
            response (dict or list or str): The response to extract the dictionary from.

        Returns:
            dict: The extracted dictionary.

        """
        if isinstance(response, dict):
            return response
        elif isinstance(response, list):
            return response
        elif isinstance(response, str):
            response = response.strip()
            try:
                start = response.index('{')
                end = response.rindex('}') + 1
                dict_str = response[start:end]
                return json.loads(dict_str)
            except ValueError:
                return {}
        else:
            return {}


    def extract_list_of_dicts_from_string(self, response):
        """
        Extracts a list of dictionaries from a string.

        Args:
            response (str): The string containing the list of dictionaries.

        Returns:
            list: A list of dictionaries extracted from the string.

        """
        # Remove the new line characters
        response = response.replace('\n', ' ')

        # assuming `response` is the string with the list of dicts
        start_index = response.index('[') 
        end_index = response.rindex(']') + 1
        list_of_dicts_str = response[start_index:end_index]

        return eval(list_of_dicts_str)


    def extract_agent_name_and_message(self,response):
        """
        Extracts the agent name, instruction, and the rest of the response from a given response string.

        Args:
            response (str): The response string containing the agent name and instruction.

        Returns:
            tuple: A tuple containing the agent name, instruction, and the rest of the response.

        Example:
            >>> response = "<<<Agent1: Do this instruction>>> Some other text"
            >>> extract_agent_name_and_message(response)
            ('Agent1', 'Do this instruction', ' Some other text')
        """
        pattern = r"<<<([\s\S]*?):([\s\S]*?)>>>"
        matches = re.findall(pattern, response)
        if len(matches) == 1:
            agent_name = matches[0][0].strip()
            instruction = matches[0][1].strip()
            rest_of_response = response.replace(f'<<<{agent_name}:{instruction}>>>', '')
        else:
            agent_name = None
            instruction = None
            rest_of_response = response
        return agent_name, instruction, rest_of_response

    def extract_destination_edge(self, response):
        """
        Extracts the destination edge from the response.

        The destination edge is the edge that the agent manager should traverse to
        after the agent is done with the current edge.

        Args:
            response (str): The response from the LLM.

        Returns:
            str: The destination edge.
        """
        pattern = r"<<<([\s\S]*?)>>>"
        matches = re.findall(pattern, response)
        if len(matches) == 1:
            destination_edge = matches[0].strip()
        else:
            destination_edge = None
        return destination_edge

    def extract_message_to_agent(self, response):
        """
        Extracts the message to the agent from the response.
        This is found within <<< and >>>.
        There will be at least one set of triple angle brackets
        for this function to be invoked.

        Parameters:
        - response (str): The response string from which to extract the message.

        Returns:
        - message_to_agent (str): The extracted message to the agent.
        """
        pattern = r"<<<([\s\S]*?)>>>"
        matches = re.findall(pattern, response)
        if len(matches) == 1:
            message_to_agent = matches[0].strip()
        else:
            message_to_agent = '\n'.join(matches)
        
        # Add it to the session state
        st.session_state.message_to_agent = message_to_agent
        return message_to_agent

    def extract_func_call_info(self, response):
        """
        Extracts code or requirements from the given response.

        The LLM can return code or requirements in the content.  
        Ideally, requirements come in triple pipe delimiters, 
        but sometimes they come in triple backticks.

        Figure out which one it is and return the extracted code or requirements.

        Args:
            response (str): The response containing code or requirements.

        Returns:
            tuple: A tuple containing the extracted code or requirements and the corresponding function name.
        """
        # If there are ```, it could be code or requirements
        function_name = None
        if '```' in response:
            # If it's python code, it should have at least one function in it
            if 'def ' in response:
                extracted = self.extract_code_from_response(response)
                function_name = 'save_code_to_file'
            elif 'data_agent' in response:
                extracted = self.extract_modified_user_requirements_from_response(response)
                function_name = 'data_agent'
            elif 'json' in response:
                extracted = self.extract_json_from_response(response)
                function_name = ''
            # If it's not python code, it's probably requirements
            else:
                extracted = self.extract_modified_user_requirements_from_response(response)
                function_name = 'save_requirements_to_file'
        # If there are |||, it's probably requirements
        elif '|||' in response:
            extracted = self.extract_modified_user_requirements_from_response(response)
            function_name = 'save_requirements_to_file'
        else:
            extracted = None
        return extracted, function_name
                
    def extract_code_from_response(self,response):

        """
        Returns the code from the response from LLM.
        In the prompt to code, we have asked the LLM to return the code inside triple backticks.

        Args:
        - response (str): The response from LLM

        Returns:
        - matches (list): A list of strings with the code

        """

        pattern = r"```python([\s\S]*?)```"
        matches = re.findall(pattern, response)
        if len(matches) > 0:
            matches = '\n'.join(matches)
        else:
            matches = matches[0]
        return matches

    def extract_modified_user_requirements_from_response(self,response):
        
        """
        Returns the modified user requirements from the response from LLM. 
        In the prompt to modify, we have asked the LLM to return the modified user requirements inside triple pipe delimiters.

        Args:
        - response (str): The response from LLM

        Returns:
        - matches (list): A list of strings with the modified user requirements

        """
        if '|||' in response:
            pattern = r"\|\|\|([\s\S]*?)\|\|\|"
        if '```' in response:
            # It shouldnt have ```python in it
            pattern = r"```([\s\S]*?)```"

        matches = re.findall(pattern, response)
        # if there are multiple matches, join by new line
        if len(matches) > 0:
            matches = '\n'.join(matches)
        else:
            matches = matches[0]
        return matches

    
    def get_docstring_of_tool(self, tool, function_name='tool_main'):
        """
        Return the docstring of the file as a dict.

        Parameters:
        - file: The file to retrieve the docstring from.
        - function_name: The name of the function to retrieve the docstring from. Default is 'tool_main'.

        Returns (str):
        - The docstring of the file.
        """
        # Import the module
        module_name = f"tools.{tool}"
        module = importlib.import_module(module_name)
        # Get the function object from the module
        function = getattr(module, function_name, None)
        if function is None:
            return f"No function named '{function_name}' found in 'tools.{tool}'."

        # Get the docstring
        docstring = inspect.getdoc(function)
        return docstring if docstring else "No docstring available."

    def get_docstring_of_tools(self,tool_list):
        """
        Given the list of tools, get their docstrings.

        Parameters:
        - tool_list: A list of tools to get the docstrings of.  This should be the name of the python file in the "tools" directory.

        Returns (dict):
        - A dictionary of the tool name and the docstring of the tool.
        """
        tools = {}
        for tool in tool_list:
            file = os.path.join(os.path.dirname(__file__), 'tools', f"{tool}.py")
            tools[tool] = self.get_docstring_of_tool(tool)
        return tools

    def extract_json_from_text(self, text):
        # Regular expression to find JSON-like structures, possibly enclosed in triple backticks
        # This pattern looks for an optional triple backtick, followed by optional non-critical characters (like the word "json"),
        # followed by either a curly brace (indicating a JSON object) or a square bracket (indicating a JSON array),
        # and then continues until it finds a closing brace or bracket, potentially followed by triple backticks.
        json_patterns = [
            r"```.*?({.*?})```",  # Pattern for JSON objects within triple backticks
            r"```.*?(\[.*?\])```",  # Pattern for JSON arrays within triple backticks
            r"({.*})",  # Pattern for JSON objects not within backticks
            r"(\[.*\])"  # Pattern for JSON arrays not within backticks
        ]

        extracted_jsons = []

        for pattern in json_patterns:
            potential_jsons = re.findall(pattern, text, re.DOTALL)
            for json_str in potential_jsons:
                try:
                    # Attempt to parse the string as JSON
                    parsed_json = json.loads(json_str)
                    extracted_jsons.append(parsed_json)
                except json.JSONDecodeError:
                    # If parsing fails, it's not valid JSON; ignore it
                    continue

        return extracted_jsons


--------------------File: helpers.py
import tempfile
import streamlit as st
import os
from llm_access import LLMConfigurator
import session_state_management
import toml
from simple_auth import logout
import openai
import time
import os
from glob import glob
import pandas as pd
from extractors import Extractors
from streamlit_ace import st_ace


def update_text_file(file, value):
    """
    On Change, this saves the current labels to a file called labels.md in the root folder.
    """
    
    with open(file, 'w') as f:
        f.write(value)
    return None


def text_areas(file, key, widget_label):
    """
    We have stored text that the user can edit.  
    Given a file, load the text from the file and display it in a text area.
    On change, save the text to the file.
    """
    # Get the directory and create it if it does not exist
    directory = os.path.dirname(file)
    if not os.path.exists(directory):
        os.makedirs(directory)
    # Load current value from file.  If not create empty file
    if not os.path.exists(file):
        with open(file, 'w') as f:
            f.write('')
    with open(file, 'r') as f:
        value = f.read()
    
    out = st_ace(
        value=value,
        placeholder="Write your requirements here...",
        language="markdown",
        theme="github",
        show_gutter=False,
        font_size=14,
        wrap=True,
        keybinding="vscode",
        key=f"{key}_{st.session_state.project_folder}",
        )
    
    if out != value:
        update_text_file(file, value=out)
    # Add user requirements to session state
    st.session_state.user_requirements = out
    return out

def extract_python_code(text):
    """
    Extracts Python code snippets from within triple backticks in the given text.
    
    Parameters:
    -----------
    text: str
        The text to search for Python code snippets.
    
    Returns:
    --------
    A list of Python code snippets (strings).
    """
    snippets = []
    in_code_block = False
    for line in text.split('\n'):
        if line.strip() == '```python':
            in_code_block = True
            code = ''
        elif line.strip() == '```' and in_code_block:
            in_code_block = False
            snippets.append(code)
        elif in_code_block:
            code += line + '\n'
    
    # Evaluate code snippets and return python objects
    correct_code = []
    for i, snippet in enumerate(snippets):
        try:
            correct_code.append(eval(snippet))
        except:
            pass
    return correct_code

def get_approved_libraries():
    return "import streamlit as st\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom glob import glob\nimport datetime\nimport faker\nfrom faker import Faker, import faker-commerce"

#--------------CODE TO RUN AT THE START OF THE APP----------------
def set_function_calling_availability(toggle=False):
    """
    This sets the function calling availability to the session state.
    If function_call is not in session state, it looks at secrets.

    Args:
    - toggle: bool, whether to toggle the function calling availability.
    """

    # Get the secrets file path
    secrets_file_path = st.session_state.secrets_file_path

    # Look at the secrets if function_call is not in session state
    if 'function_call' not in st.session_state:
        if not os.path.exists(secrets_file_path):
            st.session_state.function_call = False
        else:
            with open(secrets_file_path, 'r') as f:
                config = toml.load(f)
            if config != {}:
                if 'function_call_availabilty' in config:
                    st.session_state.function_call = config['function_call_availabilty']
                else:
                    st.session_state.function_call = False
            else:
                st.session_state.function_call = False

    # Toggle the function calling availability
    if toggle:
        st.session_state.function_call = not st.session_state.function_call

    return None

def search_placeholder():
    """
    This function is a placeholder for the search function. On the menu bar, when the user clicks on the search icon, this function is called and do nothing.
    
    Returns:
        None: If no placeholder is found.
    """
    return None

def google_search_interface_for_menu():
    """
    This function calls the `google_search_interface` function from the `tools.google_search` module.
    It is used to provide a menu interface for performing Google searches.
    """
    from tools.google_search import GoogleSearcher
    google_tool = GoogleSearcher()
    google_tool.google_search_interface()
    return None

def youtube_search_interface_for_menu():
    """
    This function serves as an interface for searching YouTube transcripts.
    It imports the 'youtube_transcript_search' module and calls its 'main' function.
    """
    from tools.youtube_transcript_search import YoutubeSearcher
    youtube_tool = YoutubeSearcher()
    youtube_tool.youtube_search_interface()

    return None

def data_management_interface():
    """
    This function serves as an interface for managing data.
    It imports the 'data_management' module and calls its 'main' function.
    """
    from data_management.uploader import DataUploader
    from data_management.deleter import DataDeleter
    from data_management.selector import DataSelector
    from data_management.modeler import DataModeler

    if 'DataUploader' not in st.session_state:
        data_uploader = DataUploader()
        st.session_state.DataUploader = data_uploader
    else:
        data_uploader = st.session_state.DataUploader

    if st.session_state.selected_node == 'Upload Data~helpers':
        data_uploader.tabular_data()
    elif st.session_state.selected_node == 'Upload Documents~helpers':
        data_uploader.upload_document_files()
    
    if 'DataDeleter' not in st.session_state:
        data_deleter = DataDeleter()
        st.session_state.DataDeleter = data_deleter

    data_deleter = st.session_state.DataDeleter

    if st.session_state.selected_node == 'Delete Data~helpers':
        data_deleter.delete_files()

    if 'DataSelector' not in st.session_state:
        data_selector = DataSelector()
        st.session_state.DataSelector = data_selector
    
    data_selector = st.session_state.DataSelector
    
    if st.session_state.selected_node == 'Select Data~helpers':
        file_name, input_column = data_selector.interface()
        st.error(f"Selected file: {file_name}")
        st.error(f"Selected column: {input_column}")

    if 'DataModeler' not in st.session_state:
        data_modeler = DataModeler()
        st.session_state.DataModeler = data_modeler

    data_modeler = st.session_state.DataModeler

    if st.session_state.selected_node == 'Data Model~helpers':
        data_modeler.interface()
    return None

def llm_research_interface():

    from project_management.research_management import LLMResearch

    if 'LLMResearch' not in st.session_state:
        llm_research = LLMResearch()
        st.session_state.LLMResearch = llm_research

    llm_research = st.session_state.LLMResearch

    llm_research.research_with_llm()

    return None

def arxiv_search_interface_for_menu():
    """
    This function serves as an interface for searching Arxiv.
    It imports the 'arxiv_search' module and calls its 'main' function.
    """
    from tools.arxiv_search import ArxivSearch
    arxiv_tool = ArxivSearch()
    arxiv_tool.arxiv_interface()
    return None

def create_user_folder():
    """
    Creates a user folder in the .typebuild folder.
    """
    # Get the user folder from the session state
    if 'user_folder' not in st.session_state:
        home_dir = os.path.expanduser("~")
        st.session_state.user_folder = os.path.join(home_dir, ".typebuild", 'users' ,st.session_state.token)
    
    return None


def starter_code():
    """
    This function is responsible for running the necessary code at the top of the app.
    
    Steps:
    1. Add all default session states using the `session_state_management.main()` function.
    2. Set or get the LLM keys using the `set_or_get_llm_keys()` function.
    3. Instantiate the `Extractors` class and add it to the session state if it doesn't already exist.
    4. If the selected node in the session state is 'logout', call the `logout()` function.
    5. Set the availability of function calling using the `set_function_calling_availability()` function.
    6. If 'upgrade' is not in the session state, call the `temp_upgrade()` function and set `st.session_state.upgrade` to True.
    
    Returns:
    None
    """
    # Add all default session states
    session_state_management.main()
    if 'llm_configurator' not in st.session_state:
        st.session_state.llm_configurator = LLMConfigurator()
    lc = st.session_state.llm_configurator
    lc.set_or_get_llm_keys()
    
    # Instantiate the Extractors class and add it to the session state
    if 'extractor' not in st.session_state:
        st.session_state.extractor = Extractors()
    
    if st.session_state.selected_node == 'logout':
        logout()
    # create_user_folder()
    set_function_calling_availability()
    if 'upgrade' not in st.session_state:
        temp_upgrade()
        st.session_state.upgrade = True
    return None
    
def temp_upgrade():
    """
    Use this for upgrades
    """
    user_folder = st.session_state.user_folder
    if user_folder.endswith(os.path.sep):
        user_folder = user_folder[:-1]

    projects = glob(os.path.join(user_folder, '**', ''))
     
    for project in projects:
        project_research_file = os.path.join(project, 'research_projects_with_llm.parquet')
        if not os.path.exists(project_research_file):
            create_research_data_file(project)
        else:
            df = pd.read_parquet(project_research_file)
            if 'research_name' not in df.columns:
                create_research_data_file(project)
        

    return None

def x(arr):
    file, col = arr
    file = os.path.basename(file).replace('.parquet', '').replace('_', ' ').title()
    col = col.replace('llm_', '').replace('_', ' ').upper()
    return f"{col} {file}"

def sys_ins_get(row):
    col, file = row
    file = os.path.basename(file).replace('.parquet', '')
    project = os.path.join(*file.split(os.path.sep)[1:-1])
    sys_ins = f"{project}{file}_{col}_sys_ins.txt"
    
    if os.path.exists(sys_ins):
        with open(sys_ins, 'r') as f:
            text = f.read()
    else:
        text = ''
    
    return text

def sys_ins_get(row):
    col, file = row
    file_name = os.path.basename(file)
    file_name_without_extension = os.path.splitext(file_name)[0]
    project_path = os.path.dirname(file_name_without_extension)
    sys_ins = os.path.normpath(os.path.join(project_path, f"{file_name_without_extension}_{col}_sys_ins.txt"))

    if os.path.exists(sys_ins):
        with open(sys_ins, 'r') as f:
            text = f.read()
    else:
        text = ''
    return text


def create_research_data_file(project):
    """
    Create a data model if it does not exist.  Temp upgrade from 0.0.22 to 0.0.23
    """

    data_model = os.path.join(project, 'data_model.parquet')
    
    st.sidebar.warning(f"No data model for {project}")
    if not os.path.exists(data_model):
        return None
    df = pd.read_parquet(data_model)
    
    res_projects = df[df.column_name.str.contains('llm')][['column_name', 'file_name']]
        
    if len(res_projects) == 0:
        res_projects = pd.DataFrame(columns=['research_name', 'project_name', 'file_name', 'input_col', 'output_col', 'word_limit', 'row_by_row', 'system_instruction'])
    else:
        res_projects = res_projects.rename(columns={'column_name': 'output_col'})
        
        res_projects['input_col'] = 'SELECT'
        
        res_projects['research_name'] = res_projects[['file_name', 'output_col']].apply(x, axis=1)

        res_projects['project_name'] = project
        st.sidebar.warning(project)
        res_projects = res_projects.reset_index(drop=True)
        
        res_projects['word_limit'] = 1000

        res_projects['system_instruction'] = ''
        
        res_projects.loc[res_projects.input_col.str.contains('conso'), 'row_by_row'] = True
        
        res_projects.row_by_row = res_projects.row_by_row.fillna(False)
        
        res_projects = res_projects[['project_name', 'research_name', 'file_name', 'input_col', 'output_col', 'word_limit', 'row_by_row', 'system_instruction']]
    
        res_projects['system_instruction'] = res_projects[['output_col', 'file_name']].apply(sys_ins_get, axis=1)
    
    project_research_file = f"{project}research_projects_with_llm.parquet"    
    res_projects.to_parquet(project_research_file, index=False)
    
    return None
--------------------File: master_planner.py
"""
This file creates the script for the master planner
with different instructions at different stages of the process.
"""
import streamlit as st
import os
from glob import glob
from task import Task
from extractors import Extractors


# Instructions to create the task graph
instruction_before_task_graph = """Your first job is to create a task graph to help the user with their project.
    - First talk to the user to understand their objective.  
    - Once you understand, create one or more tasks to help the user achieve their objective.  
    - Most agents can do multiple steps in one task.  Understand their description and create more than one task only if necessary.
    - Then create a task graph to help the user achieve their objective.
    - If the user wants the small business agent
To create a task graph, you should create a valid json in this format:
{
"type": "task_graph",
"name": "a detailed name for the task graph that the user can retrieve later.  Use all key details in name.",
"objective": "A detailed description of the user's objective that will be used by other agents in creating tasks.",
}
    """

create_task_general_instruction = """
Unless the user asks for it, keep the tasks minimal.
The output should be a list of valid json with an array of dicts.
The dicts should have the task_name, task_description, and agent_name.  
parent_task, add_before, and add_after are optional.  You can omit them or set them to null.
Upon creating the task list, set ask_llm to true if you need to ask the user for more information.
If the task is finished, set task_finished to true.  Else, set to false.

Use this format:
{
"task_list": [array of task dicts],
"ask_llm": bool,
"task_finished": bool
}
task dicts should be in this format:
{
"task_name": "name of the task",
"task_description": "Instruction to the agent on what it should do.",
"agent_name": "name of the agent that will perform the task",
"parent_task": "Set a parent if this task depends on the completion of another task, and needs its input.  Else, set to null",
"add_before": "name of the task that this task should be added before.  If this is the first task, set this to null",
"add_after": "name of the task that this task should be added after.  If this is the last task, set this to null",
} 

  Some tasks need data.  If a file and column are already selected, pass that information. Else work with the data_agent to select the data.
"""

# Instructions to create tasks after the task graph is created but any tasks has been created
instruction_first_tasks = """The task graph contains no task yet. You should create tasks to help the user achieve their objective.
  First think of all the tasks needed to for this project.
  Then generate a list of tasks to help the user achieve their objective. The list should be in the order in which tasks should be performed."""

instruction_first_tasks += create_task_general_instruction

# Instructions to add tasks
instruction_additional_tasks = """The task graph already contains tasks given above.
Based on the conversation, if you assess that more tasks are needed, add them to the task graph."""

instruction_additional_tasks += create_task_general_instruction


instructions_to_update_tasks = """Sometimes you have to update a task by giving it additional instructions or by modifying the task description.
In that case:
- Update the task description clearly.
- Set the task_finished to false, so that it is run again
- If other tasks depended on this, you have to set all the following tasks to incomplete as well.

Use this format and return a valid json:
{
"update_task": 
    {
    "task_name": "name of the task to update, verbatim",
    "task_description": "new instructions for the task",
    "task_finished": bool,
    "update_dependent_tasks": bool
    }
}
"""

# Instruction to be added to agents on getting back to planning.
back_to_planning_instructions = """As you work, if you do not have required details, or if the user asked you about a task that you 
are not working on, send a message to the master planner, who will help you with the next steps.  The message should have enough
details for the planner to understand the situation and help you with the next steps.  Use the format below to send a message to the planner:

{
"message_to_planner": "Your message to the master planner"
}

"""

def get_task_graph_details():
    """
    Get the name of the task, the description, and the markdown of subtasks
    from the task graph as a string
    """
    tg = st.session_state.task_graph
    task_name = tg.name
    task_objective = tg.objective
    task_md = tg.generate_markdown()
    
    task_info = f"""
    
    HERE IS THE INFORMATION ABOUT THE TASK GRAPH:
    TASK GRAPH NAME: {task_name}
    OBJECTIVE: {task_objective}
    LIST OF SUBTASKS:
    {task_md}
    """
    extractors = Extractors()
    return extractors.remove_indents_in_lines(task_info)

def add_planning_to_session_state():
    """
    Create the master planner and add it to the session state
    """
    # Get all the agents from the agent_definitions folder, in os independent way
    # Current directory plus agent_definitions
    agent_definitions = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'agent_definitions')
    # Get all the files in the agent_definitions folder
    agent_files = glob(os.path.join(agent_definitions, '*.yml'))
    # Get the agent names
    agent_names = [os.path.basename(i).replace('.yml', '') for i in agent_files]
    # TODO: Remove agent manager until we remove it from the definitions
    agent_names.remove('agent_manager')
    
    # Provide agent names to Planning task
    planning = Task(
            task_name='planning', 
            task_description="To be set by update master planner function",
            agent_name='master_planner',
            available_agents=agent_names,
            )

    # The system instruction should change for the task graph
    # based on the situation.  It is added dynamically. 
    planning.system_instruction = instruction_before_task_graph
    st.session_state.planning = planning

    st.session_state.current_task = 'planning'
    return None


def update_master_planner():
    """
    Update the description and instructions for the master planner
    based on the current state of the task graph.
    There are three states:
    1. No task graph exists
    2. Task graph exists but no tasks have been added
    3. Task graph exists and tasks have been added, and more tasks may be added
    """
    planning = st.session_state.planning
    tg = st.session_state.task_graph
    # If there is no task graph name, the task graph has not been created
    if not tg.name:
        desc = "\nWe do not know the user's objective yet and the task graph has not been created.\n"
        planning.task_description = desc
        planning.system_instruction = instruction_before_task_graph
    
    # If the only node in the task graph is the root, no tasks have been added
    # Add tasks to the task graph
    elif tg.graph.number_of_nodes() == 1:
        # Since there is just the root node, there are no tasks
        # The graph details will fetch the task graph name and objective
        desc = get_task_graph_details()
        planning.task_description = desc
        # If any template has been selected, add that
        if st.session_state.task_graph.templates:
            # Update the template information
            template_info = "The user has selected a canned template that can help with planning.  Follow these instructions very carefully to create tasks for the objective:\n"  
            for template_name, template in st.session_state.task_graph.templates.items():
                template_info += f"\nINSTRUCTIONS FOR PLANNING: {template}\n\n"

            planning.task_description = desc + template_info
        planning.system_instruction = instruction_first_tasks

    # Tasks exist in the task graph
    # We should allow for new tasks to be added
    else:
        # Get all the tasks that exist
        desc = get_task_graph_details()
        planning.task_description = desc
        planning.system_instruction = instruction_additional_tasks + "\n\n" + instructions_to_update_tasks
    return None

def create_master_planner():
    """
    Add the planner to the session state, and update the task description and system instruction
    based on the current state of the task graph
    """
    if 'planning' not in st.session_state:
        add_planning_to_session_state()

    # If planning is in session state, update the task description
    else:
        update_master_planner()
    return None

def get_planner_instructions():
    """
    Get the instructions for the master planner
    """
    planning = st.session_state.planning
    system_instruction = planning.get_system_instruction()
    # If there are templates, use it.
    tg = st.session_state.task_graph
    if tg.templates:
        template_info = "The user has selected a canned template that can help with planning.  See the details below."
        for template_name, template in tg.templates.items():
            template_info += f"\n\nTemplate name: {template_name}"
            template_info += f"\nTemplate description: {template}\n\n"
        template_info += "Please use the information above to help with planning."
        system_instruction += f"\n\n{template_info}"
    # Store the system instruction in the session state for us to understand what 
    # information the planner was given
    st.session_state.planner_instructions = system_instruction
    return system_instruction
--------------------File: object_management.py
import streamlit as st

def test():
    # file = "/home/vivek/.typebuild/users/vivek/data/arxiv_all_confabulation_hallucination_language_models.parquet"
    # display = Display(file)
    # check if dynamic_functions exists
    if 'dynamic_functions' in st.session_state:
        dynamic_functions = st.session_state.dynamic_functions
        # Go through all the keys and dynamically call the values
        for key in dynamic_functions:
            the_function = dynamic_functions[key].get('func')
            if the_function:
                the_function()


--------------------File: upload_custom_llm.py
import streamlit as st
import os
import tempfile
import time
import ast

def upload_file():
    """
    This function handles the file uploading process.
    Returns the uploaded file and its extension.
    """
    uploaded_file = st.file_uploader("Upload a file", type=['py'])
    if uploaded_file is not None:
        file_extension = uploaded_file.name.split('.')[-1]
        return uploaded_file, file_extension
    return None, None

def verify_functions(file_path, function_dict):
    """
    Verifies the functions in the uploaded file.
    Returns True if verification is successful, False otherwise.
    """
    with open(file_path, 'r') as f:
        tree = ast.parse(f.read())

    functions = []
    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):
            functions.append({'function_name': node.name, 'args': [arg.arg for arg in node.args.args]})

    custom_llm_output = next((func for func in functions if func['function_name'] == 'custom_llm_output'), None)
    if not custom_llm_output or set(custom_llm_output['args']) != set(function_dict['args']):
        return False
    return True

def save_file(uploaded_file, file_path):
    """
    Saves the uploaded file to the specified path.
    """
    if st.button('Save Custom LLM', key='save_custom_llm'):
        success_message = st.empty()
        # Get the typebuild root directory from the session state
        typebuild_root = st.session_state.typebuild_root
        file_path = os.path.join(typebuild_root, 'custom_llm.py')

        # if the file already exists, ask the user if they want to overwrite it or not
        if os.path.exists(file_path):
            overwrite = st.radio('File already exists, do you want to overwrite it?', ['Yes', 'No'], index=1)
            if overwrite == 'Yes':
                with st.spinner('Saving file...'):
                    time.sleep(2)
                    # Save the file to the data folder
                    with open(file_path, 'wb') as f:
                        f.write(uploaded_file.getbuffer())
                    st.success(f'File saved successfully')
            else:
                st.warning('File not saved')
        else:
            # Save the file to the data folder
            with open(file_path, 'wb') as f:
                f.write(uploaded_file.getbuffer())
            st.success(f'File saved successfully')

    with open(file_path, 'wb') as f:
        f.write(uploaded_file.getbuffer())

def upload_custom_llm_file():
    """
    Main function to upload a custom LLM file.
    Calls other functions to handle specific parts of the process.
    """
    uploaded_file, file_extension = upload_file()
    if uploaded_file is not None:
        with tempfile.TemporaryDirectory() as tmp_folder:
            tmp_file_path = os.path.join(tmp_folder, uploaded_file.name)
            if verify_functions(tmp_file_path, {'function_name': 'custom_llm_output', 'args': ['input', 'max_tokens', 'temperature', 'model', 'functions']}):
                success_message = st.empty()
                success_message.success('Functions verified successfully.')
                # File saving logic
                save_file(uploaded_file, tmp_file_path)
            else:
                st.error('Function verification failed.')
    return None
--------------------File: agents.py
"""
TODO:
- (Vivek) Graphs for tasks, not agents

- (Ranu) Create a menu tool and agent.  
    - Connect the agent to current projects, views, and research projects.
- (Ranu) Work on the LLM for reserch agent that creates the instruction first, runs samples, and then runs it on all text.

- (Ranu) Create a requests tool, separate that from the google search tool.
- (Ranu) Save messages by default, may be just save the graph.
- (later) Allow the user to retrieve not just the main thread but also subthreads, may be with expanders.

"""

"""
REVISING AGENTS

- One class now, which will be like AgentManager
- Orchestration will have all agents, while others will find out from definitions.
- All the task & objective related methods will move to task_graph.py
- Add _ to all private methods
- Simplify get_agent & get_task.  Make sure that the correct task gets the correct message.

"""

import time
import yaml
import os
import streamlit as st
import importlib

from extractors import Extractors
from collections import namedtuple, OrderedDict

class Agent:
    # Class variable to store message history

    def __init__(self, agent_name):        
        self.assets_needed = []
        self.messages = []
        self.tools = []
        self.parse_instructions(agent_name)
        return None
   

    def get_messages_with_instruction(self, system_instruction, prompt=None):
        """
        Returns a copy of the messages list with a new system instruction message added at the beginning.

        Args:
            system_instruction (str): The system instruction message to be added.

        Returns:
            list: A new list of messages with the system instruction message added at the beginning.
        """
        messages = self.messages.copy()
        messages.insert(0, {'role': 'system', 'content': system_instruction})
        if prompt:
            messages.append({'role': 'user', 'content': prompt})
        return messages

    def replace_placeholder_with_actual_values(self):
        # TODO: DOCUMENT WHAT THIS DOES.  
        # Should we rename this so we understand what context means?
        if hasattr(self, 'get_data_from'):
            module_name = self.get_data_from.get('module_name', '')
            function_name = self.get_data_from.get('function_name', '')
            # import the function from the module and run it
            tool_module = importlib.import_module(module_name)
            tool_function = getattr(tool_module, function_name)
            data_for_system_instruction = tool_function()
        else:
            data_for_system_instruction = None

        # Take the data_for_system_instruction and replace the variables in the system_instruction
        instruction = self.system_instruction
        if data_for_system_instruction:
            for key, value in data_for_system_instruction.items():
                instruction = instruction.replace(f"{{{key}}}", value) # We are using triple curly braces to avoid conflicts with the f-strings
            
        
        return instruction

    def get_system_instruction_for_agent(self):
        """
        Returns the system instruction for the agent.

        The system instruction includes the tool definitions and a final message for the agent manager.

        Returns:
            str: The system instruction.
        """
        instruction = self.replace_placeholder_with_actual_values()
        # Add tools to the instruction
        instruction += self.get_tool_defs()

        instruction += """You can use tools multiple times and talk to the user.
        If you need human input, you have to do two things:
        1. Set the ask_human flag to true.
        2. Pose a question in the output so that the user know to respond.

        When your job is done, set the task_finished flag to true.  Until then, task_finished should be false.

        In every turn, your response should only be a valid JSON in this format:
        {"output": output, "task_finished": boolean, "ask_human": boolean}

        You must return the result in the format above and the keys have to be verbatim. 
        """
        return instruction

    
    def parse_instructions(self, agent_name):
        """
        Parse the instructions for a given agent.

        Args:
            agent_name (str): The name of the agent.

        Returns:
            dict: The parsed instructions.

        Raises:
            FileNotFoundError: If no system instruction is found for the given agent.
        """
        
        path = os.path.join(os.path.dirname(__file__), 'agent_definitions', f'{agent_name}.yml')
        
        if os.path.exists(path):
            with open(path, 'r') as f:
                instructions = yaml.load(f, Loader=yaml.FullLoader)
            # Parse the variables
            for key in instructions:
                setattr(self, key, instructions[key])
            return instructions
        else:
            raise FileNotFoundError(f'No system instruction found for {agent_name}.')


    def get_instance_vars(self):
        """
        Returns a dictionary containing all the instance variables of the object.
        """
        return self.__dict__


    def get_tool_defs(self):
        """
        Returns a formatted string containing the list of available tools and their docstrings.
        
        The method retrieves the docstrings of the tools specified in the `self.tools` attribute.
        It then formats the information into a well-structured string, including the tool name and its docstring.
        
        Returns:
            str: A formatted string with the list of available tools and their docstrings.
        """
        add_to_instruction = ""
        if self.tools:
            add_to_instruction += """THE FOLLOWING IS A LIST OF TOOLS AVAILABLE.  DO NOT MAKE UP OTHER TOOLS.  
            CALL THEM BY THEIR NAME VERBATIM.
            
            TO USE THE TOOL, RETURN A WELL FORMATTED JSON OBJECT WITH the tool_name and kwargs as keys.
            YOU CAN FIND THE DOCSTRING OF THE TOOLS BELOW:

            """
            tools = st.session_state.extractor.get_docstring_of_tools(self.tools)
            
            for tool in tools:
                add_to_instruction += f"===\n\n{tool}: {tools[tool]}\n\n"
            
        
        return st.session_state.extractor.remove_indents_in_lines(add_to_instruction)

    def available_tools(self):
        """
        Returns a dictionary of available tools.

        The dictionary contains the names of the available tools as keys and their respective docstrings as values.

        Returns:
            dict: A dictionary of available tools with their docstrings.
        """
        extractor = Extractors()
        tools = {}
        for file in os.listdir(os.path.join(os.path.dirname(__file__), 'tools')):
            if file.endswith('.py'):
                tools[file] = extractor.get_docstring_of_tool(file)
        return tools


    def send_response_to_chat_framework(self):
        """
        Sends all the chat messages or the final message to the chat framework
        """
        pass

    def respond(self):
        # Implement the response logic
        # This can involve using the default_model, temperature, and max_tokens
        # to generate a response based on the current message history and assets
        pass

class AgentManager(Agent):
    def __init__(self, agent_name, available_agents):
        super().__init__(agent_name)

        self.available_agents = available_agents
        # TODO: Create a graph of tasks rather than a dict of agents
        
        self.completed_tasks = []
        self.scheduled_tasks = []
        self.agent_name = "agent_manager"
        # Only one agent can work at a time.  The default is the manager
        self.current_task = 'orchestration'
        self.managed_tasks = {}
        self.objectives = OrderedDict()
        self.task_tuple = namedtuple(
            'Task', 
            ['agent_name', 'agent', 'task_name', 'prompt', 'status'])
        # Agent names and descriptions of all available agents
        # All the agents available to this manager
        
        self.agent_descriptions = {}
        self.set_available_agent_descriptions(available_agents)
        
    def get_messages(self):
        """
        Returns the messages in the agent manager followed by messages of the 
        current task in one list.

        Parameters:
        None

        Returns:
        list: The messages in the agent manager followed by messages of the current task in one list.
        """
        messages = self.messages.copy()
        if self.current_task != 'orchestration':
            task = self.get_task(self.current_task)
            agent_messages = task.agent.messages.copy()
            
            messages.extend(agent_messages)
        return messages

    def add_task(self, agent_name, task_name, task_description):
        """
        Add a new agent to the list of managed agents.

        Args:
            agent_name (str): The name of the agent to be added.

        Returns:
            None
        """
        if task_name not in self.managed_tasks:
            # Create an named tuple with the agent name, agent and description
            if agent_name == 'agent_manager':
                agent = self
            else:
                agent = Agent(agent_name)
            self.managed_tasks[task_name] = self.task_tuple(
                agent_name=agent_name, 
                agent=agent, 
                task_name=task_name, 
                prompt=task_description,
                status='just_started'
                )
            # Add the task name to scheduled tasks
            self.scheduled_tasks.append(task_name)
        return None

    def add_objective(self, objective_name, task_list):
        """
        Add a new objective to the objectives attribute, which is an ordered dictionary.

        Args:
            objective_name (str): The name of the objective to be added as the key
            task_list (list): A list of tasks that are part of the objective.  
                These are prompts that will be passed to the agent manager to initiate each task.

        Returns:
            None
        """
        # Note: The ideal thing to do is to have unique names for each task, so that they can be
        # completed at any time.  However, we are not doing that for now.
        self.objectives[objective_name] = {
            'scheduled': task_list, 
            'completed': [], 
            'status': 'not_started'}
        return None


    def complete_task(self, task_name):
        """
        Mark a task as completed.

        Args:
            task_name (str): The name of the task to be marked as completed.

        Returns:
            None
        """
        if task_name in self.scheduled_tasks:
            task_index = self.scheduled_tasks.index(task_name)
            removed = self.scheduled_tasks.pop(task_index)
            self.completed_tasks.append(removed)
        else:
            with st.spinner(f"Task {task_name} not found."):
                st.error(f"Task {task_name} not found.")
        
        return None

    def get_next_task(self):
        """
        Returns the next task to be completed
        and sets the current task.

        Returns:
            str: The name of the next task to be completed.
        """
        if len(self.scheduled_tasks) > 0:
            self.current_task = self.scheduled_tasks[0]
            st.session_state.current_task = self.current_task
            return self.scheduled_tasks[0]
        else:
            return None

    def set_available_agent_descriptions(self, available_agents):
        """
        Set the available agent descriptions.

        Args:
            available_agents (list): A list of available agent names.
        """
        for agent_name in available_agents:
            path = os.path.join(os.path.dirname(__file__), 'agent_definitions', f'{agent_name}.yml')
            with open(path, 'r') as f:
                instructions = yaml.load(f, Loader=yaml.FullLoader)

            description = instructions.get('description', '')
            if description:
                self.agent_descriptions[agent_name] = description
        return None
    
    def get_system_instruction(self, task_name):
        """
        Add the agent name and description to the system instructions

        Args:
            agent_name (str): The name of the agent

        Returns:
            str: The system instruction with the agent name and description
        """

        if task_name == 'orchestration':
            instruction = self.replace_placeholder_with_actual_values()
            # Add tools to the instruction
            instruction += self.get_tool_defs()
            instruction += "THE FOLLOWING IS A LIST OF AGENTS AVAILABLE.  DO NOT MAKE UP OTHER AGENTS.  CALL THEM BY THEIR NAME VERBATIM:\n"
            for agent_name, description in self.agent_descriptions.items():
                instruction += f"- {agent_name}: {description}\n"
            if self.scheduled_tasks:
                scheduled_tasks = '- ' + '\n- '.join(self.scheduled_tasks)
                instruction += f"""The following are the scheduled tasks:
                {scheduled_tasks}
                Explain to the user what you are working on."""

        else:
            task = self.get_task(task_name)
            agent_name = task.agent_name
            agent = task.agent
            instruction = agent.get_system_instruction_for_agent()
        
        return instruction

    def get_agent(self, task_name):
        if task_name == 'orchestration':
            return self
        else:
            return self.managed_tasks[task_name].agent

    def get_task(self, task_name):
        """
        Returns the agent for the given agent_name

        Parameters:
        agent_name (str): The name of the agent to retrieve.

        Returns:
        agent: The agent object associated with the given agent_name. If the agent_name is not found in the managed_tasks dictionary, returns self.
        """
        if task_name in self.managed_tasks:
            task = self.managed_tasks[task_name]
            return task
        else:
            return None
    
    def remove_task(self, task):
        if task in self.managed_tasks:
            del self.managed_tasks[task]
        return None
    
    def set_task_status(self, task_name, status):
        if task_name in self.managed_tasks:
            task = self.managed_tasks[task_name]
            self.managed_tasks[task_name] = task._replace(status=status)
        return None


    def set_message(self, role, content, task='orchestration'):
        """
        Adds a user, assistant or system content to the chat.

        Args:
            content (str): The content to add.
        """
        current_task = self.current_task
        if current_task == 'orchestration':
            self.messages.append({'role': role, 'content': content})
            st.session_state.all_messages.append({'role': role, 'content': content, 'task': 'orchestration'})
        else:
            task = self.get_task(current_task)
            agent = task.agent
            agent_name = task.agent_name
            agent.messages.append({'role': role, 'content': content})
            st.session_state.all_messages.append({'role': role, 'content': content, 'task': agent_name})    
    
        return None


    def chat_input_method(self):
        """
        Handles the input of chat messages.

        This method provides an input field for the user to enter their message.
        Upon receiving a message, it updates the messages list and sets the
        `ask_llm` flag to True.

        Returns:
            None
        """
        prompt = st.chat_input("Enter your message", key="chat_input")
        if prompt:
            self.set_message(role="user", content=prompt)
            self.ask_llm = True
            st.session_state.ask_llm = True
            self.display_expanded = True
        return None


--------------------File: llm_access.py
import tempfile
import openai
import anthropic
import streamlit as st
import os
import toml
import time
from upload_custom_llm import upload_custom_llm_file

class LLMConfigurator():
    def __init__(self):
        self.user_folder = st.session_state.user_folder
        self.secrets_file_path = os.path.join(self.user_folder, 'secrets.toml')
        self.ensure_secrets_file_exists()
        self.load_config()

    def ensure_secrets_file_exists(self):
        if not os.path.exists(self.secrets_file_path):
            with open(self.secrets_file_path, 'w') as f:
                f.write('')
        return None

    def load_config(self):
        with open(self.secrets_file_path, 'r') as f:
            config = toml.load(f)
            st.session_state.config = config

    def set_or_get_llm_keys(self):
        api_key = st.session_state.config.get('openai', {}).get('key', '')
        claude_key = st.session_state.config.get('claude', {}).get('key', '')
        if api_key:
            st.session_state.openai_key = api_key
        if claude_key:
            st.session_state.claude_key = claude_key
        return None

    def config_project(self):
        self.load_config()
        self.set_or_get_llm_keys()
        default_index = 0
        
        # Check if the user has a custom_llm.py file in the typebuild_root folder
        if os.path.exists(os.path.join(st.session_state.typebuild_root, 'custom_llm.py')):
            default_index = 1

        llm_selection = st.radio(
            'Select an option',
            ['Set LLM API key', 'Upload Custom LLM'],
            captions=["Set keys for OpenAI or Anthropic", "Access other LLMs"],
            horizontal=True,
            index=default_index
        )

        if llm_selection == 'Upload Custom LLM':
            upload_custom_llm_file()
            st.stop()
        else:
            self.update_llm_keys()

    def update_llm_keys(self):
        api_key = st.text_input(
            'Enter OpenAI key',
            value=st.session_state.config.get('openai', {}).get('key', '')
        )
        claude_key = st.text_input(
            'Enter Claude key',
            value=st.session_state.config.get('claude', {}).get('key', '')
        )
        function_call_availabilty = st.checkbox(
            "(Expert setting) I have access to function calling",
            value=st.session_state.config.get('function_call_availabilty', True),
            help="Do you have access to openai models ending in 0613? they have a feature called function calling."
        )

        if st.button("Submit config"):
            if not api_key:
                st.error('Enter the API key')
                st.stop()
            self.save_llm_keys(api_key, claude_key, function_call_availabilty)
        else:
            st.stop()
            
    def save_llm_keys(self, api_key, claude_key, function_call_availabilty):
        openai.api_key = api_key
        config = {
            'openai': {'key': api_key},
            'claude': {'key': claude_key} if claude_key else {},
            'function_call_availabilty': function_call_availabilty
        }

        with st.spinner('Saving config...'):
            time.sleep(2)

        with open(self.secrets_file_path, 'w') as f:
            toml.dump(config, f)
            time.sleep(.5)
            st.success('Config saved successfully')
        return None

# # # Usage
# # configurator = ProjectConfigurator()
# # configurator.config_project()
# #------------OLD CODE----------------
# def set_or_get_llm_keys():

#     # Check if the user has a secrets file and openai key in the secrets.toml file. if yes, then set the openai key

#     # Get the project folder from the session state
#     user_folder = st.session_state.user_folder
#     # Create the secrets.toml file if it does not exist
#     secrets_file_path = os.path.join(user_folder, 'secrets.toml')
#     if not os.path.exists(secrets_file_path):
#         with open(secrets_file_path, 'w') as f:
#             f.write('')
#         st.session_state.config = {}
#     else:
#         with open(secrets_file_path, 'r') as f:
#             config = toml.load(f)
#             st.session_state.config = config
#     api_key = st.session_state.config.get('openai', {}).get('key', '')
#     claude_key = st.session_state.config.get('claude', {}).get('key', '')
#     if api_key != '':
#         openai.api_key = api_key
#     if claude_key != '':
#         st.session_state.claude_key = claude_key
#     return api_key

# def config_project():
#     """
#     For a new project, there should be a config.json file in the project_settings folder. if not, then this function will create one.
#     this config file should have the following keys:
#     - preferred model (str): The preferred model for the project, e.g. gpt-3.5-turbo-16k, gpt-3.5-turbo, gpt-4, etc.
#     - api key (str): The API key for the openai API, if preferred model is openai's
#     - function_call_availabilty (bool): Does the user have access to the 0613 models of openai?

#     Save the api key to streamlit secrets.toml file
    
#     """
#     # Get the secrets_file_path from the session state
#     secrets_file_path = st.session_state.secrets_file_path
#     with open(secrets_file_path, 'r') as f:
#         config = toml.load(f)
#         st.session_state.config = config

#     # Check if the user wants to upload a custom LLM file or set the openai API key
#     default_index = 0
#     api_key = set_or_get_llm_keys()
#     if os.path.exists(os.path.join(st.session_state.typebuild_root, 'custom_llm.py')):
#         default_index = 1

#     llm_selection = st.radio('Select an option', ['Set OpenAI API key', 'Upload Custom LLM'], horizontal=True, index=default_index)

#     if llm_selection == 'Upload Custom LLM':
#         upload_custom_llm_file()
#         st.stop()
#     else:
#         api_key = st.text_input('Enter OpenAI key', value=st.session_state.config.get('openai', {}).get('key', ''))
#         claude_key = st.text_input('Enter Claude key', value=st.session_state.config.get('claude', {}).get('key', ''))

#         function_call_availabilty = st.checkbox(
#             "(Expert setting) I have access to function calling", 
#             value=st.session_state.config.get('function_call_availabilty', True),
#             help="Do you have access to openai models ending in 0613? they have a feature called function calling.",
#             )
            
#         if st.button("Submit config"):
#             if api_key == '':
#                 st.error('Enter the API key')
#                 st.stop()
#             # Save the config to the config.json file
#             config = {}
#             # set the openai key
#             openai.api_key = api_key
#             # Save the API key in the secrets module
#             config['openai'] = {'key': api_key}
#             if claude_key != '':
#                 config['claude'] = {'key': claude_key}
#             config['function_call_availabilty'] = function_call_availabilty
#             if function_call_availabilty:
#                 st.session_state.function_call_type = 'auto'
#             else:
#                 st.session_state.function_call_type = 'manual'
#             # Save the config to the config.json file
#             with st.spinner('Saving config...'):
#                 time.sleep(2)

#             if not os.path.exists(secrets_file_path):
#                 with open(secrets_file_path, 'w') as f:
#                     f.write('')
#             with open(secrets_file_path, 'r') as f:
#                 config_ = toml.load(f)

#             # Add the API key to the config dictionary
#             config_['openai'] = {'key': api_key}
#             if claude_key != '':
#                 config_['claude'] = {'key': claude_key}
#             config_['function_call_availabilty'] = function_call_availabilty
#             # Save the config to the secrets.toml file
#             with open(secrets_file_path, 'w') as f:
#                 toml.dump(config_, f)
#                 st.toast('Hip!')
#                 time.sleep(.5)
#                 st.success('Config saved successfully')
#         return None
    


--------------------File: graphical_menu.py
import time
import networkx as nx
import streamlit as st
from streamlit_elements import elements, mui, html, sync
import streamlit as st


def display_menu_bar(menu_options):
    """
    Display a menu bar with clickable buttons based on the given menu options.

    Args:
        menu_options (list): A list of menu options to be displayed.

    Returns:
        None
    """
    # create_dynamic_functions(menu_options)
    for index, option in enumerate(menu_options):
        # Create the function if it doesn't exist in locals
        # lower case and replace spaces with underscores and ~ with _
        clean_option = option.lower().replace(' ', '_').replace('~', '_').replace('.', '_').replace('-', '_').replace(',','_')
        if f"menu_button_function_{clean_option}" not in locals():
            myfunc = f"""def menu_button_function_{clean_option}(event):
                st.session_state.activeStep = "{option}"
                st.session_state.should_rerun = True
                return None
            """            
            exec(myfunc)
            
    if 'activeStep' not in st.session_state:
        st.session_state.activeStep = 'HOME'     
    
    unique_labels = []
    for option in menu_options:
        label = option.split('~')[0]
        if label not in unique_labels:
            unique_labels.append(label)

    with elements(st.session_state.activeStep):
        with mui.AppBar(position="relative", sx = {'borderRadius': 10}, key=f"{st.session_state.activeStep}_appbar"):
            with mui.Toolbar(disableGutters=True, variant="dense"):
                for index, option in enumerate(menu_options):
                    # lower case and replace spaces with underscores and ~ with _
                    clean_option = option.lower().replace(' ', '_').replace('~', '_').replace('.', '_').replace('-', '_').replace(',','_')
                    with mui.Grid(container=True, spacing=0):
                        mui.Button(color="inherit", onClick= eval(f'menu_button_function_{clean_option}'))(option.split('~')[0])
                        st.session_state.menu_displayed = True
    return None

class GraphicalMenu:
    G = None

    def __init__(self):
        # Create the Graph
        self.G = nx.DiGraph()
        self.G.add_node("HOME", node_name="HOME", func_name="home_page", module_name="home_page")
        return None

    def add_edges(self, menu_edges_data):
        """
        This method adds nodes and edges to a graph.

        Parameters:
        - menu_edges_data (list): A list of edges data containing source, node names, function names, and module names.

        Returns:
        - None

        For uniqueness, the node names are formatted as "node_name~source".
        """
        # Create a reference to the graph
        G = self.G

        # Iterate over each edge in the menu_edges_data
        for edge in menu_edges_data:
            source = edge[-1]  # Get the source from the last item in the sublist

            # Format node_0 and node_1 based on edge[0] and edge[1]
            if edge[0] == "HOME":
                node_0 = "HOME"
            else:
                node_0 = f"{edge[0]}~{source}"

            if edge[1] == "HOME":
                node_1 = "HOME"
            else:
                node_1 = f"{edge[1]}~{source}"

            # Add the edge to the graph with the function name and module name as properties
            if node_1 not in G.nodes:
                G.add_node(node_1, node_name=edge[1], func_name=edge[2], module_name=source)
            G.add_edge(node_0, node_1)

        return None


    def add_functions(self, edges):
        """
        Add functions to the graphical menu.

        Parameters:
        - edges (list): A list of edges representing the connections between nodes.

        Returns:
        None
        """
        run = False
        for edge in edges:
            if len(edge) == 3:
                src, dst, func_name = edge
            else:
                dst = 'HOME'
                func_name = 'home_page'
                source = 'home_page'

            node_name = f"{dst}~{source}"
            # Append if the node name is not already in the source functions
            if node_name not in [func[0] for func in self.source_functions]:
                self.source_functions.append([node_name, source, func_name, run])

    def add_menu_items(self, menu_data, source):
        if source not in self.menu_sources:
            self.menu_sources.append(source)
            self.menu_edges += menu_data
        return None
    
    def top_level_nodes(self):
        """
        Get the top level nodes from a list of edges.  These are the nodes that have no parents.
        """
        G = self.G
        # Get children of "HOME"
        roots = [n for n in G.successors("HOME")]
        return roots

    def get_nodes_from_edges(self):
        nodes = set()
        for edge in self.menu_edges:
            # Get the source and destination nodes only.  
            # The third element is the function name
            edge = edge[:2]
            for node in edge:
                if node != "HOME":
                    nodes.add(node)
        return list(nodes)


    def get_children_and_parent(self, selected_node):
        G = self.G
        children = list(G.successors(selected_node))

        parent = "HOME" if not list(G.predecessors(selected_node)) else list(G.predecessors(selected_node))[0]
        return parent, children

    def get_ancestors(self, selected_node):
        G = self.G
        ancestors = list(nx.ancestors(G, selected_node))
        # Reverse the ancestors
        ancestors.reverse()
        # Add the current node name
        current_node_name = G.nodes[selected_node].get('node_name')
        ancestors.append(current_node_name)
        return ancestors

    def get_module_and_function(self, selected_node):
        """
        Get the module and function name from the selected node.
        """
        # Get the list with the selected node from the menu
        node_info = [node for node in self.source_functions if node[0] == selected_node][0]
        module_name = node_info[1].split('~')[1]
        func_name = node_info[2]
        return module_name, func_name

    def create_menu(self):
        if 'selected_node' not in st.session_state:
            st.session_state['selected_node'] = 'HOME'

        # If selected_node is HOME, get top level nodes
        options = ['HOME']
        if st.session_state['selected_node'] == 'HOME':
            options += self.top_level_nodes()
        else:
            parent, children = self.get_children_and_parent(st.session_state['selected_node'])
            options += children
    
        # Display the options
        display_menu_bar(options)
        
        selected_option = st.session_state.activeStep

        if selected_option == 'HOME':
            st.session_state['selected_node'] = 'HOME'
        elif selected_option == 'SELECT':
            st.session_state.should_rerun = False
        else:
            st.session_state['selected_node'] = selected_option
        

        if st.session_state.selected_node != 'HOME':
            ancestors = self.get_ancestors(st.session_state['selected_node'])
            
            # Show breadcrumbs
            breadcrumbs = " > ".join(ancestors)
            st.markdown(f"**{breadcrumbs}**")
        
        if st.session_state.should_rerun == True:
            st.session_state.should_rerun = False
            st.rerun()
    
def show_node_properties():
    """
    Allow the user to select a node and show the properties of the selected node.
    """
    G = st.session_state.menu.G
    # Get the list of nodes
    nodes = G.nodes
    node_name = st.selectbox("Select a node", nodes)
    node_info = G.nodes[node_name]
    st.markdown(f"**Node name:** {node_info.get('node_name')}")
    st.markdown(f"**Function name:** {node_info.get('func_name')}")
    st.markdown(f"**Module name:** {node_info.get('module_name')}")

    from function_management import get_docstring_of_function
    docstring = get_docstring_of_function(node_name)
    st.markdown(f"### Docstring\n{docstring}")
    return None
--------------------File: requirements.py
# JUST CREATED THIS FILE...HAVE TO WORK ON IT.
# Moved functiions from project_management.py to here.


def set_user_requirements():
    """
    This stores the user requirement for the given view,
    based on the selected menu. 
    """
    file_path = os.path.join(st.session_state.project_folder, 'project_settings', 'user_requirements.txt')
    key = 'User Requirements'
    widget_label = 'User Requirements'
    st.subheader('User requirements')
    user_requirements = text_areas(file=file_path, key=key, widget_label=widget_label)
    # Save to session state
    st.session_state.user_requirements = user_requirements

    user_requirements_chat()
    st.stop()
    return None

def user_requirements_chat():

    """
    A chat on the user requirements.
    That could be exported to the user requirements file.
    """
    # If there is no user requirements chat in the session state, create one
    if 'user_requirements_chat' not in st.session_state:
        st.session_state.user_requirements_chat = []
    
    chat_container = st.container()
    prompt = st.chat_input("Enter your message", key='user_requirements_chat_input')
    if prompt:
        # Create the messages from the prompts file
        prompts.blueprint_prompt_structure(prompt=prompt)
        with st.spinner('Generating response...'):
            res = get_llm_output(st.session_state.user_requirements_chat, model='gpt-3.5-turbo-16k')
            # Add the response to the chat
            st.session_state.user_requirements_chat.append({'role': 'assistant', 'content': res})
    
    # Display the user and assistant messages
    with chat_container:
        for msg in st.session_state.user_requirements_chat:
            if msg['role'] in ['user', 'assistant']:
                with st.chat_message(msg['role']):
                    st.markdown(msg['content'])

    return None



--------------------File: chat.py
import streamlit as st
from plugins.llms import get_llm_output
import os
from glob import glob
from agents import AgentManager, Agent
import importlib
import json    
import time
import inspect

def display_messages(messages, expanded=True):
    """
    Displays the messages in the chat.

    Utilizes Streamlit's expander and chat_message for displaying messages.
    This method iterates through the messages list and displays each one based
    on the role (user, assistant, system).

    Returns:
        None
    """
    if messages:
        with st.expander("View chat", expanded=expanded):
            for i, msg in enumerate(messages):
                # TODO: REMOVE SYSTEM MESSAGES AFTER FIXING BUGS
                the_content = msg['content']
                if msg['role'] in ['user', 'assistant']:
                    with st.chat_message(msg['role']):
                        if the_content.startswith('{'):
                            the_content = eval(the_content)
                            st.json(the_content)
                        if isinstance(the_content, dict):
                            st.json(the_content)
                        else:
                            st.markdown(the_content.replace('\n', '\n\n'))
                if msg['role'] == 'system':
                    st.info(the_content)        
    return None


def add_agent_manager_to_session_state():
    if 'agent_manager' not in st.session_state:
        # Get all the agents from the agent_definitions folder, in os independent way
        # Current directory plus agent_definitions
        agent_definitions = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'agent_definitions')
        # Get all the files in the agent_definitions folder
        agent_files = glob(os.path.join(agent_definitions, '*.yml'))
        # Get the agent names
        agent_names = [os.path.basename(i).replace('.yml', '') for i in agent_files]
        # Add the agent names to AgentManager
        agent_manager = AgentManager('agent_manager', agent_names)
        st.session_state.agent_manager = agent_manager
    return None

def manage_llm_interaction(agent_manager):
    """
    This function sends messages to the LLM and gets a response.
    Based on the response, it could create additional specialized agents
    to handle the request.  It also routes the response to the appropriate agent.

    Parameters:
    - agent_manager: The agent manager object.

    Returns (str):
    - The response from the LLM.
    """
    # FETC

    current_task = agent_manager.current_task
    # Get messages for the LLM
    system_instruction = agent_manager.get_system_instruction(agent_manager.current_task)
    # st.success(f"System instruction: {system_instruction}")
    prompt = None
    if agent_manager.current_task == 'orchestration':
        agent = agent_manager
    else:
        task = agent_manager.get_task(agent_manager.current_task)
        agent = task.agent
        task_name = task.task_name
        status = task.status
        if status == 'just_started':
            prompt = task.prompt
            agent_manager.set_task_status(task_name, 'current_task')
            prompt = task.prompt
        else:
            prompt = None

    messages = agent.get_messages_with_instruction(system_instruction, prompt=prompt)
    st.session_state.last_request = messages

    # Get the response from the LLM
    with st.spinner("Getting response from LLM..."):      
        res = get_llm_output(messages, model=agent.default_model)
        st.info(f"LLM output: {res}")
    return res

def populate_res_dict(res_dict, res):
    """
    If res dict is empty, use the string response to populate it.
    """
    if not res_dict:
        res_dict = {
            'output': res, 
            'task_finished': False,
            'ask_human': False,
            'ask_llm': True,
            }
    
    # If there is a final response, set the task_finished flag to true
    if 'final response' in res.lower():
        res_dict['task_finished'] = True
        res_dict['ask_llm'] = False
        res_dict['ask_human'] = False

    return res_dict

def manage_task(agent_manager, res_dict, res):
    
    # Convert the response to a dict even if it is a string
    res_dict = populate_res_dict(res_dict, res)
    
    # If the response is a request for a new agent, create the agent, if it does not exist
    if 'transfer_to_task' in res_dict:
        agent_name = res_dict.get('agent_name', 'agent_manager')
        task_name = res_dict.get('transfer_to_task', 'orchestration')
        task_description = res_dict.get('task_description', 'No description provided.')
        explain_to_user = res_dict.get('explain_to_user', None)
        # Add explanation as content to the messages
        if explain_to_user:
            agent_manager.set_message(
                role="assistant", 
                content=explain_to_user, 
                task=task_name
                )    

        if task_name != agent_manager.current_task and task_name != 'orchestration':
            st.info(f"Changing agent from {agent_manager.current_task} to {task_name}")
            agent_manager.add_task(
                agent_name=agent_name, 
                task_name=task_name, 
                task_description=task_description
                )
            # Add the message to the new task
            agent_manager.set_message(
                role="assistant", 
                content=task_description, 
                task=task_name,
                )

    if res_dict.get('task_finished', False):
        completed_task = agent_manager.current_task
        agent_manager.complete_task(completed_task)
        agent_manager.current_task = 'orchestration'
        st.session_state.ask_llm = False
    elif res_dict.get('ask_human', False):
        # Set the message to the worker agent via the agent manager
        st.session_state.ask_llm = False
    else:
        # If we are not sure, ask LLM.
        st.session_state.ask_llm = res_dict.get('ask_llm', True)
      

    # Add the response to the current task
    content = res_dict.get('output', str(res_dict))
    if content:
        agent_manager.set_message(
            role="assistant", 
            content=content, 
            task=agent_manager.current_task
            )

    # If ask_human is true in the response, set the ask_llm to false
    # If the current task is still orchestration, check for next task, if any.
    if agent_manager.current_task == 'orchestration':
        # Get the next task
        next_task = agent_manager.get_next_task()
        # If there is a next task, set the current task to the next task
        if next_task is not None:
            st.session_state.ask_llm = True

    return res_dict


def manage_tool_interaction(agent_manager, res_dict):
    """
    If an agent requested to use a tool,
    this function will run the tool and return the result to the agent.

    It will also request a response from the LLM.
    """
    tool_name = res_dict['tool_name']
    tool_module = importlib.import_module(f'tools.{tool_name}')
    tool_function = getattr(tool_module, 'tool_main')

    # Get the tool arguments needed by the tool
    tool_args = inspect.getfullargspec(tool_function).args

    # Arguments for tool will be in res_dict under the key kwargs
    args_for_tool = res_dict.get('kwargs', res_dict)

    # select the required arguments from res_dict and pass them to the tool
    kwargs = {k: v for k, v in args_for_tool.items() if k in tool_args}
    tool_result = tool_function(**kwargs)
    # TODO: Some tools like search need to consume the tool results.
    # Others like navigator need not.  Create a system to pass to the
    # correct task. 
    if isinstance(tool_result, str):
        tool_result = {'content': tool_result}

    # Check if the the task is done and can be transferred to orchestration.
    if tool_result.get('task_finished', True):
        # Add a message from the tool to the agent manager
        content = f"""MESSAGE TO THE AGENT MANAGER.  
        The {agent_manager.current_task} task is done.\n"""

        if tool_result.get('content', None):
            content += f"""Here are some notes from the completed task that you can use to help the user:  
            {tool_result['content']}"""
        # Set the current task to orchestration
        agent_manager.current_task = 'orchestration'
        # Message to agent manager goes as system message
        role = 'system'
    else:
        content = tool_result.get('content', '')
        role = 'user'
    # Set ask_llm status
    st.session_state.ask_llm = tool_result.get('ask_llm', True)
    if content:
        # Add this to the agent's messages
        agent_manager.set_message(
            role=role, 
            content=content,
            task=agent_manager.current_task
            )

    with st.spinner("Let me study the results..."):
        st.sidebar.warning(f"Ask llm: {st.session_state.ask_llm}\n\nCurrent task: {agent_manager.current_task}")
    return None

def show_task_messages(agent_manager):
    """
    Show the task names in a drop down
    and the messages for the selected task.
    """
    # Get the task names
    task_names = agent_manager.managed_tasks.keys()
    # Show the task names in a drop down
    all_tasks = list(task_names)
    all_tasks.insert(0, 'orchestration')
    all_tasks.insert(0, 'SELECT')
    selected_task = st.sidebar.selectbox("Select task", all_tasks)
    # Show the messages for the selected task
    if selected_task != 'SELECT':
        if selected_task == 'orchestration':
            messages = agent_manager.messages
        else:
            messages = agent_manager.managed_tasks[selected_task].agent.messages
        st.header(f"Messages for {selected_task}")
        display_messages(messages)
        st.stop()
    return None

def add_objective(agent_manager):
    """
    Adds the next tasks to the agent manager.
    """

    # TODO: FIND OUT WHY THIS GETS REPEATED MANY TIMES.
    objective = "Haiku collection on each season"
    tasks = ['summer', 'winter']
    # Tasks not in managed tasks
    tasks_to_add = [i for i in tasks if i not in agent_manager.managed_tasks]
    
    if tasks_to_add:
        st.sidebar.info("There are new tasks to add.  Click the button below to add them.")
        if st.sidebar.button("Add new tasks"):
            for task in tasks_to_add:
                # Create new search task
                agent_manager.add_task(
                    agent_name='agent_manager', 
                    task_name=f'{task}_haiku', 
                    task_description=f'Write a haiku about {task}'
                    )
                # Set ask llm to true
            st.session_state.ask_llm = True
    completed_tasks = "\n- ".join([i for i in tasks if i in agent_manager.completed_tasks])
    if completed_tasks:
        completed_tasks = f"### Completed tasks:\n\n- {completed_tasks}"
    scheduled_tasks = "\n- ".join([i for i in tasks if i in agent_manager.scheduled_tasks])
    if scheduled_tasks:
        scheduled_tasks = f"### Scheduled tasks:\n\n- {scheduled_tasks}"
    task_info = f"""# {objective}
    There are {len(tasks)} tasks in this objective.
    {completed_tasks}
    {scheduled_tasks}
    """
    # Remove indents
    task_info = "\n".join([i.strip() for i in task_info.split('\n')])
    st.sidebar.markdown(task_info)
    return None



# TODO: MAKE THIS A CHAT FRAMEWORK CLASS
def chat():

    # Add the agent manager to the session state
    add_agent_manager_to_session_state()
    agent_manager = st.session_state.agent_manager
     
    add_objective(agent_manager)

    # Create the chat input and display
    st.sidebar.success(f"Scheduled tasks: {agent_manager.scheduled_tasks}")
    agent_manager.chat_input_method()    
    show_task_messages(agent_manager)
    messages = agent_manager.get_messages()    
    display_messages(messages, expanded=True)

    st.sidebar.info(f"Ask llm: {st.session_state.ask_llm}\n\nCurrent task: {agent_manager.current_task}")

    # ask_llm took can be set to true by agents or by tools
    # that add to the message queue without human input
    # and request a response from the llm
    if st.session_state.ask_llm:
        # Get the response from the llm
        res = manage_llm_interaction(agent_manager)        
        # Extract the response dictionary
        res_dict = st.session_state.extractor.extract_dict_from_response(res)
        # st.write(res_dict)
        # st.code(res)
        res_dict = manage_task(agent_manager, res_dict, res)
        # If a tool is used, ask the llm to respond again
        if 'tool_name' in res_dict:
            manage_tool_interaction(agent_manager, res_dict)
            

        st.rerun()

    return None

--------------------File: home_page.py
"""
Home page of the app.  There's a home page for logged in users 
and a home page for non-logged in users.

Currently focusing on the home page for logged in users.
"""
import streamlit as st
import os
from PIL import Image

def home_page():
    st.header("What you can do with TypeBuild", divider='rainbow')
    st.info("""**Click Functionalities in the menu on top to access the functionalities discussed below.**""")
    what_can_you_do()

def what_can_you_do():
    dir_path = st.session_state.dir_path
    c1, c2 = st.columns([1, 1])
    with c1:
        image_path = os.path.join(dir_path, 'images', 'upload_data.png')
        st.image(image=Image.open(image_path), output_format='PNG', use_column_width=True)
        caption='''**DATA**: Upload your own data or fetch data from the web.
            ***You should have some data to use other functionalities.***
            '''
        st.warning(caption)
    
    with c2:
        image_path = os.path.join(dir_path, 'images', 'ideate.png')
        st.image(image=Image.open(image_path), output_format='PNG', use_column_width=True)
        caption='''**IDEATE**: Ideate using the language model.  Useful to identify personas and to create user journeys'''
        st.warning(caption) 

    c1, c2 = st.columns([1, 1])
    # Add a button to go to the ideate page
    with c2:
        image_path = os.path.join(dir_path, 'images', 'llm.png')
        st.image(image=Image.open(image_path), output_format='PNG', use_column_width=True)
        caption='''**LLM ANALYSIS**: Analyze your data using a language model.  
            Useful to synthesize research, categorize data, and to extract specific insights from documents.
            '''
        st.warning(caption)

    with c1:
        # Build mini-apps, forms, and data analysis
        image_path = os.path.join(dir_path, 'images', 'build.png')
        st.image(image=Image.open(image_path), output_format='PNG', use_column_width=True)
        caption='''**APPS & ANALYSIS**: Build mini-apps, create forms, or analyze data using natural language.
            This is great for creating prototypes, simple workflows, and to analyze data.
            '''
        st.warning(caption)
--------------------File: session_state_management.py
import os
import streamlit as st

def add_default_session_states():
    # Add a dict with session state variables
    ss_vars = {
        'messages': [],
        'conversations': [],
        'ss_num': 0,
        'menu_id': 0,
        'developer_options': False,
        'ask_llm': False,
        'current_task': 'orchestration',
        'current_chat': None,
        'call_status': None,
        'dir_path': os.path.dirname(os.path.realpath(__file__)),
        'home_dir' : os.path.expanduser("~"),
        'typebuild_root' : os.path.join(os.path.expanduser("~"), '.typebuild'),
        'user_folder': os.path.join(os.path.expanduser("~"), '.typebuild', 'users', st.session_state.token),
        'project_folder': os.path.join(os.path.expanduser("~"), '.typebuild', 'users', st.session_state.token),
        'data_folder': os.path.join(os.path.expanduser("~"), '.typebuild', 'users', st.session_state.token, 'data'),
        'audio_folder': os.path.join(os.path.expanduser("~"), '.typebuild', 'users', st.session_state.token, 'audio'),
        'secrets_file_path' : os.path.join(os.path.expanduser("~"), '.typebuild', 'users', st.session_state.token, 'secrets.toml'),
        'profile_dict_path': os.path.join(os.path.expanduser("~"), '.typebuild', 'users', 'admin', 'profile_dict.pk'),
        'should_rerun': False,
        'selected_node': 'HOME',
        'all_messages': [],
        "developer_options": False,
        "activeStep": "Home",
        "dynamic_functions": {},
        "dynamic_variables": {},        
        }

    # check if data_folder and audio_folder exists
    if not os.path.exists(ss_vars['data_folder']):
        os.makedirs(ss_vars['data_folder'])
    if not os.path.exists(ss_vars['audio_folder']):
        os.makedirs(ss_vars['audio_folder'])

    for key in ss_vars:
        if key not in st.session_state:
            st.session_state[key] = ss_vars[key]
    return None

def change_view():
    """
    When the view is changed, clear the session state
    of key variables.
    """
    # Increment view number so that the widget selection changes
    # Clear the session state of key variables
    key_vars = ['code', 'code_str', 'response', 
                'user_requirements', 'messages', 'data_description',
                'error', 'message_to_agent',
                'last_function_call',
                ]
    for key in key_vars:
        if key in st.session_state:
            del st.session_state[key]

    # Set ask llm as false
    st.session_state.ask_llm = False
    return None

def change_ss_for_project_change():
    """
    When the project is changed, clear the session state
    of key variables that affect a project.
    """
    change_view()
    additional_vars = ['df', 'project_file', 'project_folder', 'column_info']
    for key in additional_vars:
        if key in st.session_state:
            del st.session_state[key]
    st.session_state.project_description_chat = []
    return None


def main():
    # load_header_states()
    add_default_session_states()
    return None
--------------------File: chat_with_planning.py
"""
# Think about things
- [ ] Master planner should not be the default.  Let the user have a conversation naturally.  If there's a project, then we can call the master planner. 
- [ ] Based on the template, I could set the agent directly.  Try this.

# Tasks to do
***Current objective:***
- [x] Vivek: Provide the planner with metadata and access to the last few messages.
- [ ] Vivek: Give planner the ability to query an task for information.
- [x] If a graph exists, allow the user to add tasks to it. Right now, planner is creating new graphs.  Current task should be the planner, when needed as well.
- [ ] Vivek: How to use metadata in files while executing tasks.  We need this to get system instruction from 
    prompt agent to the llm for tables agent.
- [x] Vivek: User should be able to go back to a task anytime.
- [ ] Vivek: Start task button should not appear after a task is done.

# TODO: Delete things
- [ ] Ranu: User should be able to delete UI manually.  When they do it, we have to delete the corresponding message & task.
- [ ] Ranu: User should be able to delete a task.  When they do it, we have to delete the corresponding message & task.  They should be able to do it via chat.

# TODO: Self-fixing errors
- [x] When a task is updated, it sends a new instruction to the tool.  But the previous response from the tool is still there, and that's what we display.  How do we change that?
- [ ] Ranu: Make task name as the key.  See if the agent needs to do it, or if we can do it using code.  
- [ ] Ranu: Modify all the tools to take the task name as the key.
- [ ] Ranu: Tools should add functions to dynamic functions where we need to display something. 
- [ ] Ranu: We will have access to the task name.  Pass this info to the tool whenever we call it in manage_tool_interaction function.  

- [] Record if tool was run, if not run it when the error is fixed
- [] In case of an error, send back to the previous node or the relevant node to get the input fixed.


# Usability
- [ ] Vivek: Previous conversations in the order generated (sort by created date).
- [ ] Vivek: Change the layout of load templates.  Offer generic template and specialized ones as a grid.
- [ ] Create a generic typebuild page for installation (Ollama style)
- [ ] Email verification & Tokens to access our api
- [ ] Self fixing errors
- [ ] Check with GPT 4.

# Ranu: Create APIs    
- [x] Create youtube search
- [ ] Add comment search, search by channel and playlist
- [ ] Write endpoint API functions for Azure. 
- [ ] Run the data from the tools through data model. 
- [x] Create bing search function
- [ ] Create "subscription" based access to the apis. 
- [ ] How to store credentials securely.


# TODO: Ranu: Task graph management
- If the graph exists, do not overwrite.  Ask the user if the old one should be used.
- Make sure that the names are descriptive.
- When a new task starts it should have a new name and new conversations to be attached to it.
- Work on the palnner to add new tasks anytime.

# TODO: Vivek: LLM Research
- Sampling
- Showing research
- Adding system instruction

# TODO: Categorization template:
- Create a template for categorization and make sure it works well.

# TODO: Better layout for templates
- When a conversation starts, provide templates as cards with images and information for users to select.

# TODO: Menu
- Hamburger when the menu collapses.
- Add Parent to the menu bar.
- Change color of the menu bar when we go to different levels.

# TODO:
- Send errors back to LLM fix.
- Remove admin for non server users.
# FOR ANOTHER DAY: NAVIGATION FIXES
- Navigation right now is not called as a task.  Either create a task, or call the nav agent directly.
- Access to gpt 4 turbo is available only after someone pays $1.
- Make sure that we do not have too many calls for simple navigation.

# BUGS TO FIX
- [ ] Vivek: When we click on new chat, it creates a message.  As a result, load graph option disappears.
"""


import time
import streamlit as st
from plugins.llms import get_llm_output

from task_graph import TaskGraph
from master_planner import create_master_planner, get_planner_instructions
import importlib
import json
import inspect
import yaml

def display_messages(expanded=True):
    """
    Displays the messages in the chat.

    Utilizes Streamlit's expander and chat_message for displaying messages.
    This method iterates through the messages list and displays each one based
    on the role (user, assistant, system).

    Returns:
        None
    """
    # Display the name of the task graph as header
    tg = st.session_state.task_graph
    if tg.name:
        st.header(tg.name)

    messages = tg.messages.get_all_messages()
    for i, msg in enumerate(messages):
        if st.session_state.developer_options:
            st.code(msg)
        content = ""
        if msg.get('content', None) is None:
            pass
        # Some tools return a key called res_dict.  Parse it here.
        elif 'res_dict' in msg:
            res_dict = msg['res_dict']
            content = res_dict.get('content', res_dict.get('user_message', ''))
        
        # If content is a dict, look for user message
        elif isinstance(msg.get('content', ''), dict):
            
            res_dict = msg['content']
            content = res_dict.get('content', res_dict.get('user_message', ''))
        # LLMs return a dict, but the content is typically json
        # Parse the content key, which is the json.        
        elif msg.get('content', '').strip().startswith('{'):
            st.code(msg['content'])
            res_dict = json.loads(msg['content'])
            content = res_dict.get('content', res_dict.get('user_message', ''))
        else:
            res_dict = {}
            content = msg.get('content', '')

        res_dict['created_by'] = msg.get('created_by', None)
        if content:
            if msg['role'] in ['user', 'assistant']:

                with st.chat_message(msg['role']):
                    st.markdown(content.replace('\n', '\n\n'))
            # TODO: REMOVE SYSTEM MESSAGES AFTER FIXING BUGS
            if msg['role'] == 'system':
                st.info(content)        
        else:
            # st.code(res_dict)
            pass
        
        if "tool_name" in res_dict:
            # st.success(f"Tool name: {res_dict['tool_name']}")
            manage_tool_interaction(res_dict, from_llm=False)

        tool_calls = msg.get('tool_calls', None)
        if tool_calls:
            for res_dict in tool_calls:
                # Add created by
                res_dict['created_by'] = msg.get('created_by', None)
                manage_tool_interaction(res_dict, from_llm=False)
        
    return None

def what_task_now():
    """
    Determines what the next task is and sets it as the current task.
    """

    tg = st.session_state.task_graph
    
    # Current task will be set to planning by default
    current_task_name = 'planning'
    task_object = None
    # If the current task in session state is not planning 
    # Check if there is a follow up task
    if st.session_state.current_task != 'planning':
        next_task_name = tg.get_next_task()
        next_task = tg.get_next_task_node()
        
        # If a follow up task exists, make that the current task
        if next_task:
            st.session_state.current_task = next_task_name
            current_task_name = next_task_name
            task_object = next_task['task']
    return current_task_name, task_object


def manage_llm_interaction():
    """
    This functions gets a response from the LLM by providing 
    it with system instructions and messages appropriate for the current task.
    """
    # Get a few key variables needed for the task    
    planning = st.session_state.planning
    model = planning.default_model
    tg = st.session_state.task_graph
    current_task, task_object = what_task_now()    

    if current_task == 'planning':
        system_instruction = get_planner_instructions()
        messages = tg.messages.get_all_messages(add_task_name=False)
    
    # If there is a next task
    else:
        system_instruction = task_object.get_system_instruction()
        messages = tg.get_messages_for_task_family(st.session_state.current_task)
    
    
        if 'default_model' in task_object.__dict__:
            model = task_object.default_model

    # Remove messages with no content, which happens with function calling
    messages = [m for m in messages if m.get('content', None)]

    # Open tools
    with open('tools/tools.json', 'r') as f:
        tools = json.load(f)
    messages.insert(0, {"role": "system", "content": system_instruction})

    res, tool_calls = get_llm_output(messages, model=model, functions=tools)
    
    tg.messages.set_message(role="assistant", content=res, tool_calls=tool_calls, created_by=st.session_state.current_task, created_for=st.session_state.current_task)
    return res, tool_calls


def set_next_actions(res_dict):
    """
    Parse the response to see 
    1. If the next step should go to the LLM or a human
    2. If tasks must be added to the task graph
    3. To set the current task in the session state
    """
    tg = st.session_state.task_graph
    if res_dict.get('type', None) == 'task_graph':
        tg.name = res_dict.get('name', None)
        tg.objective = res_dict.get('objective', None)
        st.success("I set the task graph name and objective.")

    # If ask human is in the response, set ask_llm to the opposite
    if "ask_human" in res_dict:
        st.session_state.ask_llm = not res_dict['ask_human']

    for task_res in res_dict.get('task_list', []):
        tg.add_task(**task_res)
    
        # If ask human is in the response, set ask_llm to the opposite
        if "ask_human" in res_dict:
            st.session_state.ask_llm = not res_dict['ask_human']
    
    if res_dict.get("task_finished", False):
        finish_tasks(res_dict)
    return None

def loop_through_message_to_update_task():
    """
    Loop through the messages to see if any task needs to be updated.
    If yes, update existing task, and mark it as updated.
    """
    tg = st.session_state.task_graph
    messages = tg.messages.get_all_messages()
    
    for i,msg in enumerate(messages):
        # Get just the content key and load it
        # TODO: I have to look for update task in the content.
        content = msg.get('content', {})
        if content is None:
            continue

        if 'update_task' in content:
            # If the key 'update_task' is there, it must contain a dict.
            # Convert content to dict if it is a string
            if isinstance(content, str):
                content = json.loads(content)
            
            update_existing_task(content)
            # Mark the message as updated
            # Take update task out of the message and make it updated task
            updated_task = content.pop('update_task')
            content['updated_task'] = updated_task
            tg.messages.update_message_content(i, content)
    return None

def update_existing_task(res_dict):
    """
    Extract relevant information and update the task graph
    """
    tg = st.session_state.task_graph
    params_to_update = res_dict['update_task']
    # Get the task name, task description, and task_finished variables.  Remove them from the dict.
    task_name = params_to_update.pop('task_name', None)
    task_description = params_to_update.pop('task_description', None)
    task_finished = params_to_update.pop('task_finished', None)

    # Get the value for update_dependent_tasks, if it exists.
    update_dependent_tasks = params_to_update.pop('update_dependent_tasks', False)

    # Update the task
    tg.update_task(
        task_name=task_name,
        task_description=task_description,
        task_finished=task_finished,
        other_attributes=params_to_update
        )
    
    # Update the dependent tasks, if relevant
    if update_dependent_tasks:
        tg.update_dependent_tasks(task_name)
    st.success(f"Updated {task_name}")
    


    # Now set the current task to task graph, not planning
    st.session_state.current_task = 'task_graph'
    return None

def finish_tasks(res_dict):
    """
    Determines what to do when a task is finished
    """
    # If task_finished is True, set ask llm to False
    # If current task is planning, set it as not planning and other way around
    if st.session_state.current_task == 'planning':
        st.session_state.current_task = 'task_graph'
        st.session_state.task_graph.send_to_planner = False
    else:
        # Finish the current task
        st.session_state.task_graph.update_task(
            task_name=st.session_state.current_task,
            completed=True
            )
        next_task = st.session_state.task_graph.get_next_task()
        if next_task:
            st.session_state.current_task = next_task
            st.session_state.ask_llm = True
            st.session_state.task_graph.send_to_planner = False
        else:
            st.session_state.current_task = 'planning'
            st.session_state.ask_llm = False
            st.session_state.task_graph.send_to_planner = True

    return None

def check_for_auto_rerun(func):
    # Get the signature of the function
    sig = inspect.signature(func)

    # Get the parameters from the signature
    params = sig.parameters

    # Get the arguments and their default values
    args_and_defaults = {name: param.default if param.default is not param.empty else None for name, param in params.items()}

    # Check if the function has an auto_rerun argument
    return args_and_defaults.get('auto_rerun', False)
 

def manage_tool_interaction(res_dict, from_llm=False, run_tool=False):
    """
    Some tools run each time messages are loaded
    while others are run once (e.g. to fetch data).
    This function determines if the tool should be run,
    runs it if needed, and returns the result.

    Args:
        res_dict (dict): Dict with key variables in messages.
        from_llm (bool, optional): Whether the response was just created by the LLM. Defaults to False.
        run_tool (bool, optional): Whether the tool should be run. Defaults to False.
    """
    # Part 1: Get the information needed for this function

    # Find the function that needs to be run
    tool_name = res_dict['tool_name']

    # Get the keyword arguments for the function by the LLM
    args_for_tool = res_dict.get('kwargs', res_dict)
    # If there is a key called 'arguments', add that to the args_for_tool
    if 'arguments' in res_dict:
        args_for_tool.update(res_dict['arguments'])
    args_for_tool['key'] = res_dict['created_by']
    args_for_tool['task_name'] = res_dict['created_by']

    tool_module = importlib.import_module(f'tools.{tool_name}')
    tool_function = getattr(tool_module, 'tool_main')

    # Get the arguments needed by the function
    tool_args = inspect.getfullargspec(tool_function).args
    st.warning(f"These are the tool args: {tool_args}")
    
    # Sometimes the required arguments are not within the kwargs key
    for a in tool_args:
        if a not in args_for_tool:
            if a in res_dict:
                args_for_tool[a] = res_dict[a]

    # Make sure the keyword args are only the ones needed by the tool
    kwargs = {k: v for k, v in args_for_tool.items() if k in tool_args}
    
    # If the tool_args contains kwargs, put all the other arguments in kwargs
    if 'kwargs' in tool_args and 'kwargs' not in args_for_tool:
        kwargs['kwargs'] = {k: v for k, v in args_for_tool.items() if k not in tool_args}
        st.info(f"Adding kwargs {kwargs['kwargs']} to the tool args.")
    if not run_tool:
        # When the LLM creates a tool, it will create a 'task_for_tool' key in the session state.
        # If this key exists, it means that the tool has not been run yet.
        if 'task_for_tool' in st.session_state:
            run_tool = True
            # Set from llm to True
            from_llm = True
        # Some tools should be run each time the message is loaded (e.g. visuzlizations, showing tables)
        # Such tools have an auto_rerun argument in the tool_main function.
        run_tool = check_for_auto_rerun(tool_function)


    if run_tool:
        # Allow for errors in the tool
        try:
            # Show the args being sent to the tool
            if st.session_state.developer_options:
                st.code(f"Args for the tool {tool_name}:")
                st.json(kwargs)
            with st.spinner(f"Running {tool_name}..."):
                tool_result = tool_function(**kwargs)
            
            # When we get the tool result, delete task for tool in session state
            if 'task_for_tool' in st.session_state:
                del st.session_state['task_for_tool']

            # if st.session_state.developer_options:
            #     st.code(f'Tool result: {tool_result}')
            
            
            if isinstance(tool_result, str):
                tool_result = {'content': tool_result}
        except Exception as e:
            # Add the error to the content to see if the LLM fixes it.
            # TODO Setting task finished to True since LLM will not be able to fix it for now.
            # Change this when we have a way to fix errors.
            tool_result = {'content': f"Error: {e}", 'task_finished': False, 'ask_human': True}
            st.error(f"We ran into this error while running {tool_name}: {e}")

        # Check if the the task is done and can be transferred to orchestration.

        if from_llm:
            # Add the tool result to the task graph
            # TODO: SHOLD WE UPDATE THE TASK GRAPH HERE DURING A RERUN? 
            tg = st.session_state.task_graph
            tg.update_task(
                task_name=st.session_state.current_task,
                other_attributes=tool_result
                )
            if tool_result.get('task_finished', False) == True:
                finish_tasks(tool_result)
            else:
                st.session_state.ask_llm = True
    
            content = tool_result.get('content', '')
            if content:
                # Add this to the agent's messages
                st.session_state.task_graph.messages.set_message(
                    role='user', 
                    content=content, 
                    created_by=st.session_state.current_task, 
                    created_for=st.session_state.current_task
                    )

    return None

def init_chat():
    """
    Initiate some key variables for the chat
    """
    if st.sidebar.button("New chat"):
        st.session_state.task_graph = TaskGraph()
        st.session_state.current_task = 'planning'
        st.session_state.ask_llm = False
        # Make dynamic functions and dynamic variables in session state empty dicts
        st.session_state.dynamic_functions = {}
        st.session_state.dynamic_variables = {}
        if 'task_for_tool' in st.session_state:
            del st.session_state['task_for_tool']
        st.rerun()
    if st.sidebar.button("Stop LLM"):
        st.session_state.ask_llm = False
    if 'task_graph' not in st.session_state:
        st.session_state.task_graph = TaskGraph()
    create_master_planner()
    tg = st.session_state.task_graph
    tg.messages.chat_input_method(task_name=st.session_state.current_task)
    try:
        current_agent = tg.graph.nodes[st.session_state.current_task]['task'].agent_name
    except:
        current_agent = 'unknown'
    st.sidebar.info(f"Ask llm: {st.session_state.ask_llm}\n\nCurrent task: {st.session_state.current_task}\n\nCurrent agent: {current_agent}")
    with st.sidebar.expander("Select data"):
        select_data()
    loop_through_message_to_update_task()
    display_messages()

def select_data():
    from data_management.data_selection_ui import interface
    res = interface()
    # Add the content to the messages for planning
    if res:
        if st.button("Set data source"):
            st.session_state.task_graph.messages.set_message(
                role='user', 
                content=res, 
                created_by=st.session_state.current_task, 
                created_for=st.session_state.current_task
                )


def post_llm_processes():
    tg = st.session_state.task_graph
    next_task = tg.get_next_task()
    if next_task:
        run_next_task(tg, next_task)
        pass
    else:
        show_templates(tg)
    return None

def run_next_task(tg, next_task):
    """
    Get the next task and display it
    """    
    if st.button(f"Start task {next_task}"):
        st.session_state.ask_llm = True
        tg.send_to_planner = False
        st.session_state.current_task = next_task
        the_task = tg.graph.nodes[next_task]['task']
        si = the_task.get_system_instruction()
        if st.session_state.developer_options:
            st.code(si)
        st.markdown(the_task.task_description)
        st.rerun()

def show_templates(tg):
    """
    Show task templates if there are no
    messages in the conversation

    Parameters:
    tg (object): The task generator object.

    Returns:
    None
    """
    
    # See if there are no messages
    with st.expander("Load templates & past work", expanded=True):
        if not tg.messages.get_all_messages() and st.session_state.selected_node == 'HOME':
            # DL version
            # tg._load_from_file()
            # json version
            new_old = st.radio("Create new task or view old task", ['Create new task', 'View old task'], horizontal=True)
            if new_old == 'Create new task':
                st.info("The templates below will make it easier to create certain types of tasks.  For a generic task, just start typing below.")
                load_templates()
            else:
                tg._load_from_json()
            if tg.templates:
                st.sidebar.info(f"{len(tg.templates)} templates loaded.")
    return None

def load_templates():

    # Open templates.yml file
    with open('templates.yml', 'r') as f:
        templates = yaml.safe_load(f)

    # Let the user select a template, with "SELECT" as the default
    template_names = st.multiselect('Select a template', list(templates.keys()))
    
    # If the user selects a template, show the description.
    for template_name in template_names:
        st.info(templates[template_name]['description'])
    

    # If the user clicks on the button, load the template
    if st.button("Load templates"):
        tg = st.session_state.task_graph
        for template_name in template_names:
            template = templates[template_name]
            tg.templates[template_name] = template
            # If template_name is small_business, make that the current task.
            if template_name == 'small_business_assistant':
                st.session_state.current_task = 'small_business'
                st.success("Set current task to small_business.")
                
                
        st.success("Loaded the template.  You can start typing below now.")
    
    return None

def chat():


    from object_management import test

    test()
    init_chat()
    tg = st.session_state.task_graph
    

    # Check if any task has been allocated for a tool.
    # If yes, run it before we get to the ask llm loop.
    if 'task_for_tool' in st.session_state:
        tasks = st.session_state.task_for_tool
        if not isinstance(tasks, list):
            tasks = [tasks]
        for res_dict in tasks:
            res_dict['created_by'] = st.session_state.current_task
            manage_tool_interaction(res_dict, from_llm=True, run_tool=True)

    # Show the system instruction for the current task if it exists
    # if st.session_state.current_task == 'planning':
        # si = st.session_state.planning.get_system_instruction()
        # st.sidebar.warning(si)

    if st.session_state.ask_llm:
        res, tool_calls = manage_llm_interaction()
        
        if res:
            res_dict = json.loads(res)
        else:
            res_dict = {}
    
        
        # If there is 'message_to_planner' key, it means that an agent is 
        # trying to send a message to the planner.
        # Set ask_llm to True and current task to planning.
        if 'message_to_planner' in res_dict:
            st.session_state.task_graph.messages.set_message(
                role='user', 
                content=res_dict['message_to_planner'], 
                created_by=st.session_state.current_task, 
                created_for="planning"
                )
            st.session_state.ask_llm = True
            st.session_state.current_task = 'planning'
            st.rerun()
        # Find if LLM should be invoked automatically
        # and next action if task is finished
        try:
            set_next_actions(res_dict)
        except Exception as e:
            # Add the errror to the messages and ask LLM to fix it.
            st.session_state.task_graph.messages.set_message(
                role='user', 
                content=f"I got this error: {e}.\nPlease fix it.", 
                created_by=st.session_state.current_task, 
                created_for=st.session_state.current_task
                )
            st.session_state.ask_llm = True

        if 'tool_name' in res_dict:
            st.session_state.task_for_tool = res_dict
        
        # If there is a tool_call, add that to the task_for_tool
        if tool_calls:
            st.session_state.task_for_tool = tool_calls

        # Save the graph to file, if it has a name
        # TODO: Make sure that we don't create a name that overwrites an existing one.
        if tg.name:
            tg._save_to_json()
        st.rerun()
    # Display the agent name
    post_llm_processes()
    return None
--------------------File: messages.py
"""
A class for messages.
"""

from collections import namedtuple
import streamlit as st
import time

class Messages:
    """
    A class to hold the messages for one task graph.
    """
    def __init__(self, task_name):
        self.task_name = task_name
        self.messages = []
        self.message_tuple = namedtuple('message_tuple', ['content', 'tool_calls', 'role', 'created_by', 'created_for', 'ts'])

    def set_message(self, content, role, created_by, created_for, tool_calls=None):
        self.messages.append(self.message_tuple(content, tool_calls, role, created_by, created_for, time.time()))
        return None

    def update_message_content(self, index, new_content):
        """
        Updates the content of a message at the given index.
        If the index is out of range, it does nothing.
        """
        if index < len(self.messages):
            self.messages[index] = self.messages[index]._replace(content=new_content)
        return None

    def get_messages_for_task(self, task_name):
        return [{'role': m.role, 'content': m.content} for m in self.messages if m.created_for == task_name]
    
    def get_messages_for_task_family(self, task_family):
        return [{'role': m.role, 'content': m.content} for m in self.messages if m.created_for in task_family]
    
    def get_messages_by_task(self, task_name):
        return [m for m in self.messages if m.created_by == task_name]
    
    def get_all_messages(self, add_task_name=True):
        if add_task_name:
            return [{'role': m.role, 'content': m.content, 'tool_calls': m.tool_calls, 'created_by': m.created_by} for m in self.messages]
        else:
            return [{'role': m.role, 'content': m.content} for m in self.messages]
    
    def export_all_messages(self):
        return [m._asdict() for m in self.messages]
    
    def chat_input_method(self, task_name=None):
        """
        Handles the input of chat messages.

        This method provides an input field for the user to enter their message.
        Upon receiving a message, it updates the messages list and sets the
        `ask_llm` flag to True.

        Returns:
            None
        """
        prompt = st.chat_input("Enter your message", key="chat_input")
        if prompt:
            if not task_name:
                task_name = st.session_state.current_task
            self.set_message(role="user", content=prompt, created_by="user", created_for=task_name)
            self.ask_llm = True
            st.session_state.ask_llm = True
            self.display_expanded = True
            st.session_state.all_messages.append({'role': 'user', 'content': prompt})
        return None

    def get_messages_with_instruction(self, system_instruction, created_for=None, prompt=None):
        """
        Returns a copy of the messages list with a new system instruction message added at the beginning.

        Args:
            system_instruction (str): The system instruction message to be added.
            prompt (str, optional): The prompt to be added at the end of the list. Defaults to None.

        Returns:
            list: A new list of messages with the system instruction message added at the beginning.
        """
        messages = self.messages.copy()
        # Get role and content from the named tuple as a dict
        if created_for:
            messages = [{'role': m.role, 'content': m.content} for m in messages if m.created_for == created_for]
        else:
            messages = [{'role': m.role, 'content': m.content} for m in messages]

        # Add the system instruction message at the beginning
        messages.insert(0, {'role': 'system', 'content': system_instruction})

        if prompt:
            messages.append({'role': 'user', 'content': prompt})
        return messages

--------------------File: function_management.py
import streamlit as st
import importlib


# For the selected_node, get the func_name and hte module_name
def run_function(module_name: str, func_name: str):
    module = importlib.import_module(module_name)
    func = getattr(module, func_name)
    func()
    return None

def get_module_and_function(selected_node):
    """
    Get the module and function name from the selected node.
    """
    if not selected_node:
        selected_node = st.session_state.activeStep
    menu = st.session_state.menu

    if selected_node == 'HOME':
        module_name = 'home_page'
        func_name = 'home_page'
    else:
        mynode = menu.G.nodes[selected_node]
        module_name = mynode.get('module_name')
        func_name = mynode.get('func_name')
    return module_name, func_name

def run_current_functions():
    """
    Runs the functions selected in the menu.

    This function retrieves the module name and function name from the menu based on the active step.
    If the active step is 'HOME', it sets the module name and function name to 'home_page'.
    Otherwise, it retrieves the module name and function name from the menu's graph based on the active step.
    
    If the module name is 'home_page', the function does nothing.
    If both the module name and function name are not None, it calls the 'run_function' function with the module name and function name as arguments.
    If either the module name or function name is None, it raises an error using the 'st.error' function.

    Parameters:
    None

    Returns:
    None
    """

    module_name, func_name = get_module_and_function(st.session_state.activeStep)
    if module_name == 'home_page':
        pass
    elif module_name and func_name:  
        run_function(module_name, func_name)
    else:
        # Tell me what was None
        if not module_name:
            st.error("Module name was None")
        if not func_name:
            st.error("Function name was None")

def get_docstring_of_function(active_step):
    """
    Gets the docstring of the function selected in the menu.
    """
    if not active_step:
        active_step = st.session_state.activeStep
    module_name, func_name = get_module_and_function(active_step)
    st.info(f"M: {module_name}, F: {func_name}")
    module = importlib.import_module(module_name)
    func = getattr(module, func_name)
    return func.__doc__
--------------------File: import_data.py
import streamlit as st

# RANU: I CREATED THIS NEW FUNCTION JUST TO LET YOU KNOW THAT WE 
# HAVE TO ADD THESE FUNCTIONALITIES SOMEWHERE.
def data_menus():
    options = [
        'Upload your data',
        'Fetch data',
    ]


    default_index = 0

    selected_option = st.radio(
        "Select an option", 
        options, 
        captions=["CSV, XLSX, TXT, VTT, etc.", "YouTube, Google Search"],
        horizontal=True, 
        index=default_index
        )
    st.markdown("---")
    if selected_option == 'Upload your data':
        file_upload_and_save()
        get_data_model()
        st.stop()

    if selected_option == 'Fetch data':
        if st.checkbox("Get data from YouTube"):
            from tools.yt_search import main as yt_search
            yt_search()
            st.warning("Uncheck get data from YouTube to go to project settings")
        if st.checkbox("Get data from Google"):
            from tools.google_search import main as google_search
            google_search()
            st.warning("Uncheck get data from Google to go to project settings")
        st.stop()



def export_sqlite_to_parquet(uploaded_file, output_dir):
    
    tmp_folder = os.path.join(st.session_state.project_folder, 'documents')
    # Create the tmp folder if it does not exist
    if not os.path.exists(tmp_folder):
        os.makedirs(tmp_folder)
    
    with open(os.path.join(tmp_folder, 'tmp.sqlite'), 'wb') as f:
        f.write(uploaded_file.read())
    # Connect to the SQLite database
    conn = sqlite3.connect(os.path.join(tmp_folder, 'tmp.sqlite'))
    # Get the list of all tables in the database
    query = "SELECT name FROM sqlite_master WHERE type='table';"
    tables = conn.execute(query).fetchall()
    tables = [table[0] for table in tables]

    tables = st.multiselect("Select tables to import", tables)
    if st.button("Import these tables"):
        # Ensure output directory exists
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        # For each table, read it into a pandas DataFrame and then write it to a Parquet file
        for table in tables:
            df = pd.read_sql(f"SELECT * FROM {table}", conn)
            df.to_parquet(os.path.join(output_dir, f"{table}.parquet"), index=False)

    # Close the SQLite connection
    conn.close()
    return None

def upload_data_file(uploaded_file, file_extension):
    """
    Upload data files to import them into the project.
    """
    data_folder = os.path.join(st.session_state.project_folder, 'data')
    # Load the file as a dataframe
    if file_extension == 'csv':
        df = pd.read_csv(uploaded_file)
    elif file_extension == 'parquet':
        df = pd.read_parquet(uploaded_file)
    elif file_extension == 'tsv':
        df = pd.read_csv(uploaded_file, sep='\t')
    elif file_extension in ['xlsx']:
        df = pd.read_excel(uploaded_file)
    else:
        st.error(f'File type {file_extension} not supported')
        st.stop()    
    # Clean column names.  Strip, lower case, replace spaces with underscores
    df.columns = [i.strip().lower().replace(' ', '_') for i in df.columns]
    
    # Create a streamlit form, with all columns and data types and allow the user to edit the data types

    # Get the list of column names
    col_names = df.columns.tolist()

    # If there are duplicate column names, add _1, _2, etc. to the end of the column name
    for i, col_name in enumerate(col_names):
        if col_names.count(col_name) > 1:
            col_names[i] = col_name + '_' + str(col_names[:i].count(col_name) + 1)

    # Rename the columns with the updated column names
    df.columns = col_names

    # If there are duplicate col names, add _1, _2, etc. to the end of the col name
    # Get the list of col names
        
    # Get the column names and data types


    # Get the column names and data types
    all_col_infos = []
    for column in df.columns:
        column_info = {}
        column_info['column_name'] = column
        column_info['column_type'] = str(df[column].dtype)
        column_info['column_info'] = ''
        all_col_infos.append(column_info)

    # Update the data types of the dataframe
    for col_info in all_col_infos:
        col_name = col_info['column_name']
        col_type = col_info['column_type']
        if col_type != 'object':
            df[col_name] = df[col_name].astype(col_type)

    # Show the dataframe
    
    st.dataframe(df)

    # Get the name of the uploaded file
    file_name = uploaded_file.name
    # Remove the file extension
    file_name = file_name.replace(f'.{file_extension}', '')
    st.info("Once you save the data, we will explore a few lines of data to a language model to understand the data.  This will help us later to generate code for the data.")
    # Create a button to save the file as a parquet file with the same name
    if st.button('Save Data'):
        
        # Save the file to the data folder
        file_path = os.path.join(data_folder, file_name + '.parquet')
        # Create folder if it does not exist
        folder_name = os.path.dirname(file_path)
        if not os.path.exists(folder_name):
            os.makedirs(folder_name)
        df = clean_col_formats(df)
        df.to_parquet(file_path, index=False)
        st.success(f'File saved successfully')
        st.session_state.files_uploaded = False

    return None

def clean_col_formats(df):
    """
    Apache arrow requires clearn formatting.  Look at the 
    column names and data types and clean them up.

    Try saving as column type and see if it works. If not, save as string.

    Args:
    - df (dataframe): The dataframe to clean

    Returns:
    - df (dataframe): The cleaned dataframe
    """

    # Get the list of column names
    col_names = df.columns.tolist()

    # If there are duplicate column names, add _1, _2, etc. to the end of the column name
    for i, col_name in enumerate(col_names):
        if col_names.count(col_name) > 1:
            col_names[i] = col_name + '_' + str(col_names[:i].count(col_name) + 1)

    # Rename the columns with the updated column names
    df.columns = col_names

    # st.dataframe(df)

    # If there are duplicate col names, add _1, _2, etc. to the end of the col name
    # Get the list of col names
        
    # Get the column names and data types
    all_col_infos = []
    for column in df.columns:
        column_info = {}
        column_info['column_name'] = column
        column_info['column_type'] = str(df[column].dtype)
        column_info['column_info'] = ''
        all_col_infos.append(column_info)

    # Update the data types of the dataframe
    for col_info in all_col_infos:
        col_name = col_info['column_name']
        col_type = col_info['column_type']
        if col_type != 'object':
            try:
                df[col_name] = df[col_name].astype(col_type)
            except:
                df[col_name] = df[col_name].astype(str)
        
        if col_type == 'object':
            df[col_name] = df[col_name].astype(str)

    return df

def upload_document_file(uploaded_file, file_extension):
    """
    This function allows the user to upload a document file and save it to the project folder.
    Args:
    - uploaded_file (file): The file uploaded by the user.
    - file_extension (str): The file extension of the uploaded file.

    Returns:
    - None
    """
    tmp_folder = os.path.join(st.session_state.project_folder, 'documents')
    # Create the tmp folder if it does not exist
    if not os.path.exists(tmp_folder):
        os.makedirs(tmp_folder)
    
    # Get the name of the uploaded file
    file_name = uploaded_file.name
    # Get the file extension
    file_extension = file_name.split('.')[-1]
    # Remove the file extension
    file_name = file_name.replace(f'.{file_extension}', '')
    # Save the file to the tmp folder
    
    tmp_file_path =  os.path.join(tmp_folder, f"{file_name}.{file_extension}")
    with open(tmp_file_path, 'wb') as f:
        f.write(uploaded_file.getbuffer())
    
        uploaded_file = None
    return None


def file_upload_and_save():
    """
    This function allows the user to upload a CSV or a parquet file, load it as a dataframe,
    and provides a button to save the file as a parquet file with the same name.
    """
    data_folder = os.path.join(st.session_state.project_folder, 'data')
    # Define the allowed file types
    allowed_data_file_types = ['csv', 'parquet', 'xlsx' , 'tsv', 'sqlite', 'db', 'sqlite3']
    allowed_document_file_types = ['pdf', 'txt', 'vtt']
    # Ask the user to upload a file
    uploaded_files = st.file_uploader(
        "Upload a file", 
        type=allowed_data_file_types + allowed_document_file_types, 
        accept_multiple_files=True)

    file_extension = None
    if len(uploaded_files) ==1:
        st.session_state.files_uploaded = True
        st.warning(f'Adding your new document(s) to the existing documents database')   
        uploaded_file = uploaded_files[0]
        # Get the file extension
        file_extension = uploaded_file.name.split('.')[-1]
        # If the file is a data file, upload it as a data file
        if file_extension in ['sqlite', 'db', 'sqlite3']:
            export_sqlite_to_parquet(uploaded_file, data_folder)
            st.success(f'Files saved successfully')
        elif file_extension in allowed_data_file_types:
            upload_data_file(uploaded_file, file_extension)
        # If the file is a document file, upload it as a document file
        elif file_extension in allowed_document_file_types:
            upload_document_file(uploaded_file, file_extension)

    elif len(uploaded_files) > 1:
        st.warning(f'Adding your new document(s) to the existing documents database')
        # Get the file extension
        file_extension = uploaded_files[0].name.split('.')[-1]
        # If the file is a document file, upload it as a document file
        if file_extension in allowed_document_file_types:
            for uploaded_file in uploaded_files:
                upload_document_file(uploaded_file, file_extension)
    if file_extension:
        if file_extension in allowed_document_file_types:
            tmp_folder = os.path.join(st.session_state.project_folder, 'documents')
            # Create chunks of the document and save it to the data folder
            df_chunks = create_document_chunk_df(tmp_folder)
            # Add documents_tbid to the dataframe
            df_chunks['documents_tbid'] = df_chunks.index+1
                # Move the id column to the front
            cols = df_chunks.columns.tolist()
            cols = cols[-1:] + cols[:-1]
            df_chunks = df_chunks[cols]

            # Show the dataframe
            st.dataframe(df_chunks)
            uploaded_file = None
            # Create a button to save the file as a parquet file in the data folder with the same name
            # If the parquet file already exists, append the data to the existing file
            if st.button('Save Document'):
                # Save the file to the data folder
                file_path = os.path.join(data_folder, 'documents.parquet')
                # If the file already exists, append the data to the existing file
                if os.path.exists(file_path):
                    # Load the existing file as a dataframe
                    df = pd.read_parquet(file_path)
                    # Append the data
                    df = pd.concat([df, df_chunks])
                    df = df.drop_duplicates(keep='first')
                    # Save the file to the data folder
                    df.to_parquet(file_path, index=False)
                    st.success(f'Data added successfully')
                else:
                    # Save the file to the data folder
                    df_chunks = df_chunks.drop_duplicates(keep='first')
                    df_chunks.to_parquet(file_path, index=False)
                    st.success(f'Data saved successfully')

    return None


def append_data_to_exisiting_file():

    """
    This function allows the user to append data to an existing file. 
    It also allows the user to process the data and save it to a new file.
    You can upload a CSV, JSON, PARQUET, EXCEL, or PICKLE file.

    Once the file is uploaded, it is added to an existing parquet file.

    """

    file_path = os.path.join(st.session_state.project_folder, 'data')

    # Get the list of files in the project folder
    files = glob(os.path.join(file_path, '*.parquet'))

    # Ask the user to select a file to append data to
    selected_file = st.selectbox("Select a file to append data to", files)
    df1 = pd.read_parquet(selected_file)
    # Upload a new file
    uploaded_file = st.file_uploader("Upload a file", type=['csv', 'parquet'])
    # If a file was uploaded, create a df2 dataframe
    if uploaded_file is not None:
        # Get the file extension
        file_extension = uploaded_file.name.split('.')[-1]

        # Load the file as a dataframe
        if file_extension == 'csv':
            df2 = pd.read_csv(uploaded_file)
        elif file_extension == 'parquet':
            df2 = pd.read_parquet(uploaded_file)

        # Show the dataframe
        st.dataframe(df2)

        # If the columns are different, show the missing columns
        df1_cols = set(df1.columns.tolist())
        df2_cols = set(df2.columns.tolist())
        if df1_cols != df2_cols:
            missing_cols = df1_cols.difference(df2_cols)
            st.warning(f'The following columns are missing in the uploaded file: {missing_cols}')
        else:
            st.info("The columns in the uploaded file match the columns in the existing file")


        # Create a button to append the data to the existing file
        if st.button('Append data'):
            # Append the data
            df = pd.concat([df1, df2])
            # Save the file to the data folder
            df.to_parquet(selected_file, index=False)
            st.success(f'Data appended successfully')
            uploaded_file = None
    return None


#######################################################################

##### GPT-4 REFACTORED CODE #####

"""
This file is about managing the project data.  Currently all
project data is stored as parquet and they are stored in a folder
called 'data' in the root folder of the project.

- Document the project data so that it can be sent to the LLM
- Add CRUD functionality to the project data
"""
import time
import pandas as pd
import streamlit as st
import os
from plugins.llms import get_llm_output
from glob import glob

def convert_to_appropriate_dtypes(df, data_model):
    
    """
    Convert the data types of the dataframe columns to the appropriate data types.

    Parameters:
    df (dataframe): The dataframe to be converted.
    data_model (dataframe): The dataframe with the column names and data types.

    Returns:
    A dataframe with the converted data types.
    
    """
    dtype_dict = dict(zip(data_model.column_name, data_model.column_type))
    for index, col in enumerate(dtype_dict):
        dtype = dtype_dict[col]
        if dtype == 'object': 
            pass
        elif 'date' in dtype:
            df[col] = pd.to_datetime(df[col], errors='coerce')
        else:
            try:
                df[col] = df[col].astype(dtype)
            except Exception as e:
                st.error(f"Could not convert the column **{col}** to {dtype}.  Got the following error: {e}")  
                pass  
    return df
    

def get_column_info_for_df(df):
    """
    Given a dataframe, return the column names and data types.

    Parameters:
    parquet_file (str): The path to the parquet file.

    Returns:
    str: A string containing the column names and data types, with one line per column.
    """

    # Read the parquet file using pandas

    # Get the column names and data types
    all_col_infos = []
    for column in df.columns:
        column_info = {}
        column_info['column_name'] = column
        column_info['column_type'] = str(df[column].dtype)
        column_info['column_info'] = ''
        
        all_col_infos.append(column_info)
        
    # Send this to the LLM with two rows of data and ask it to describe the data.
    
    if len(df) > 2:
        sample_data = df.sample(3).to_dict('records')
    else:
        sample_data = df.to_dict('records')
    sample_buf = "HERE IS SOME SAMPLE DATA:\n"

    for row in sample_data:
        for col in row:
            value = row[col]
            # If the value is a string, add quotes around it.  Limit the length of the string to 100 characters.
            if isinstance(value, str):
                if len(value) > 100:
                    value = value[:100] + '...'
                value = f"'{value}'"
            sample_buf += f"- {col}: {value}\n"
        sample_buf += '====================\n'

    # Add possible values for categorical columns
    # Assuming catgorical columns have less than 10 categories
    # and they repeat frequently.
    
    for col in df.columns:
        try:
            if df[col].nunique() / len(df) < 0.2:
                if df[col].nunique() < 10:
                    sample_buf += f"\nPossible values for {col}: {', '.join(df[col].dropna().astype(str).unique().tolist())}\n"
        except:
            # If the column is not a string, this will fail
            pass

    system_instruction = """You are helping me document the data.  
    Using the examples revise the column info by:
    - Adding a detailed description for each column
    - If the column is a date, add the date format
    - I will provide the initial column type, you need to check if the initial column type is appropriate or suggest the new column type
    - The possible column dtypes are ['object', 'int64', 'datetime64', 'float64']
    - You should strictly return the column information in the same format provided to you.  Include information about possible values, if provided.
    - Don't add any comments
    """

    prompt = f"""Given below is the draft column information.
    Return the revised column information with descriptions and data types.
    {all_col_infos}

    SAMPLE DATA:
    {sample_buf}

    You should strictly return the column information in the same format provided to you
    """
    messages = [
        {'role': 'system', 'content': system_instruction},
        {'role': 'user', 'content': prompt},
    ]
    res = get_llm_output(messages, model='gpt-3.5-turbo', max_tokens=2000, temperature=0)
    # Get the list of dicts from the string
    list_of_dicts = extract_list_of_dicts_from_string(res)
    df_res = pd.DataFrame(list_of_dicts)
    # if there are any missing values, fill them with object
    df_res.column_type = df_res.column_type.fillna('object')
    # There are some additional columns created by the LLM. we need the three columns we are interested in
    df_res = df_res[['column_name', 'column_type', 'column_info']]    
    return df_res


def get_column_info(data_model, new_files_only=True):
    """
    Loop through all the data files and get the column info for each file.
    """
    # Get the list of files in data model

    project_folder = st.session_state.project_folder
    data_folder = os.path.join(project_folder, 'data')
    data_files = glob(os.path.join(data_folder, '*.parquet'))

    # Get the list of files that have already been processed
    if data_model is None:
        # Create an empty dataframe
        data_model = pd.DataFrame(columns=['column_name', 'column_type', 'column_info', 'file_name'])
        data_model.to_parquet(os.path.join(project_folder, 'data_model.parquet'), index=False)
    processed_files = data_model.file_name.unique().tolist()

    if data_model is not None:
        data_model.column_info = data_model.column_info.fillna('')


    column_info = {}
    status = st.empty()
    all_col_infos = []
    all_col_info_markdown = ''

    for parquet_file_path in data_files:
        df = pd.read_parquet(parquet_file_path)  
        # Check which columns have col info
        avbl_col_info_for_file = data_model[
            (data_model.file_name == parquet_file_path) &
            (data_model.column_info != '')
            ].column_name.tolist()
        # Get missing cols
        missing_col_info_for_file = [i for i in df.columns.tolist() if i not in avbl_col_info_for_file]
        all_missing_dfs = []
        if len(missing_col_info_for_file) > 0:
            st.info(f"Getting column info for {missing_col_info_for_file} in {parquet_file_path}")
            df_missing = df[missing_col_info_for_file]
            df_col_info_missing = get_column_info_for_df(df_missing)
            all_missing_dfs.append(df_col_info_missing)
            df_col_info = df_col_info_missing
        else:
            df_col_info = data_model[data_model.file_name == parquet_file_path]
            
        df_col_info['file_name'] = parquet_file_path
        save_data_model(df_col_info, parquet_file_path)
        df = convert_to_appropriate_dtypes(df, df_col_info)
        try:
            df.to_parquet(parquet_file_path, index=False)
        except Exception as e:
            st.error(f"Could not save the file.  Got the following error: {e}")
        
        status.warning("Pausing for 5 secs to avoid rate limit")
        cols = df_col_info.columns.tolist()
        cols = cols[-1:] + cols[:-1]
        df_col_info = df_col_info[cols]
        all_col_infos.append(df_col_info)
        column_info = df_col_info.to_markdown(index=False)
        all_col_info_markdown += column_info + '\n\n'
        status.empty()

    st.session_state.column_info = all_col_info_markdown

    return None

def save_data_model(data_model_for_file, file_name):
    """
    Saves the data model.  If the model already exists, it will be appended to.
    Any old data model for the given file_name will be overwritten.
    """
    data_model_file = os.path.join(st.session_state.project_folder, 'data_model.parquet')
    all_dfs = []
    if os.path.exists(data_model_file):
        current_model = pd.read_parquet(data_model_file)
        # Remove information about file_name from the current model
        # current_model = current_model[current_model.file_name != file_name]
        all_dfs.append(current_model)

    all_dfs.append(data_model_for_file)
    df_all_col_infos = pd.concat(all_dfs)
    # Remove duplicates for given file name and column name, keeping the last one
    df_all_col_infos = df_all_col_infos.drop_duplicates(subset=['file_name', 'column_name'], keep='last')
    df_all_col_infos.to_parquet(data_model_file, index=False)
    return None


import os
import pandas as pd
from glob import glob
import streamlit as st
import time

def check_uploaded_files():
    """
    Checks if files have been uploaded and updates the session state.
    """
    if 'files_uploaded' not in st.session_state:
        st.session_state.files_uploaded = False

def get_processed_files(data_model_file, data_files):
    """
    Determines which files have been processed and which need processing.

    Args:
    - data_model_file: The path to the data model file.
    - data_files: List of data file paths.

    Returns:
    - A tuple containing lists of processed files and files to process.
    """
    processed_files = []
    files_to_process = data_files

    if os.path.exists(data_model_file):
        data_model_df = pd.read_parquet(data_model_file)
        st.session_state.column_info = data_model_df.to_markdown(index=False)

        processed_files = data_model_df.file_name.unique().tolist()
        files_to_process = [i for i in data_files if i not in processed_files]

        for file in processed_files:
            df = pd.read_parquet(file)
            cols_in_file = df.columns.tolist()
            data_model_df.column_info = data_model_df.column_info.fillna('')
            cols_in_data_model = data_model_df[
                (data_model_df.file_name == file) &
                (data_model_df.column_info != '')].column_name.tolist()
            cols_not_in_data_model = [i for i in cols_in_file if i not in cols_in_data_model]
            if cols_not_in_data_model:
                files_to_process.append(file)

    return processed_files, files_to_process

def update_column_info(files_to_process, data_model_df, data_model_file):
    """
    Updates column info based on user input and session state.

    Args:
    - files_to_process: List of files that need processing.
    - data_model_df: DataFrame containing the data model.
    - data_model_file: The path to the data model file.
    """
    if files_to_process and not st.session_state.files_uploaded:
        if st.checkbox(
            "🚨 Re-generate column info automatically for the filtered files 🚨",
            help="The LLM will recreate column definitions. Use this only if the table needs major changes."
        ):
            st.warning("Use this only if the table needs major changes")
            if st.button("Confirm regeneration"):
                data_model_df.loc[data_model_df.file_name.isin(files_to_process), 'column_info'] = ''
                # Additional logic for regeneration can be added here

def get_data_model():
    """
    Main function to orchestrate the data model generation process.
    """
    check_uploaded_files()

    project_folder = st.session_state.project_folder
    data_folder = os.path.join(project_folder, 'data')
    data_model_file = os.path.join(project_folder, 'data_model.parquet')
    data_files = glob(os.path.join(data_folder, '*.parquet'))

    processed_files, files_to_process = get_processed_files(data_model_file, data_files)

    data_model_df = None if not os.path.exists(data_model_file) else pd.read_parquet(data_model_file)

    update_column_info(files_to_process, data_model_df, data_model_file)

    if 'column_info' not in st.session_state or files_to_process:
        with st.spinner("Studying the data to understand it..."):
            get_column_info(data_model=data_model_df, new_files_only=True)
            st.success("Done studying the data. You can start using it now")
            time.sleep(3)
            st.rerun()

    return None


def update_colum_types_for_table(data_model, data_model_file):
    """
    Looks at the data model and converts
    the selected files
    """
    st.markdown("""---""")
    st.subheader("Does this look right?")
    info = """Take a look at the column_info and see if it looks right.  Getting it right will help us a lot when we work with the data."""
    st.info(info)
    # Get the list of files
    files = data_model.file_name.unique().tolist()
    # Get the list of files that have been selected

    selected_files = st.multiselect(
        "Filter files", 
        files,
        help="You can filter the file(s) you wish to examine now."
        )

    if not selected_files:
        display_editable_data(data_model, data_model_file)
    else:
        st.session_state.filtered_files_for_data_model = selected_files
        display_editable_data(
            df=data_model,
            filtered_df=data_model[data_model.file_name.isin(selected_files)], 
            file_name=data_model_file
            )
        if st.button("Update the column types"):
            for file in selected_files:
                df = pd.read_parquet(file)
                df = convert_to_appropriate_dtypes(df, data_model[data_model.file_name == file])
                df.to_parquet(file, index=False)
            st.success("Updated the column types")
    return None
--------------------